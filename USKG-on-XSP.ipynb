{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "a0b2a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from utils.configue import Configure\n",
    "from utils.training_arguments import WrappedSeq2SeqTrainingArguments\n",
    "from models.unified import finetune, prefixtuning\n",
    "from models.unified.prefixtuning import Model\n",
    "\n",
    "import nltk\n",
    "\n",
    "# from filelock import FileLock\n",
    "# with FileLock(\".lock\") as lock:\n",
    "#     nltk.download(\"punkt\", quiet=True)\n",
    "#     nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "e72e182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "from collections import Counter, defaultdict\n",
    "import importlib\n",
    "import pickle\n",
    "\n",
    "from seq2seq_construction import spider\n",
    "from third_party.spider.preprocess.get_tables import dump_db_json_schema\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import editdistance\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "# from SpeakQL.Allennlp_models.utils.spider import process_sql, evaluation\n",
    "# from SpeakQL.Allennlp_models.utils.misc_utils import EvaluateSQL, EvaluateSQL_full, \\\n",
    "#     Postprocess_rewrite_seq, Postprocess_rewrite_seq_freeze_POS, Postprocess_rewrite_seq_modify_POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2760b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from language.xsp.data_preprocessing import spider_preprocessing, wikisql_preprocessing, michigan_preprocessing\n",
    "\n",
    "import sdr_analysis\n",
    "importlib.reload(sdr_analysis.helpers.general_helpers)\n",
    "from sdr_analysis.helpers.general_helpers import db_dict_to_general_fmt, collect_link_prediction_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec3032d",
   "metadata": {},
   "source": [
    "## Read data - Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce7c4a4",
   "metadata": {},
   "source": [
    "### Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8c8e50",
   "metadata": {},
   "source": [
    "#### Original loading test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a49db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_cache = dict()\n",
    "\n",
    "db_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/database'\n",
    "\n",
    "def format_spider_schema(db_id):\n",
    "    if db_id not in schema_cache:\n",
    "        schema_cache[db_id] = dump_db_json_schema(\n",
    "            db_path + \"/\" + db_id + \"/\" + db_id + \".sqlite\", db_id)\n",
    "    schema = schema_cache[db_id]\n",
    "\n",
    "    return {\n",
    "        \"db_id\": db_id,\n",
    "        \"db_path\": db_path,\n",
    "        \"db_table_names\": schema[\"table_names_original\"],\n",
    "        \"db_column_names\": {\n",
    "            \"table_id\": [table_id for table_id, column_name in schema[\"column_names_original\"]],\n",
    "            \"column_name\": [column_name for table_id, column_name in schema[\"column_names_original\"]]\n",
    "        },\n",
    "        \"db_column_types\": schema[\"column_types\"],\n",
    "        \"db_primary_keys\": [{\"column_id\": column_id} for column_id in schema[\"primary_keys\"]],\n",
    "        \"db_foreign_keys\": [\n",
    "            {\"column_id\": column_id, \"other_column_id\": other_column_id}\n",
    "            for column_id, other_column_id in schema[\"foreign_keys\"]\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7e2a686b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'world_1',\n",
       " 'db_path': '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/database',\n",
       " 'db_table_names': ['city', 'sqlite_sequence', 'country', 'countrylanguage'],\n",
       " 'db_column_names': {'table_id': [-1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3],\n",
       "  'column_name': ['*',\n",
       "   'ID',\n",
       "   'Name',\n",
       "   'CountryCode',\n",
       "   'District',\n",
       "   'Population',\n",
       "   'name',\n",
       "   'seq',\n",
       "   'Code',\n",
       "   'Name',\n",
       "   'Continent',\n",
       "   'Region',\n",
       "   'SurfaceArea',\n",
       "   'IndepYear',\n",
       "   'Population',\n",
       "   'LifeExpectancy',\n",
       "   'GNP',\n",
       "   'GNPOld',\n",
       "   'LocalName',\n",
       "   'GovernmentForm',\n",
       "   'HeadOfState',\n",
       "   'Capital',\n",
       "   'Code2',\n",
       "   'CountryCode',\n",
       "   'Language',\n",
       "   'IsOfficial',\n",
       "   'Percentage']},\n",
       " 'db_column_types': ['text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number'],\n",
       " 'db_primary_keys': [{'column_id': 1}, {'column_id': 8}, {'column_id': 23}],\n",
       " 'db_foreign_keys': [{'column_id': 3, 'other_column_id': 8},\n",
       "  {'column_id': 23, 'other_column_id': 8}]}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmt_schema = format_spider_schema('world_1')\n",
    "fmt_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd267017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'concert_singer',\n",
       " 'table_names_original': ['stadium', 'singer', 'concert', 'singer_in_concert'],\n",
       " 'table_names': ['stadium', 'singer', 'concert', 'singer in concert'],\n",
       " 'column_names_original': [(-1, '*'),\n",
       "  (0, 'Stadium_ID'),\n",
       "  (0, 'Location'),\n",
       "  (0, 'Name'),\n",
       "  (0, 'Capacity'),\n",
       "  (0, 'Highest'),\n",
       "  (0, 'Lowest'),\n",
       "  (0, 'Average'),\n",
       "  (1, 'Singer_ID'),\n",
       "  (1, 'Name'),\n",
       "  (1, 'Country'),\n",
       "  (1, 'Song_Name'),\n",
       "  (1, 'Song_release_year'),\n",
       "  (1, 'Age'),\n",
       "  (1, 'Is_male'),\n",
       "  (2, 'concert_ID'),\n",
       "  (2, 'concert_Name'),\n",
       "  (2, 'Theme'),\n",
       "  (2, 'Stadium_ID'),\n",
       "  (2, 'Year'),\n",
       "  (3, 'concert_ID'),\n",
       "  (3, 'Singer_ID')],\n",
       " 'column_names': [(-1, '*'),\n",
       "  (0, 'stadium id'),\n",
       "  (0, 'location'),\n",
       "  (0, 'name'),\n",
       "  (0, 'capacity'),\n",
       "  (0, 'highest'),\n",
       "  (0, 'lowest'),\n",
       "  (0, 'average'),\n",
       "  (1, 'singer id'),\n",
       "  (1, 'name'),\n",
       "  (1, 'country'),\n",
       "  (1, 'song name'),\n",
       "  (1, 'song release year'),\n",
       "  (1, 'age'),\n",
       "  (1, 'is male'),\n",
       "  (2, 'concert id'),\n",
       "  (2, 'concert name'),\n",
       "  (2, 'theme'),\n",
       "  (2, 'stadium id'),\n",
       "  (2, 'year'),\n",
       "  (3, 'concert id'),\n",
       "  (3, 'singer id')],\n",
       " 'column_types': ['text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'others',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text'],\n",
       " 'primary_keys': [1, 8, 15, 20],\n",
       " 'foreign_keys': [[18, 1], [21, 8], [20, 15]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_cache['concert_singer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fb019ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '*'),\n",
       " (1, 'Stadium_ID'),\n",
       " (2, 'Location'),\n",
       " (3, 'Name'),\n",
       " (4, 'Capacity'),\n",
       " (5, 'Highest'),\n",
       " (6, 'Lowest'),\n",
       " (7, 'Average'),\n",
       " (8, 'Singer_ID'),\n",
       " (9, 'Name'),\n",
       " (10, 'Country'),\n",
       " (11, 'Song_Name'),\n",
       " (12, 'Song_release_year'),\n",
       " (13, 'Age'),\n",
       " (14, 'Is_male'),\n",
       " (15, 'concert_ID'),\n",
       " (16, 'concert_Name'),\n",
       " (17, 'Theme'),\n",
       " (18, 'Stadium_ID'),\n",
       " (19, 'Year'),\n",
       " (20, 'concert_ID'),\n",
       " (21, 'Singer_ID')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(fmt_schema['db_column_names']['column_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c2756",
   "metadata": {},
   "source": [
    "#### New loading from nested dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb67c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsp_data_dir = \"/Users/mac/Desktop/syt/Deep-Learning/Repos/Google-Research-Language/language/language/xsp/data\"\n",
    "\n",
    "spider_tables_path = os.path.join(xsp_data_dir, 'spider', 'tables.json')\n",
    "\n",
    "spider_dbs_dict = spider_preprocessing.load_spider_tables(spider_tables_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb49f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['perpetrator', 'college_2', 'flight_company', 'icfp_1', 'body_builder', 'storm_record', 'pilot_record', 'race_track', 'academic', 'department_store', 'music_4', 'insurance_fnol', 'cinema', 'decoration_competition', 'phone_market', 'store_product', 'assets_maintenance', 'student_assessment', 'dog_kennels', 'music_1', 'company_employee', 'farm', 'solvency_ii', 'city_record', 'swimming', 'flight_2', 'election', 'manufactory_1', 'debate', 'network_2', 'local_govt_in_alabama', 'climbing', 'e_learning', 'scientist_1', 'ship_1', 'entertainment_awards', 'allergy_1', 'imdb', 'products_for_hire', 'candidate_poll', 'chinook_1', 'flight_4', 'pets_1', 'dorm_1', 'journal_committee', 'flight_1', 'medicine_enzyme_interaction', 'local_govt_and_lot', 'station_weather', 'shop_membership', 'driving_school', 'concert_singer', 'music_2', 'sports_competition', 'railway', 'inn_1', 'museum_visit', 'browser_web', 'baseball_1', 'architecture', 'csu_1', 'tracking_orders', 'insurance_policies', 'gas_company', 'e_government', 'school_bus', 'machine_repair', 'theme_gallery', 'film_rank', 'party_people', 'hospital_1', 'customers_campaigns_ecommerce', 'gymnast', 'restaurants', 'mountain_photos', 'battle_death', 'cre_Doc_Control_Systems', 'tracking_share_transactions', 'apartment_rentals', 'student_transcripts_tracking', 'cre_Docs_and_Epenses', 'ship_mission', 'company_office', 'tracking_software_problems', 'products_gen_characteristics', 'coffee_shop', 'riding_club', 'customers_card_transactions', 'county_public_safety', 'performance_attendance', 'club_1', 'singer', 'culture_company', 'cre_Doc_Template_Mgt', 'musical', 'world_1', 'device', 'tracking_grants_for_research', 'employee_hire_evaluation', 'movie_1', 'network_1', 'poker_player', 'program_share', 'aircraft', 'restaurant_1', 'customers_and_invoices', 'insurance_and_eClaims', 'college_1', 'local_govt_mdm', 'book_2', 'hr_1', 'soccer_1', 'sakila_1', 'real_estate_properties', 'college_3', 'course_teach', 'roller_coaster', 'customer_deliveries', 'game_injury', 'school_finance', 'scholar', 'voter_1', 'match_season', 'small_bank_1', 'wta_1', 'yelp', 'student_1', 'manufacturer', 'store_1', 'train_station', 'document_management', 'formula_1', 'game_1', 'loan_1', 'bike_1', 'entrepreneur', 'orchestra', 'cre_Drama_Workshop_Groups', 'car_1', 'geo', 'behavior_monitoring', 'cre_Doc_Tracking_DB', 'university_basketball', 'soccer_2', 'activity_1', 'cre_Theme_park', 'twitter_1', 'election_representative', 'voter_2', 'wedding', 'news_report', 'wine_1', 'customers_and_addresses', 'protein_institute', 'school_player', 'phone_1', 'tvshow', 'wrestler', 'customer_complaints', 'department_management', 'customers_and_products_contacts', 'company_1', 'workshop_paper', 'epinions_1', 'party_host', 'product_catalog'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spider_dbs_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6def22a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stadium': [{'field name': 'Stadium_ID',\n",
       "   'is primary key': True,\n",
       "   'is foreign key': True,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'Location',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Name',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Capacity',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'Highest',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'Lowest',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'Average',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'number'}],\n",
       " 'singer': [{'field name': 'Singer_ID',\n",
       "   'is primary key': True,\n",
       "   'is foreign key': True,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'Name',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Country',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Song_Name',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Song_release_year',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Age',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'Is_male',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'others'}],\n",
       " 'concert': [{'field name': 'concert_ID',\n",
       "   'is primary key': True,\n",
       "   'is foreign key': True,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'concert_Name',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Theme',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Stadium_ID',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': True,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Year',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'}],\n",
       " 'singer_in_concert': [{'field name': 'concert_ID',\n",
       "   'is primary key': True,\n",
       "   'is foreign key': True,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'Singer_ID',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': True,\n",
       "   'type': 'text'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spider_dbs_dict['concert_singer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94d1f104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def db_dict_to_fmt_schema(db_dict):\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#         db_dict: Dict[table_name, List[column_dict[\"field name\", \"is primary key\", \"is foreign key\", \"type\"]]]\n",
    "    \n",
    "#     Output:\n",
    "#         fmt_schema = (for reference) {\n",
    "#             \"db_table_names\": schema[\"table_names_original\"],\n",
    "#             \"db_column_names\": {\n",
    "#                 \"table_id\": [table_id for table_id, column_name in schema[\"column_names_original\"]],\n",
    "#                 \"column_name\": [column_name for table_id, column_name in schema[\"column_names_original\"]]\n",
    "#             },\n",
    "#             \"db_column_types\": schema[\"column_types\"],\n",
    "#             \"db_primary_keys\": [{\"column_id\": column_id} for column_id in schema[\"primary_keys\"]],\n",
    "#             \"db_foreign_keys\": [\n",
    "#                 {\"column_id\": column_id, \"other_column_id\": other_column_id}\n",
    "#                 for column_id, other_column_id in schema[\"foreign_keys\"]\n",
    "#             ],\n",
    "#         }\n",
    "#     \"\"\"\n",
    "\n",
    "#     db_table_names = []\n",
    "#     db_column_names = ['*']  # default values in spider, same below \n",
    "#     db_column_table_ids = [-1]\n",
    "#     db_column_types = ['text']\n",
    "#     db_primary_keys = []\n",
    "#     db_foreign_keys = []\n",
    "    \n",
    "#     # for a column name, find the primary key idx. If this is a foreign key and not primary key, then found a f-p pair\n",
    "#     # (assume the f-p pair have the same name)\n",
    "#     # This is not always true. Example DB: architecture::architect_id, college_1::PROF_NUM\n",
    "#     col_name2p_key_column_idx = dict()\n",
    "#     col_name2f_keys_column_idx = defaultdict(list)\n",
    "    \n",
    "#     for table_name, table_columns in db_dict.items():\n",
    "#         table_idx = len(db_table_names)\n",
    "#         db_table_names.append(table_name)\n",
    "        \n",
    "#         for col_dict in table_columns:\n",
    "#             col_idx = len(db_column_names)\n",
    "#             db_column_names.append(col_dict[\"field name\"])\n",
    "#             db_column_table_ids.append(table_idx)\n",
    "#             db_column_types.append(col_dict[\"type\"])\n",
    "            \n",
    "#             # in michigan datasets, \"is primary||foreign key\" is \"y\"/\"n\"; in spider and wikisql, it is true/false\n",
    "#             if col_dict[\"is primary key\"] in {True, 'y'}:\n",
    "#                 db_primary_keys.append({\"column_id\": col_idx})\n",
    "#                 if col_dict[\"field name\"] in col_name2p_key_column_idx:\n",
    "#                     ## already exists?? Yes, there could be f-p pairs where both are primary! \n",
    "#                     # print(f'Warning: {col_dict[\"field name\"]} already exists')\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     col_name2p_key_column_idx[col_dict[\"field name\"]] = col_idx\n",
    "            \n",
    "#             if col_dict[\"is foreign key\"] in {True, 'y'}:\n",
    "#                 col_name2f_keys_column_idx[col_dict[\"field name\"]].append(col_idx)\n",
    "        \n",
    "#     for col_name in col_name2f_keys_column_idx.keys():\n",
    "#         f_key_column_ids = col_name2f_keys_column_idx[col_name]\n",
    "#         if col_name in col_name2p_key_column_idx:\n",
    "#             p_key_column_idx = col_name2p_key_column_idx[col_name]\n",
    "#         else:\n",
    "#             print(f'Warning: {col_name}, no primary key found')\n",
    "#             continue\n",
    "            \n",
    "#         for f_key_column_idx in f_key_column_ids:\n",
    "#             if f_key_column_idx != p_key_column_idx:\n",
    "#                 db_foreign_keys.append({\"column_id\": f_key_column_idx, \"other_column_id\": p_key_column_idx})\n",
    "    \n",
    "#     return {\n",
    "#         \"db_table_names\": db_table_names,\n",
    "#         \"db_column_names\": {\n",
    "#             \"table_id\": db_column_table_ids,\n",
    "#             \"column_name\": db_column_names,\n",
    "#         },\n",
    "#         \"db_column_types\": db_column_types,\n",
    "#         \"db_primary_keys\": db_primary_keys,\n",
    "#         \"db_foreign_keys\": db_foreign_keys\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abb251b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_fmt_dict_to_uskg_schema(general_fmt_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        general_fmt_dict (Dict): {\n",
    "            \"db_id\": str\n",
    "            \"table_names_original\": List[str], original table name (concert_singer)\n",
    "            \"table_names_clean\": List[str], clean table names (concert_singer)\n",
    "            \"column_names_original\": List[str], original column name (singer_id)\n",
    "            \"column_names_clean\": List[str], clean columns names (singer id)\n",
    "            \"column_db_full_names\": List[str], name of table::column in DB (may differ from column_names) (singer::singer_id)\n",
    "            \"column_table_ids\": List[int], for each column, the corresponding table index\n",
    "            \"column_types\": List[str], column types\n",
    "            \"primary_keys\": List[int], the columns indices that are primary key\n",
    "            \"foreign_keys\": List[[int, int]], the f-p column index pairs (fk_id, pk_id)\n",
    "            \"sqlite_path\": str\n",
    "            \"sqlite_conn\": sqlite3.Connection\n",
    "        }\n",
    "    \n",
    "    Output:\n",
    "        uskg_schema = (for reference) {\n",
    "            \"db_table_names\": schema[\"table_names_original\"],\n",
    "            \"db_column_names\": {\n",
    "                \"table_id\": [table_id for table_id, column_name in schema[\"column_names_original\"]],\n",
    "                \"column_name\": [column_name for table_id, column_name in schema[\"column_names_original\"]]\n",
    "            },\n",
    "            \"db_column_types\": schema[\"column_types\"],\n",
    "            \"db_primary_keys\": [{\"column_id\": column_id} for column_id in schema[\"primary_keys\"]],\n",
    "            \"db_foreign_keys\": [\n",
    "                {\"column_id\": column_id, \"other_column_id\": other_column_id}\n",
    "                for column_id, other_column_id in schema[\"foreign_keys\"]\n",
    "            ],\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    db_id = general_fmt_dict[\"db_id\"]\n",
    "    db_table_orig_names = general_fmt_dict[\"table_names_original\"]\n",
    "    db_table_clean_names = general_fmt_dict[\"table_names_clean\"]\n",
    "    db_column_orig_names = general_fmt_dict[\"column_names_original\"]\n",
    "    db_column_clean_names = general_fmt_dict[\"column_names_clean\"]\n",
    "    col_db_full_names = general_fmt_dict[\"column_db_full_names\"]\n",
    "    db_column_table_ids = general_fmt_dict[\"column_table_ids\"]\n",
    "    db_column_types = general_fmt_dict[\"column_types\"]\n",
    "    db_primary_keys = general_fmt_dict[\"primary_keys\"]\n",
    "    db_foreign_keys = general_fmt_dict[\"foreign_keys\"]\n",
    "    sqlite_path = general_fmt_dict[\"sqlite_path\"]\n",
    "    sqlite_conn = general_fmt_dict[\"sqlite_conn\"]\n",
    "    \n",
    "    # USKG specific\n",
    "    uskg_primary_keys = [{\"column_id\": col_idx} for col_idx in db_primary_keys]\n",
    "    uskg_foreign_keys = [{\"column_id\": fk_idx, \"other_column_id\": pk_idx} for fk_idx, pk_idx in db_foreign_keys]\n",
    "\n",
    "    uskg_schema = {\n",
    "        \"db_id\": db_id,\n",
    "        \"db_table_names\": db_table_orig_names,\n",
    "        \"db_column_names\": {\n",
    "            \"table_id\": db_column_table_ids,\n",
    "            \"column_name\": db_column_orig_names,\n",
    "        },\n",
    "        \"db_column_types\": db_column_types,\n",
    "        \"db_primary_keys\": [{\"column_id\": column_id} for column_id in db_primary_keys],\n",
    "        \"db_foreign_keys\": [\n",
    "            {\"column_id\": column_id, \"other_column_id\": other_column_id}\n",
    "            for column_id, other_column_id in db_foreign_keys\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    return uskg_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c4fa067",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_id = 'architecture'\n",
    "db_dict = spider_dbs_dict[db_id]\n",
    "\n",
    "general_fmt_dict = db_dict_to_general_fmt(db_dict, db_id,\n",
    "                                          sqlite_path=f\"/Users/mac/Desktop/syt/Deep-Learning/Repos/Google-Research-Language/language/language/xsp/data/spider/database/{db_id}/{db_id}.sqlite\",\n",
    "                                          rigorous_foreign_key=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aebc084c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'architecture',\n",
       " 'table_names_original': ['architect', 'bridge', 'mill'],\n",
       " 'table_names_clean': ['architect', 'bridge', 'mill'],\n",
       " 'column_names_original': ['*',\n",
       "  'id',\n",
       "  'name',\n",
       "  'nationality',\n",
       "  'gender',\n",
       "  'architect_id',\n",
       "  'id',\n",
       "  'name',\n",
       "  'location',\n",
       "  'length_meters',\n",
       "  'length_feet',\n",
       "  'architect_id',\n",
       "  'id',\n",
       "  'location',\n",
       "  'name',\n",
       "  'type',\n",
       "  'built_year',\n",
       "  'notes'],\n",
       " 'column_names_clean': ['*',\n",
       "  'id',\n",
       "  'name',\n",
       "  'nationality',\n",
       "  'gender',\n",
       "  'architect id',\n",
       "  'id',\n",
       "  'name',\n",
       "  'location',\n",
       "  'length meters',\n",
       "  'length feet',\n",
       "  'architect id',\n",
       "  'id',\n",
       "  'location',\n",
       "  'name',\n",
       "  'type',\n",
       "  'built year',\n",
       "  'notes'],\n",
       " 'column_db_full_names': ['NONE::*',\n",
       "  'architect::id',\n",
       "  'architect::name',\n",
       "  'architect::nationality',\n",
       "  'architect::gender',\n",
       "  'bridge::architect_id',\n",
       "  'bridge::id',\n",
       "  'bridge::name',\n",
       "  'bridge::location',\n",
       "  'bridge::length_meters',\n",
       "  'bridge::length_feet',\n",
       "  'mill::architect_id',\n",
       "  'mill::id',\n",
       "  'mill::location',\n",
       "  'mill::name',\n",
       "  'mill::type',\n",
       "  'mill::built_year',\n",
       "  'mill::notes'],\n",
       " 'column_table_ids': [-1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2],\n",
       " 'column_types': ['text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text'],\n",
       " 'primary_keys': [1, 6, 12],\n",
       " 'foreign_keys': [[5, 1], [11, 1]],\n",
       " 'sqlite_path': '/Users/mac/Desktop/syt/Deep-Learning/Repos/Google-Research-Language/language/language/xsp/data/spider/database/architecture/architecture.sqlite',\n",
       " 'sqlite_conn': <sqlite3.Connection at 0x7fa6cdfd3b90>}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_fmt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e643554a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt_schema_2 = general_fmt_dict_to_uskg_schema(general_fmt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "592f9ec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'architecture',\n",
       " 'db_table_names': ['architect', 'bridge', 'mill'],\n",
       " 'db_column_names': {'table_id': [-1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2],\n",
       "  'column_name': ['*',\n",
       "   'id',\n",
       "   'name',\n",
       "   'nationality',\n",
       "   'gender',\n",
       "   'architect_id',\n",
       "   'id',\n",
       "   'name',\n",
       "   'location',\n",
       "   'length_meters',\n",
       "   'length_feet',\n",
       "   'architect_id',\n",
       "   'id',\n",
       "   'location',\n",
       "   'name',\n",
       "   'type',\n",
       "   'built_year',\n",
       "   'notes']},\n",
       " 'db_column_types': ['text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text'],\n",
       " 'db_primary_keys': [{'column_id': 1}, {'column_id': 6}, {'column_id': 12}],\n",
       " 'db_foreign_keys': [{'column_id': 5, 'other_column_id': 1},\n",
       "  {'column_id': 11, 'other_column_id': 1}]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmt_schema_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc117671",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fmt_schema = format_spider_schema(db_id)\n",
    "# del fmt_schema['db_id']\n",
    "del fmt_schema['db_path']\n",
    "\n",
    "fmt_schema == fmt_schema_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cb9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(fmt_schema[\"db_foreign_keys\"], key=lambda d: d['column_id']), sorted(fmt_schema_2[\"db_foreign_keys\"], key=lambda d: d['column_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cc7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate(fmt_schema['db_column_names']['column_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954ffc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f10218d5",
   "metadata": {},
   "source": [
    "### WikiSQL\n",
    "- Should be able to reuse db_dict_to_fmt_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "81d1f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisql_tables_path = os.path.join(xsp_data_dir, 'wikisql', 'dev.tables.jsonl')\n",
    "\n",
    "wikisql_dbs_dict = wikisql_preprocessing.load_wikisql_tables(wikisql_tables_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "57a317fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['RACE_RESULTS']),\n",
       " [{'field name': 'RD',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'RACE',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'POLE_POSITION',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'FASTEST_LAP',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'MOST_LAPS_LED',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'WINNING_DRIVER',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'WINNING_TEAM',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'}])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikisql_dbs_dict['1-29690363-3'].keys(), wikisql_dbs_dict['1-29690363-3']['RACE_RESULTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9b107a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_table_names': ['RACE_RESULTS'],\n",
       " 'db_column_names': {'table_id': [-1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  'column_name': ['*',\n",
       "   'RD',\n",
       "   'RACE',\n",
       "   'POLE_POSITION',\n",
       "   'FASTEST_LAP',\n",
       "   'MOST_LAPS_LED',\n",
       "   'WINNING_DRIVER',\n",
       "   'WINNING_TEAM']},\n",
       " 'db_column_types': ['text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text'],\n",
       " 'db_primary_keys': [],\n",
       " 'db_foreign_keys': []}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_dict_to_fmt_schema(wikisql_dbs_dict['1-29690363-3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7843dea",
   "metadata": {},
   "source": [
    "### Michigan\n",
    "- Should be able to reuse db_dict_to_fmt_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a92cb06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "atis_schema_path = os.path.join(xsp_data_dir, 'atis', 'atis_schema.csv')\n",
    "\n",
    "atis_db_dict = michigan_preprocessing.read_schema(atis_schema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3393c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "atis_db_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aea615",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db_dict_to_fmt_schema(atis_db_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6557e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20ffa822",
   "metadata": {},
   "source": [
    "## Get USKG encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03558b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#     - set up inference (for sanity check)\n",
    "#     - look into source (of transformers Bert.generate) to find ways to get the encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b89133",
   "metadata": {},
   "source": [
    "### Check USKG inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa40a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_pred(txt, model, tokenizer):\n",
    "    tokenized_txt = tokenizer([txt], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "    pred = tokenizer.batch_decode(\n",
    "      model.generate(\n",
    "        torch.LongTensor(tokenized_txt.data['input_ids']),\n",
    "        torch.LongTensor(tokenized_txt.data['attention_mask']),\n",
    "        num_beams=1, \n",
    "        max_length=256\n",
    "        ), \n",
    "      skip_special_tokens=True \n",
    "    )\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "41a9371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set args here for runnning on notebook, we make them out here to make it more illustrative.\n",
    "sys.argv = ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', # This is the name of your .py launcher when you run this line of code.\n",
    "            # belows are the parameters we set, take spider for example\n",
    "            '--cfg', 'Salesforce/T5_large_prefix_spider_with_cell_value.cfg', \n",
    "            '--output_dir', './tmp']\n",
    "parser = HfArgumentParser((WrappedSeq2SeqTrainingArguments,))\n",
    "training_args, = parser.parse_args_into_dataclasses()\n",
    "set_seed(training_args.seed)\n",
    "args = Configure.Get(training_args.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dff954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix-tuning sequence length is 10.\n"
     ]
    }
   ],
   "source": [
    "model_path = 'hkunlp/from_all_T5_large_prefix_spider_with_cell_value2'\n",
    "# model_path = 'hkunlp/from_all_T5_large_prefix_spider_with_cell_value2'\n",
    "# model_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/UnifiedSKG/output/server_runs/A-T5_base_prefix_spider_with_cell_value-asr_mixed/checkpoint-79500/'\n",
    "# model_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/UnifiedSKG/output/server_runs/A-T5_base_prefix_spider_with_cell_value-rewritten_mixed/checkpoint-56500/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "\n",
    "# for reconstruction\n",
    "tokenizer_fast = AutoTokenizer.from_pretrained('t5-base', use_fast=True)\n",
    "\n",
    "model = Model(args)\n",
    "model.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "83b25043",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_large_fast = AutoTokenizer.from_pretrained('t5-large', use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5adbe3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select min(age), avg(age), max(age) from singer where country = \"France\"']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_in = \"| concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country ( France ) , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id\"\n",
    "text_in = \"what is the minimum, average, and maximum age of all singers from France?\"\n",
    "\n",
    "play_pred(\"{}; structed knowledge: {}\".format(text_in, struct_in), model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "76c19f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select min(age), avg(age), max(age) from singer where country = \"France\"']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_pred(\"{}; structed knowledge: {}\".format(text_in, struct_in), model, tokenizer_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c802f",
   "metadata": {},
   "source": [
    "### Check USKG inference on dataset samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eea44382",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsp_data_dir = \"/Users/mac/Desktop/syt/Deep-Learning/Repos/Google-Research-Language/language/language/xsp/data\"\n",
    "\n",
    "orig_dataset_path = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/spider/dev+ratsql_graph.json\"\n",
    "orig_tables_path = \"/Users/mac/Desktop/syt/Deep-Learning/Repos/Google-Research-Language/language/language/xsp/data/spider/tables.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ab0d32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spider_dbs_dict = spider_preprocessing.load_spider_tables(orig_tables_path)\n",
    "len(spider_dbs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9f09dd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: (db_id: imdb) keyword::kid -- tags::kid pair invalid, skipped\n",
      "Warning: (db_id: baseball_1) player::team_id -- fielding_postseason::team_id pair invalid, skipped\n",
      "Warning: (db_id: restaurants) restaurant::restaurant_id -- location::restaurant_id pair invalid, skipped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uskg_schemas_dict = dict()\n",
    "for db_id, db_dict in spider_dbs_dict.items():\n",
    "    general_fmt_dict = db_dict_to_general_fmt(db_dict, db_id,\n",
    "                                              sqlite_path=os.path.join(xsp_data_dir, f\"spider/database/{db_id}/{db_id}.sqlite\"),\n",
    "                                              rigorous_foreign_key=True)\n",
    "    \n",
    "    uskg_schema = general_fmt_dict_to_uskg_schema(general_fmt_dict)\n",
    "    \n",
    "    uskg_schemas_dict[db_id] = uskg_schema\n",
    "    \n",
    "len(uskg_schemas_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8138b0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(orig_dataset_path, 'r') as f:\n",
    "    orig_dataset = json.load(f)\n",
    "\n",
    "for d in orig_dataset:\n",
    "    d['rat_sql_graph']['relations'] = json.loads(d['rat_sql_graph']['relations'])\n",
    "\n",
    "len(orig_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0d8d6c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql', 'rat_sql_graph'])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5b47a73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPIDER_DB_PATH = os.path.join(xsp_data_dir, \"spider/database\")\n",
    "\n",
    "def uskg_sample_to_struct_input(uskg_sample):\n",
    "    db_id = uskg_sample[\"db_id\"]\n",
    "    uskg_schema = uskg_schemas_dict[db_id]\n",
    "    \n",
    "    return spider.serialize_schema(\n",
    "        question=uskg_sample[\"question\"],\n",
    "        db_path=SPIDER_DB_PATH,\n",
    "        db_id=db_id,\n",
    "        db_column_names=uskg_schema[\"db_column_names\"],\n",
    "        db_table_names=uskg_schema[\"db_table_names\"],\n",
    "        schema_serialization_type=\"peteshaw\",\n",
    "        schema_serialization_randomized=False,\n",
    "        schema_serialization_with_db_id=True,\n",
    "        schema_serialization_with_db_content=True,\n",
    "        normalize_query=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d262ee17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql', 'rat_sql_graph']),\n",
       " dict_keys(['nodes', 'q_nodes_orig', 'relations']))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 777\n",
    "sample = orig_dataset[idx]\n",
    "sample.keys(), sample['rat_sql_graph'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "be679956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What are the Asian countries which have a population larger than that of any country in Africa?',\n",
       " ' | world_1 | city : id , name , countrycode , district , population | sqlite_sequence : name , seq | country : code , name , continent ( Africa , Asia ) , region , surfacearea , indepyear , population , lifeexpectancy , gnp , gnpold , localname , governmentform , headofstate , capital , code2 | countrylanguage : countrycode , language , isofficial , percentage')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_in = sample['question']\n",
    "struct_in = uskg_sample_to_struct_input(sample)\n",
    "text_in, struct_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ffcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_pred(\"{}; structed knowledge: {}\".format(text_in, struct_in), model, tokenizer_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550b2aef",
   "metadata": {},
   "source": [
    "### Tokenized pieces-nodes mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d7c17303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (done):\n",
    "#     find mapping between tokenized pieces to ratsql nodes\n",
    "#         ratsql nodes to sentence char ids (done)\n",
    "#         sentence char ids to tokenized pieces (done)\n",
    "#     pool the encodings of pieces in a node (done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8cf38429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructCharRangesCollector:\n",
    "    def __init__(self):\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        self.db_id2char_ranges = dict()\n",
    "        self.table2char_ranges = dict()\n",
    "        self.column2char_ranges = dict()\n",
    "        \n",
    "        # Due to rat-sql stemming tokens, rat-sql nodes and uskg text may mismatch,\n",
    "        # so we save a list and use the order instead of name for indexing \n",
    "        self.db_id_char_ranges_list = []\n",
    "        self.column_char_ranges_list = []\n",
    "        self.table_char_ranges_list = []\n",
    "\n",
    "        self.bar_cnt = 0\n",
    "        self.curr_table = None\n",
    "        self.curr_node_type = None   # [None, 'db_id', 'table', 'column']\n",
    "        self.curr_node_toks = []\n",
    "        self.curr_node_char_start = None\n",
    "        self.curr_node_char_end = None\n",
    "        self.open_bracket = False\n",
    "        \n",
    "    def _register_curr_node(self):\n",
    "        curr_node_name = ' '.join(self.curr_node_toks)\n",
    "        curr_range = (self.curr_node_char_start, self.curr_node_char_end)\n",
    "        \n",
    "        if self.curr_node_type == 'db_id':\n",
    "            self.db_id2char_ranges[curr_node_name] = curr_range\n",
    "            self.db_id_char_ranges_list.append(curr_range)   \n",
    "        elif self.curr_node_type == 'table':\n",
    "            self.table2char_ranges[curr_node_name] = curr_range\n",
    "            self.table_char_ranges_list.append(curr_range)\n",
    "            self.curr_table = curr_node_name\n",
    "        elif self.curr_node_type == 'column':\n",
    "            self.column2char_ranges[(self.curr_table, curr_node_name)] = curr_range\n",
    "            self.column_char_ranges_list.append(curr_range)\n",
    "        else:\n",
    "            raise ValueError(curr_node_type)\n",
    "\n",
    "        self.curr_node_toks = []\n",
    "        self.curr_node_char_start = None\n",
    "        self.curr_node_char_end = None\n",
    "    \n",
    "    def collect(self, struct_in, tokenized_txt, _n_words_before_struct):\n",
    "#         struct_words = struct_in.strip().split(' ')\n",
    "        struct_words = struct_in.strip().split()\n",
    "        \n",
    "        for sw_id, sw in enumerate(struct_words):\n",
    "            char_range = tokenized_txt.word_to_chars(sw_id + _n_words_before_struct)\n",
    "\n",
    "            # print(sw_id, char_range, sw, self.curr_node_type, self.open_bracket)\n",
    "\n",
    "            if sw == '(':\n",
    "                self.open_bracket = True\n",
    "                continue\n",
    "\n",
    "            if sw == ')':\n",
    "                self.open_bracket = False\n",
    "                self.curr_node_char_end = char_range[1]\n",
    "                continue\n",
    "\n",
    "            if self.open_bracket:\n",
    "                # in the list of cells, do not add tokens here to name \n",
    "                continue\n",
    "\n",
    "            if sw == '|':\n",
    "                if self.curr_node_type is not None:\n",
    "                    self._register_curr_node()\n",
    "                self.bar_cnt += 1\n",
    "                if self.bar_cnt == 1:\n",
    "                    self.curr_node_type = 'db_id'\n",
    "                if self.bar_cnt > 1:\n",
    "                    self.curr_node_type = 'table'\n",
    "                continue\n",
    "\n",
    "            if sw == ':':\n",
    "                assert self.curr_node_type == 'table'\n",
    "                self._register_curr_node()\n",
    "                self.curr_node_type = 'column'\n",
    "                continue\n",
    "\n",
    "            if sw == ',':\n",
    "                assert self.curr_node_type == 'column'\n",
    "                self._register_curr_node()\n",
    "                self.curr_node_type = 'column'\n",
    "                continue\n",
    "\n",
    "            self.curr_node_toks.append(sw)\n",
    "            if self.curr_node_char_start is None:\n",
    "                self.curr_node_char_start = char_range[0]\n",
    "            self.curr_node_char_end = char_range[1]\n",
    "\n",
    "        self._register_curr_node()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7154db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine experiment codes \n",
    "def collect_node_char_ranges(sample, tokenizer=None, tokenizer_args=None, txt=None, tokenized_txt=None, debug=False):\n",
    "    text_in = sample['question']\n",
    "    struct_in = uskg_sample_to_struct_input(sample)\n",
    "    _splitter = \"; structed knowledge: \"\n",
    "    \n",
    "    if tokenizer_args is None:\n",
    "        tokenizer_args = dict()\n",
    "        \n",
    "    if txt is None:\n",
    "        txt = \"{}{}{}\".format(text_in, _splitter, struct_in)\n",
    "    \n",
    "    if tokenized_txt is None:\n",
    "        # tokenized_txt = tokenizer([txt], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "        tokenized_txt = tokenizer([txt], **tokenizer_args)\n",
    "        ## possible problem: exceeding max length!\n",
    "    \n",
    "    ratsql_graph_nodes = sample['rat_sql_graph']['nodes']\n",
    "#     question_toks = sample['question_toks']\n",
    "    question_toks = sample['rat_sql_graph']['q_nodes_orig']\n",
    "\n",
    "    _q_nodes = []  # [stem token (node name)]\n",
    "    q_nodes = []  # [(stem token (node name), orig question token)]\n",
    "    c_nodes = []  # [(orig table name, orig column name)]\n",
    "    t_nodes = []  # [orig table name]\n",
    "\n",
    "    for n in ratsql_graph_nodes:\n",
    "        if n.startswith('<C>'):\n",
    "            _n = n[3:]\n",
    "            _t, _c = _n.split('::')\n",
    "            c_nodes.append((_t, _c))\n",
    "        elif n.startswith('<T>'):\n",
    "            _n = n[3:]\n",
    "            t_nodes.append(_n)\n",
    "        else:\n",
    "            _q_nodes.append(n)\n",
    "\n",
    "    assert len(_q_nodes) == len(question_toks), (_q_nodes, question_toks)\n",
    "    q_nodes = list(zip(_q_nodes, question_toks))\n",
    "    \n",
    "    # Collection char ranges \n",
    "    q_node_chars = []   # [(st, ed)]; same below\n",
    "    c_node_chars = []\n",
    "    t_node_chars = []\n",
    "    \n",
    "    # Text part\n",
    "    # Assumption: the mismatch between whitespace words (text_words) and question words only come from trailing puncts\n",
    "    # Currently the code can handle combining question toks into whitespace words\n",
    "#     text_words = text_in.strip().split(' ') + ['<SENTINAL>']\n",
    "    text_words = text_in.lower().strip().split() + ['<SENTINAL>']\n",
    "    text_word_char_ranges = [tokenized_txt.word_to_chars(i) for i in range(len(text_words) - 1)] + [(None, None)]  # -1 to remove the sentinal \n",
    "\n",
    "    curr_tw_idx = 0\n",
    "    curr_tw = text_words[0]\n",
    "    curr_tw_char_range = text_word_char_ranges[0]\n",
    "    curr_char_ptr = 0\n",
    "    for stem_tok, orig_tok in q_nodes:\n",
    "        if curr_tw == orig_tok:\n",
    "            # finishing current word \n",
    "            q_node_chars.append((curr_char_ptr, curr_char_ptr + len(orig_tok)))   # curr pos to curr pos + len \n",
    "            curr_tw_idx += 1\n",
    "            curr_tw = text_words[curr_tw_idx]\n",
    "            curr_tw_char_range = text_word_char_ranges[curr_tw_idx]\n",
    "            curr_char_ptr = curr_tw_char_range[0]\n",
    "        else:\n",
    "            # not finishing current word \n",
    "            assert curr_tw.startswith(orig_tok), (curr_tw, orig_tok)\n",
    "            q_node_chars.append((curr_char_ptr, curr_char_ptr + len(orig_tok)))   # curr pos to curr pos + len \n",
    "            curr_char_ptr += len(orig_tok)     # move ptr forward by len \n",
    "            curr_tw = curr_tw[len(orig_tok):]  # get the remaining chars in the word \n",
    "\n",
    "    assert [txt[st:ed].lower() for st, ed in q_node_chars] == question_toks, ([txt[st:ed] for st, ed in q_node_chars], question_toks)\n",
    "    \n",
    "    # Struct part \n",
    "    _str_before_struct = text_in + _splitter\n",
    "    _n_words_before_struct = len(_str_before_struct.strip().split())\n",
    "\n",
    "    struct_ranges_collector = StructCharRangesCollector()\n",
    "    struct_ranges_collector.collect(struct_in, tokenized_txt, _n_words_before_struct)\n",
    "    \n",
    "    # Due to rat-sql stemming tokens, rat-sql nodes and uskg text may mismatch\n",
    "#     for c_node in c_nodes:\n",
    "#         if c_node == ('NONE', '*'):\n",
    "#             # the special column in spider, using db_id \n",
    "#             c_node_chars.append(list(struct_ranges_collector.db_id2char_ranges.values())[0])   # assuming only 1 db_id, which should be true...\n",
    "#         else:\n",
    "#             c_node_chars.append(struct_ranges_collector.column2char_ranges[c_node])\n",
    "\n",
    "#     for t_node in t_nodes:\n",
    "#         t_node_chars.append(struct_ranges_collector.table2char_ranges[t_node])\n",
    "\n",
    "    c_node_chars.extend(struct_ranges_collector.db_id_char_ranges_list + struct_ranges_collector.column_char_ranges_list)\n",
    "    t_node_chars.extend(struct_ranges_collector.table_char_ranges_list)\n",
    "\n",
    "    ## Check all \n",
    "    if debug:\n",
    "        for q_node, (st, ed) in zip(q_nodes, q_node_chars):\n",
    "            print(q_node, txt[st:ed])\n",
    "        print()\n",
    "        for t_node, (st, ed) in zip(t_nodes, t_node_chars):\n",
    "            print(t_node, txt[st:ed])\n",
    "        print()\n",
    "        for c_node, (st, ed) in zip(c_nodes, c_node_chars):\n",
    "            print(c_node, txt[st:ed])\n",
    "        \n",
    "    return {\n",
    "        \"q_node_chars\": q_node_chars,\n",
    "        \"c_node_chars\": c_node_chars,\n",
    "        \"t_node_chars\": t_node_chars,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4472180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 209\n",
    "sample = orig_dataset[idx]\n",
    "\n",
    "text_in = sample['question']\n",
    "struct_in = uskg_sample_to_struct_input(sample)\n",
    "\n",
    "txt = \"{}; structed knowledge: {}\".format(text_in, struct_in)\n",
    "\n",
    "tokenized_txt = tokenizer_fast([txt], max_length=1024, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ed41377e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('how', 'how') How\n",
      "('many', 'many') many\n",
      "('flight', 'flights') flights\n",
      "('arrive', 'arriving') arriving\n",
      "('in', 'in') in\n",
      "('aberdeen', 'aberdeen') Aberdeen\n",
      "('city', 'city') city\n",
      "('?', '?') ?\n",
      "\n",
      "airline airlines\n",
      "airport airports\n",
      "flight flights\n",
      "\n",
      "('NONE', '*') flight_2\n",
      "('airline', 'uid') uid\n",
      "('airline', 'airline') airline\n",
      "('airline', 'abbreviation') abbreviation\n",
      "('airline', 'country') country\n",
      "('airport', 'city') city ( Aberdeen  )\n",
      "('airport', 'airportcode') airportcode\n",
      "('airport', 'airportname') airportname\n",
      "('airport', 'country') country\n",
      "('airport', 'countryabbrev') countryabbrev\n",
      "('flight', 'airline') airline\n",
      "('flight', 'flightno') flightno\n",
      "('flight', 'sourceairport') sourceairport\n",
      "('flight', 'destairport') destairport\n"
     ]
    }
   ],
   "source": [
    "# tokenizer_args = {\n",
    "#     \"max_length\": 1024,\n",
    "#     \"padding\": \"max_length\",\n",
    "#     \"truncation\": True\n",
    "# }\n",
    "\n",
    "# char_ranges_dict = collect_node_char_ranges(sample, tokenizer=tokenizer_fast, tokenizer_args=tokenizer_args, debug=True)\n",
    "char_ranges_dict = collect_node_char_ranges(sample, txt=txt, tokenized_txt=tokenized_txt, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "eebdedf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How many flights arriving in Aberdeen city?; structed knowledge:  | flight_2 | airlines : uid , airline , abbreviation , country | airports : city ( Aberdeen  ) , airportcode , airportname , country , countryabbrev | flights : airline , flightno , sourceairport , destairport'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3e9b3adf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_node_chars': [(0, 3),\n",
       "  (4, 8),\n",
       "  (9, 16),\n",
       "  (17, 25),\n",
       "  (26, 28),\n",
       "  (29, 37),\n",
       "  (38, 42),\n",
       "  (42, 43)],\n",
       " 'c_node_chars': [(68, 76),\n",
       "  (90, 93),\n",
       "  (96, 103),\n",
       "  (106, 118),\n",
       "  (121, 128),\n",
       "  (142, 160),\n",
       "  (163, 174),\n",
       "  (177, 188),\n",
       "  (191, 198),\n",
       "  (201, 214),\n",
       "  (227, 234),\n",
       "  (237, 245),\n",
       "  (248, 261),\n",
       "  (264, 275)],\n",
       " 't_node_chars': [(79, 87), (131, 139), (217, 224)]}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_ranges_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f58bd7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_txt.char_to_token(79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "71058a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁airlines'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_txt.tokens()[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "a967a0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tokenized_txt.data['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41bf0f8",
   "metadata": {},
   "source": [
    "### Get encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "16ca7ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_USKG_node_encodings(sample, model, tokenizer, tokenizer_args=None, pooling_func=None, debug=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pooling_func (Callable): np.array(n_pieces, dim) ==> np.array(dim,); default is np.mean\n",
    "    \"\"\"\n",
    "    text_in = sample['question']\n",
    "    struct_in = uskg_sample_to_struct_input(sample)\n",
    "    \n",
    "    _splitter = \"; structed knowledge: \"\n",
    "    txt = \"{}{}{}\".format(text_in, _splitter, struct_in)\n",
    "\n",
    "    if tokenizer_args is None:\n",
    "        tokenizer_args = {\n",
    "            \"max_length\": 1024,\n",
    "            \"padding\": \"max_length\",\n",
    "            \"truncation\": True\n",
    "        }\n",
    "    if pooling_func is None:\n",
    "        pooling_func = lambda l: np.mean(l, axis=0)\n",
    "        \n",
    "    tokenized_txt = tokenizer([txt], **tokenizer_args)\n",
    "    \n",
    "    # Get encoding tensor \n",
    "    with torch.no_grad():\n",
    "        past_prompt = model.get_prompt(\n",
    "            bsz=1,              # bsz = input_ids.shape[0]\n",
    "            sample_size=1,      # sample_size=kwargs['num_beams']\n",
    "            description=None,   \n",
    "            knowledge=None,     \n",
    "        )\n",
    "        encoder_outputs = model.pretrain_model.encoder(\n",
    "            input_ids=torch.LongTensor(tokenized_txt.data['input_ids']),\n",
    "            attention_mask=torch.LongTensor(tokenized_txt.data['attention_mask']),\n",
    "            past_prompt=past_prompt,\n",
    "        )\n",
    "    encoder_output_hidden_states = encoder_outputs.last_hidden_state.detach().squeeze(0).cpu().numpy()\n",
    "    if debug:\n",
    "        print('encoder_output_hidden_states:', encoder_output_hidden_states.shape)\n",
    "    \n",
    "    # Get node-pieces mapping via char ranges \n",
    "    char_ranges_dict = collect_node_char_ranges(sample, txt=txt, tokenized_txt=tokenized_txt)\n",
    "    node_char_ranges = char_ranges_dict['q_node_chars'] + char_ranges_dict['c_node_chars'] + char_ranges_dict['t_node_chars']\n",
    "\n",
    "    # some chars can be mapped to multiple tokens (e.g. 'i' => '▁', 'i' )\n",
    "    char_to_tokens_dict = defaultdict(list)\n",
    "    for token_idx, tok in enumerate(tokenized_txt.tokens()):\n",
    "        if tok == '</s>':\n",
    "            break\n",
    "        char_span = tokenized_txt.token_to_chars(token_idx)\n",
    "        for char_idx in range(char_span[0], char_span[1]):\n",
    "            char_to_tokens_dict[char_idx].append(token_idx)\n",
    "    \n",
    "    node_pieces_ranges = []\n",
    "    for st, ed in node_char_ranges:\n",
    "        piece_ids = []\n",
    "        for char_idx in range(st, ed):\n",
    "            _piece_ids = char_to_tokens_dict[char_idx]\n",
    "            piece_ids.extend(_piece_ids)\n",
    "\n",
    "        piece_st = piece_ids[0]\n",
    "        piece_ed = piece_ids[-1] + 1\n",
    "        # the collected piece_ids should be continuous \n",
    "        # ^ not true... some chars can be mapped to multiple tokens (started by ▁ )\n",
    "        # re-collect a char-to-token\n",
    "        assert set(range(piece_st, piece_ed)) == set(piece_ids), piece_ids\n",
    "\n",
    "        node_pieces_ranges.append((piece_st, piece_ed))\n",
    "    \n",
    "    if debug:\n",
    "        print('node_pieces_ranges:', node_pieces_ranges)\n",
    "    \n",
    "    # Pool the encodings per node \n",
    "    node_encodings = []\n",
    "    for piece_st, piece_ed in node_pieces_ranges:\n",
    "        enc_vecs = encoder_output_hidden_states[piece_st : piece_ed]\n",
    "        enc_pooled = pooling_func(enc_vecs)\n",
    "        node_encodings.append(enc_pooled)\n",
    "    \n",
    "    return node_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e10d1069",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 209\n",
    "sample = orig_dataset[idx]\n",
    "\n",
    "text_in = sample['question']\n",
    "struct_in = uskg_sample_to_struct_input(sample)\n",
    "\n",
    "txt = \"{}; structed knowledge: {}\".format(text_in, struct_in)\n",
    "\n",
    "tokenized_txt = tokenizer_fast([txt], max_length=1024, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "43605386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_output_hidden_states: (1024, 1024)\n",
      "node_pieces_ranges: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (16, 19), (23, 27), (29, 30), (32, 36), (38, 39), (44, 49), (51, 53), (55, 57), (59, 60), (62, 66), (70, 71), (73, 76), (78, 81), (83, 86), (20, 21), (40, 42), (67, 68)]\n"
     ]
    }
   ],
   "source": [
    "node_encodings = get_USKG_node_encodings(sample, model, tokenizer_fast, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "31651288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_encodings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ec30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e22a5cd2",
   "metadata": {},
   "source": [
    "## Probing: link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ab8cc",
   "metadata": {},
   "source": [
    "### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "55b55a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_probing_samples_link_prediction_uskg(dataset_sample,\n",
    "                                                 db_schemas_dict,\n",
    "                                                 model,\n",
    "                                                 tokenizer,\n",
    "                                                 pos=None,\n",
    "                                                 max_rel_occ=None,\n",
    "                                                 debug=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset_sample (Dict): a sample dict from spider dataset\n",
    "        db_schemas_dict (Dict): db_id => db_schema, precomputed for all DBs (not used here)\n",
    "        model (EncDec): the rat-sql model\n",
    "        pos (List[Tuple]): the position pairs to use. If none, will randomly generate\n",
    "        max_rel_occ (int): each relation occur at most this many times in each (original) sample\n",
    "    \n",
    "    Return:\n",
    "        X (List[np.array]): input features, \"shape\" = (n, dim)\n",
    "        y (List): output labels, \"shape\" = (n,)\n",
    "        pos (List[Tuple]): actual position (node-id) pairs for X and y\n",
    "    \"\"\"\n",
    "    \n",
    "    d = dataset_sample\n",
    "    \n",
    "    db_id = d['db_id']\n",
    "    # db_schema = db_schemas_dict[db_id]\n",
    "    question = d['question']\n",
    "\n",
    "    # get relation matrix (relation_id2name not available as it needs rat-sql model)\n",
    "    graph_dict = dataset_sample['rat_sql_graph']\n",
    "    # graph_dict['relation_id2name'] = {v : k for k, v in model.encoder.encs_update.relation_ids.items()}\n",
    "    \n",
    "    # get encodings\n",
    "    # rat_sql_encoder_state = get_rat_sql_encoder_state(question=question, db_schema=db_schema, model=model)\n",
    "    # enc_repr = rat_sql_encoder_state.memory.squeeze(0).detach().cpu().numpy()\n",
    "    enc_repr = get_USKG_node_encodings(sample=dataset_sample,\n",
    "                                       model=model,\n",
    "                                       tokenizer=tokenizer,\n",
    "                                       debug=debug)\n",
    "    \n",
    "    X, y, pos = collect_link_prediction_samples(\n",
    "        graph_dict,\n",
    "        enc_repr,\n",
    "        pos=pos,\n",
    "        max_rel_occ=max_rel_occ,\n",
    "        debug=debug)\n",
    "    \n",
    "    return X, y, pos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "999e9b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "probing_data_dir = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/probing/text2sql/link_prediction/spider/ratsql\"\n",
    "\n",
    "orig_ds = 'dev'\n",
    "prob_ds = 'test'\n",
    "dataset_path = f\"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/spider/{orig_ds}+ratsql_graph.json\"\n",
    "\n",
    "pos_file_path = os.path.join(probing_data_dir, f'{orig_ds}.{prob_ds}.pos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "20dab972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034,\n",
       " dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql', 'rat_sql_graph']))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(dataset_path, 'r') as f:\n",
    "    orig_dataset = json.load(f)\n",
    "    \n",
    "for d in orig_dataset:\n",
    "    d['rat_sql_graph']['relations'] = json.loads(d['rat_sql_graph']['relations'])\n",
    "\n",
    "len(orig_dataset), orig_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "40e5a80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(pos_file_path, 'r') as f:\n",
    "    lines = f.read().strip().split('\\n')\n",
    "    all_pos_triplets = [tuple([int(s) for s in l.split('\\t')]) for l in lines]\n",
    "len(all_pos_triplets), all_pos_triplets[0]\n",
    "\n",
    "# Load pos file \n",
    "sample_ds_indices = []               # [ds_idx], based on occurring order \n",
    "pos_per_sample = defaultdict(list)   # key = ds_idx, value = pos_list: List[(i, j)]\n",
    "\n",
    "for ds_idx, i, j in all_pos_triplets:\n",
    "    if not sample_ds_indices or sample_ds_indices[-1] != ds_idx:\n",
    "        sample_ds_indices.append(ds_idx)\n",
    "    pos_per_sample[ds_idx].append((i, j))\n",
    "\n",
    "len(sample_ds_indices), len(pos_per_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f23befaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test loading pos file \n",
    "set(sample_ds_indices) == set(pos_per_sample.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2a562e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e44c87da274935a2f6704741ea450e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(16059, 16059, 16059)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: for each sample_ds_idx, get the sample & pos_list, get the X, y, pos; save. \n",
    "\n",
    "all_X = []\n",
    "all_y = []\n",
    "all_pos = []\n",
    "\n",
    "for sample_ds_idx in tqdm(sample_ds_indices):\n",
    "    dataset_sample = orig_dataset[sample_ds_idx]\n",
    "    pos_list = pos_per_sample[sample_ds_idx]\n",
    "\n",
    "    X, y, pos = extract_probing_samples_link_prediction_uskg(dataset_sample=dataset_sample,\n",
    "                                                             db_schemas_dict=None,\n",
    "                                                             model=model,\n",
    "                                                             tokenizer=tokenizer_fast,\n",
    "                                                             pos=pos_list,\n",
    "                                                             max_rel_occ=None,  # when given pos, this is not needed \n",
    "                                                             debug=False)\n",
    "    \n",
    "    all_X.extend(X)\n",
    "    all_y.extend(y)\n",
    "    pos = [(sample_ds_idx, i, j) for i, j in pos]   # add sample idx \n",
    "    all_pos.extend(pos)\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "len(all_X), len(all_y), len(all_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6dba0c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ds_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "98e503b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "probing_data_out_dir = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/probing/text2sql/link_prediction/spider/uskg\"\n",
    "os.makedirs(probing_data_out_dir, exist_ok=True)\n",
    "\n",
    "output_path_test_X = os.path.join(probing_data_out_dir, f'{orig_ds}.{prob_ds}.X.pkl')\n",
    "output_path_test_y = os.path.join(probing_data_out_dir, f'{orig_ds}.{prob_ds}.y.pkl')\n",
    "output_path_test_pos = os.path.join(probing_data_out_dir, f'{orig_ds}.{prob_ds}.pos.txt')\n",
    "\n",
    "with open(output_path_test_X, 'wb') as f:\n",
    "    pickle.dump(all_X, f)\n",
    "with open(output_path_test_y, 'wb') as f:\n",
    "    pickle.dump(all_y, f)\n",
    "with open(output_path_test_pos, 'w') as f:\n",
    "    for idx, i, j in all_pos:\n",
    "        f.write(f'{idx}\\t{i}\\t{j}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019624d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef8feb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1deca2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60bd11f6",
   "metadata": {},
   "source": [
    "## Temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093577c1",
   "metadata": {},
   "source": [
    "### Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "913e1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fast = AutoTokenizer.from_pretrained('t5-base', use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt = \"This is t5's tokenization.; structed knowledge: | model | plm(t5), rnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c4eba6d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁This',\n",
       " '▁is',\n",
       " '▁',\n",
       " 't',\n",
       " '5',\n",
       " \"'\",\n",
       " 's',\n",
       " '▁token',\n",
       " 'ization',\n",
       " '.',\n",
       " ';',\n",
       " '▁',\n",
       " 'struct',\n",
       " 'e',\n",
       " 'd',\n",
       " '▁knowledge',\n",
       " ':',\n",
       " '▁|',\n",
       " '▁model',\n",
       " '▁|',\n",
       " '▁pl',\n",
       " 'm',\n",
       " '(',\n",
       " 't',\n",
       " '5)',\n",
       " ',',\n",
       " '▁',\n",
       " 'r',\n",
       " 'n',\n",
       " 'n']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_fast = tokenizer_fast.tokenize(test_txt)\n",
    "tok_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e373a434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.encode(test_txt)\n",
    "encoded_fast = tokenizer_fast.encode(test_txt)\n",
    "type(encoded), type(encoded_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dc5a83ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100, 100),\n",
       " (19, 19),\n",
       " (3, 3),\n",
       " (17, 17),\n",
       " (755, 755),\n",
       " (31, 31),\n",
       " (7, 7),\n",
       " (14145, 14145),\n",
       " (1707, 1707),\n",
       " (5, 5),\n",
       " (117, 117),\n",
       " (3, 3),\n",
       " (7593, 7593),\n",
       " (15, 15),\n",
       " (26, 26),\n",
       " (1103, 1103),\n",
       " (10, 10),\n",
       " (1820, 1820),\n",
       " (825, 825),\n",
       " (1820, 1820),\n",
       " (4752, 4752),\n",
       " (51, 51),\n",
       " (599, 599),\n",
       " (17, 17),\n",
       " (9120, 9120),\n",
       " (6, 6),\n",
       " (3, 3),\n",
       " (52, 52),\n",
       " (29, 29),\n",
       " (29, 29),\n",
       " (1, 1)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(encoded, encoded_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "331836ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.tokenization_utils_base.BatchEncoding,\n",
       " {'input_ids': [100, 19, 3, 17, 755, 31, 7, 14145, 1707, 5, 117, 3, 7593, 15, 26, 1103, 10, 1820, 825, 1820, 4752, 51, 599, 17, 9120, 6, 3, 52, 29, 29, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_fast = tokenizer_fast(test_txt)\n",
    "type(ed_fast), ed_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "52a19df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ed_fast.data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6b53c249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " None]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_fast.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2b9dd785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ed_fast.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "7129e062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4c36f242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁This          This\n",
      "▁is            is\n",
      "▁              t5's\n",
      "t              t5's\n",
      "5              t5's\n",
      "'              t5's\n",
      "s              t5's\n",
      "▁token         tokenization.;\n",
      "ization        tokenization.;\n",
      ".              tokenization.;\n",
      ";              tokenization.;\n",
      "▁              structed\n",
      "struct         structed\n",
      "e              structed\n",
      "d              structed\n",
      "▁knowledge     knowledge:\n",
      ":              knowledge:\n",
      "▁|             |\n",
      "▁model         model\n",
      "▁|             |\n",
      "▁pl            plm(t5),\n",
      "m              plm(t5),\n",
      "(              plm(t5),\n",
      "t              plm(t5),\n",
      "5)             plm(t5),\n",
      ",              plm(t5),\n",
      "▁              rnn\n",
      "r              rnn\n",
      "n              rnn\n",
      "n              rnn\n"
     ]
    }
   ],
   "source": [
    "for sw_id, w_id in enumerate(ed_fast.word_ids()[:-1]):\n",
    "    # the last is eos\n",
    "    sw = tok_fast[sw_id]\n",
    "    w = test_txt.split(' ')[w_id]\n",
    "    print(f'{sw:<15s}{w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d52aa893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenSpan(start=7, end=11)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_fast.word_to_tokens(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "91466c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.tokenization_utils_base.BatchEncoding,\n",
       " {'input_ids': [100, 19, 3, 17, 755, 31, 7, 14145, 1707, 5, 117, 3, 7593, 15, 26, 1103, 10, 1820, 825, 1820, 4752, 51, 599, 17, 9120, 6, 3, 52, 29, 29, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed = tokenizer(test_txt)\n",
    "type(ed), ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dfa68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9f511a",
   "metadata": {},
   "source": [
    "#### Different tokenizers consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "5010e027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_path = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/spider/dev+ratsql_graph.json\"\n",
    "\n",
    "with open(ds_path, 'r') as f:\n",
    "    dataset2 = json.load(f)\n",
    "\n",
    "for d in dataset2:\n",
    "    d['rat_sql_graph']['relations'] = json.loads(d['rat_sql_graph']['relations'])\n",
    "\n",
    "len(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "19c1c421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e565419bb952498bb1b68cec36b4c40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, d in enumerate(tqdm(dataset2)):\n",
    "    text_in = d['question']\n",
    "    struct_in = uskg_sample_to_struct_input(d)\n",
    "    txt = \"{}; structed knowledge: {}\".format(text_in, struct_in)\n",
    "    \n",
    "    if \"<\" not in txt:\n",
    "        continue\n",
    "    \n",
    "    uskg_tokenized_txt = tokenizer(txt, max_length=1024, padding=\"max_length\", truncation=True)\n",
    "    base_tokenized_txt = tokenizer_fast(txt, max_length=1024, padding=\"max_length\", truncation=True)\n",
    "    large_tokenized_txt = tokenizer_large_fast(txt, max_length=1024, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    assert uskg_tokenized_txt.data == base_tokenized_txt.data == large_tokenized_txt.data, (i, txt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "ccdb9cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_uskg = tokenizer.get_vocab()\n",
    "vocab_base = tokenizer_fast.get_vocab()\n",
    "vocab_large = tokenizer_large_fast.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "fd5e5662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <= 32101 None None\n",
      " < 32100 None None\n"
     ]
    }
   ],
   "source": [
    "for k in set(vocab_uskg.keys()) | set(vocab_base.keys()) | set(vocab_large.keys()):\n",
    "    v = vocab_uskg.get(k, None)\n",
    "    v1 = vocab_base.get(k, None)\n",
    "    v2 = vocab_large.get(k, None)\n",
    "    if not v == v1 == v2:\n",
    "        print(k, v, v1, v2)\n",
    "#     assert v == v2, (k, v, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "5e0a6984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32100"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_uskg[' <']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "c7b1ea60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de01e42229ba4d78b7438b2cf9910a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, d in enumerate(tqdm(dataset2)):\n",
    "    text_in = d['question']\n",
    "    struct_in = uskg_sample_to_struct_input(d)\n",
    "    txt = \"{}; structed knowledge: {}\".format(text_in, struct_in)\n",
    "    \n",
    "    if \"<\" in txt:\n",
    "        print(i, txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004c78c",
   "metadata": {},
   "source": [
    "#### max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "cec760fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the Asian countries which have a population larger than that of any country in Africa?; structed knowledge:  | real_estate_properties | ref_feature_types : feature_type_code , feature_type_name | ref_property_types : property_type_code ( Apartment , House ) , property_type_description | other_available_features : feature_id , feature_type_code , feature_name , feature_description | properties : property_id , property_type_code ( Apartment , House ) , date_on_market , date_sold , property_name , property_address , room_count , vendor_requested_price , buyer_offered_price , agreed_selling_price , apt_feature_1 , apt_feature_2 , apt_feature_3 , fld_feature_1 , fld_feature_2 , fld_feature_3 , hse_feature_1 , hse_feature_2 , hse_feature_3 , oth_feature_1 , oth_feature_2 , oth_feature_3 , shp_feature_1 , shp_feature_2 , shp_feature_3 , other_property_details | other_property_features : property_id , feature_id , property_feature_description'"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 777\n",
    "text_in = dataset2[idx]['question']\n",
    "struct_in = uskg_sample_to_struct_input(d)\n",
    "txt = \"{}; structed knowledge: {}\".format(text_in, struct_in)\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "1db6a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tkn_txt = tokenizer_fast(txt, max_length=32, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "ecaf074b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What\n",
      "are\n",
      "the\n",
      "Asian\n",
      "countries\n",
      "which\n",
      "have\n",
      "a\n",
      "a\n",
      "population\n",
      "larger\n",
      "than\n",
      "that\n",
      "of\n",
      "any\n",
      "country\n",
      "in\n",
      "Africa\n",
      "?\n",
      ";\n",
      "s\n",
      "struct\n",
      "e\n",
      "d\n",
      "knowledge\n",
      ":\n",
      "|\n",
      "real\n",
      "_\n",
      "e\n",
      "state\n"
     ]
    }
   ],
   "source": [
    "for i in range(31):\n",
    "    st, ed = _tkn_txt.token_to_chars(i)\n",
    "    print(txt[st:ed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "41b6a74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'What'),\n",
       " (1, 'are'),\n",
       " (2, 'the'),\n",
       " (3, 'Asian'),\n",
       " (4, 'countries'),\n",
       " (5, 'which'),\n",
       " (6, 'have'),\n",
       " (7, 'a'),\n",
       " (8, 'population'),\n",
       " (9, 'larger'),\n",
       " (10, 'than'),\n",
       " (11, 'that'),\n",
       " (12, 'of'),\n",
       " (13, 'any'),\n",
       " (14, 'country'),\n",
       " (15, 'in'),\n",
       " (16, 'Africa?;'),\n",
       " (17, 'structed'),\n",
       " (18, 'knowledge:'),\n",
       " (19, '|'),\n",
       " (20, 'real_estate_properties'),\n",
       " (21, '|'),\n",
       " (22, 'ref_feature_types'),\n",
       " (23, ':'),\n",
       " (24, 'feature_type_code'),\n",
       " (25, ','),\n",
       " (26, 'feature_type_name'),\n",
       " (27, '|'),\n",
       " (28, 'ref_property_types'),\n",
       " (29, ':'),\n",
       " (30, 'property_type_code'),\n",
       " (31, '('),\n",
       " (32, 'Apartment'),\n",
       " (33, ','),\n",
       " (34, 'House'),\n",
       " (35, ')'),\n",
       " (36, ','),\n",
       " (37, 'property_type_description'),\n",
       " (38, '|'),\n",
       " (39, 'other_available_features'),\n",
       " (40, ':'),\n",
       " (41, 'feature_id'),\n",
       " (42, ','),\n",
       " (43, 'feature_type_code'),\n",
       " (44, ','),\n",
       " (45, 'feature_name'),\n",
       " (46, ','),\n",
       " (47, 'feature_description'),\n",
       " (48, '|'),\n",
       " (49, 'properties'),\n",
       " (50, ':'),\n",
       " (51, 'property_id'),\n",
       " (52, ','),\n",
       " (53, 'property_type_code'),\n",
       " (54, '('),\n",
       " (55, 'Apartment'),\n",
       " (56, ','),\n",
       " (57, 'House'),\n",
       " (58, ')'),\n",
       " (59, ','),\n",
       " (60, 'date_on_market'),\n",
       " (61, ','),\n",
       " (62, 'date_sold'),\n",
       " (63, ','),\n",
       " (64, 'property_name'),\n",
       " (65, ','),\n",
       " (66, 'property_address'),\n",
       " (67, ','),\n",
       " (68, 'room_count'),\n",
       " (69, ','),\n",
       " (70, 'vendor_requested_price'),\n",
       " (71, ','),\n",
       " (72, 'buyer_offered_price'),\n",
       " (73, ','),\n",
       " (74, 'agreed_selling_price'),\n",
       " (75, ','),\n",
       " (76, 'apt_feature_1'),\n",
       " (77, ','),\n",
       " (78, 'apt_feature_2'),\n",
       " (79, ','),\n",
       " (80, 'apt_feature_3'),\n",
       " (81, ','),\n",
       " (82, 'fld_feature_1'),\n",
       " (83, ','),\n",
       " (84, 'fld_feature_2'),\n",
       " (85, ','),\n",
       " (86, 'fld_feature_3'),\n",
       " (87, ','),\n",
       " (88, 'hse_feature_1'),\n",
       " (89, ','),\n",
       " (90, 'hse_feature_2'),\n",
       " (91, ','),\n",
       " (92, 'hse_feature_3'),\n",
       " (93, ','),\n",
       " (94, 'oth_feature_1'),\n",
       " (95, ','),\n",
       " (96, 'oth_feature_2'),\n",
       " (97, ','),\n",
       " (98, 'oth_feature_3'),\n",
       " (99, ','),\n",
       " (100, 'shp_feature_1'),\n",
       " (101, ','),\n",
       " (102, 'shp_feature_2'),\n",
       " (103, ','),\n",
       " (104, 'shp_feature_3'),\n",
       " (105, ','),\n",
       " (106, 'other_property_details'),\n",
       " (107, '|'),\n",
       " (108, 'other_property_features'),\n",
       " (109, ':'),\n",
       " (110, 'property_id'),\n",
       " (111, ','),\n",
       " (112, 'feature_id'),\n",
       " (113, ','),\n",
       " (114, 'property_feature_description')]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(txt.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "4f9bd770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 131, 'real_estate')"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st, ed = _tkn_txt.word_to_chars(20)\n",
    "st, ed, txt[st:ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "deac16dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type object argument after * must be an iterable, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yv/jvxn5bdn5_q0cc9_d83cqdr00000gn/T/ipykernel_14287/568061396.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_tkn_txt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mword_to_chars\u001b[0;34m(self, batch_or_word_index, word_index, sequence_index)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_or_word_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCharSpan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchar_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_or_char_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: type object argument after * must be an iterable, not NoneType"
     ]
    }
   ],
   "source": [
    "_tkn_txt.word_to_chars(21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e72ba53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc7242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3013be1f",
   "metadata": {},
   "source": [
    "### Test get encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "895c1c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "models.prompt.modeling_t5.T5Stack"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.pretrain_model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7661204e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = tokenizer.batch_decode(\n",
    "#   model.generate(\n",
    "#     torch.LongTensor(tokenized_txt.data['input_ids']),\n",
    "#     torch.LongTensor(tokenized_txt.data['attention_mask']),\n",
    "#     num_beams=1, \n",
    "#     max_length=256\n",
    "#     ), \n",
    "#   skip_special_tokens=True \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b01ceced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_ids = self.pretrain_model.generate(\n",
    "#     input_ids=input_ids,\n",
    "#     attention_mask=attention_mask,\n",
    "#     past_prompt=past_prompt,\n",
    "#     use_cache=True,\n",
    "#     **kwargs,\n",
    "# )\n",
    "\n",
    "# model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "# outputs = self(\n",
    "#     **model_inputs,\n",
    "#     return_dict=True,\n",
    "#     output_attentions=output_attentions,\n",
    "#     output_hidden_states=output_hidden_states,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12e7a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_prompt = model.get_prompt(\n",
    "    bsz=1,              # bsz = input_ids.shape[0]\n",
    "    sample_size=1,      # sample_size=kwargs['num_beams']\n",
    "    description=None,   \n",
    "    knowledge=None,     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "86c50f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = model.pretrain_model.encoder(\n",
    "    input_ids=torch.LongTensor(tokenized_txt.data['input_ids']),\n",
    "    attention_mask=torch.LongTensor(tokenized_txt.data['attention_mask']),\n",
    "    past_prompt=past_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "afe91447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['decoder_prompt', 'cross_attention_prompt', 'encoder_prompt'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_prompt[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7c2fe2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['last_hidden_state', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12ca2b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 1024])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e4378cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tokenized_txt.data['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4b02f9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 158)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# has zero-padding and EOS, no BOS  \n",
    "len(tokenized_txt.data['input_ids'][0]), np.greater(tokenized_txt.data['input_ids'][0], 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae3cd0a",
   "metadata": {},
   "source": [
    "### Tokenized pieces-nodes mapping (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16de11ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_txt.data['input_ids'][0][:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "48be5d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "644cf1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁What',\n",
       " '▁are',\n",
       " '▁the',\n",
       " '▁Asian',\n",
       " '▁countries',\n",
       " '▁which',\n",
       " '▁have',\n",
       " '▁',\n",
       " 'a',\n",
       " '▁population',\n",
       " '▁larger',\n",
       " '▁than',\n",
       " '▁that',\n",
       " '▁of',\n",
       " '▁any',\n",
       " '▁country',\n",
       " '▁in',\n",
       " '▁Africa',\n",
       " '?',\n",
       " ';',\n",
       " '▁',\n",
       " 'struct',\n",
       " 'e',\n",
       " 'd',\n",
       " '▁knowledge',\n",
       " ':',\n",
       " '▁|',\n",
       " '▁world',\n",
       " '_',\n",
       " '1',\n",
       " '▁|',\n",
       " '▁city',\n",
       " '▁',\n",
       " ':',\n",
       " '▁',\n",
       " 'i',\n",
       " 'd',\n",
       " '▁',\n",
       " ',',\n",
       " '▁name',\n",
       " '▁',\n",
       " ',',\n",
       " '▁country',\n",
       " 'code',\n",
       " '▁',\n",
       " ',',\n",
       " '▁district',\n",
       " '▁',\n",
       " ',',\n",
       " '▁population',\n",
       " '▁|',\n",
       " '▁sq',\n",
       " 'lite',\n",
       " '_',\n",
       " 's',\n",
       " 'e',\n",
       " 'que',\n",
       " 'nce',\n",
       " '▁',\n",
       " ':',\n",
       " '▁name',\n",
       " '▁',\n",
       " ',',\n",
       " '▁se',\n",
       " 'q',\n",
       " '▁|',\n",
       " '▁country',\n",
       " '▁',\n",
       " ':',\n",
       " '▁code',\n",
       " '▁',\n",
       " ',',\n",
       " '▁name',\n",
       " '▁',\n",
       " ',',\n",
       " '▁continent',\n",
       " '▁(',\n",
       " '▁Africa',\n",
       " '▁',\n",
       " ',',\n",
       " '▁Asia',\n",
       " '▁',\n",
       " ')',\n",
       " '▁',\n",
       " ',',\n",
       " '▁region',\n",
       " '▁',\n",
       " ',',\n",
       " '▁surface',\n",
       " 'area',\n",
       " '▁',\n",
       " ',',\n",
       " '▁in',\n",
       " 'de',\n",
       " 'p',\n",
       " 'year',\n",
       " '▁',\n",
       " ',',\n",
       " '▁population',\n",
       " '▁',\n",
       " ',',\n",
       " '▁life',\n",
       " 'ex',\n",
       " 'pe',\n",
       " 'c',\n",
       " 't',\n",
       " 'ancy',\n",
       " '▁',\n",
       " ',',\n",
       " '▁',\n",
       " 'g',\n",
       " 'n',\n",
       " 'p',\n",
       " '▁',\n",
       " ',',\n",
       " '▁',\n",
       " 'g',\n",
       " 'n',\n",
       " 'pol',\n",
       " 'd',\n",
       " '▁',\n",
       " ',',\n",
       " '▁local',\n",
       " 'name',\n",
       " '▁',\n",
       " ',',\n",
       " '▁government',\n",
       " 'form',\n",
       " '▁',\n",
       " ',',\n",
       " '▁head',\n",
       " 'of',\n",
       " 'state',\n",
       " '▁',\n",
       " ',',\n",
       " '▁capital',\n",
       " '▁',\n",
       " ',',\n",
       " '▁code',\n",
       " '2',\n",
       " '▁|',\n",
       " '▁country',\n",
       " 'language',\n",
       " '▁',\n",
       " ':',\n",
       " '▁country',\n",
       " 'code',\n",
       " '▁',\n",
       " ',',\n",
       " '▁language',\n",
       " '▁',\n",
       " ',',\n",
       " '▁is',\n",
       " 'official',\n",
       " '▁',\n",
       " ',',\n",
       " '▁percentage']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0c18b41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('what be the asian country which have a population larger than that of any country in africa ? <C>NONE::* <C>city::id <C>city::name <C>city::countrycode <C>city::district <C>city::population <C>sqlite_sequence::name <C>sqlite_sequence::seq <C>country::code <C>country::name <C>country::continent <C>country::region <C>country::surfacearea <C>country::indepyear <C>country::population <C>country::lifeexpectancy <C>country::gnp <C>country::gnpold <C>country::localname <C>country::governmentform <C>country::headofstate <C>country::capital <C>country::code2 <C>countrylanguage::countrycode <C>countrylanguage::language <C>countrylanguage::isofficial <C>countrylanguage::percentage <T>city <T>sqlite_sequence <T>country <T>countrylanguage',\n",
       " 'What are the Asian countries which have a population larger than that of any country in Africa?; structed knowledge:  | world_1 | city : id , name , countrycode , district , population | sqlite_sequence : name , seq | country : code , name , continent ( Africa , Asia ) , region , surfacearea , indepyear , population , lifeexpectancy , gnp , gnpold , localname , governmentform , headofstate , capital , code2 | countrylanguage : countrycode , language , isofficial , percentage')"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(sample['rat_sql_graph']['nodes']), txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "6c4a7fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the Asian countries which have a population larger than that of any country in Africa ?'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(sample['question_toks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3ed46040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f331b",
   "metadata": {},
   "source": [
    "#### Nodes to char ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "9bd2741c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('what', 'What'),\n",
       "  ('be', 'are'),\n",
       "  ('the', 'the'),\n",
       "  ('asian', 'Asian'),\n",
       "  ('country', 'countries'),\n",
       "  ('which', 'which'),\n",
       "  ('have', 'have'),\n",
       "  ('a', 'a'),\n",
       "  ('population', 'population'),\n",
       "  ('larger', 'larger'),\n",
       "  ('than', 'than'),\n",
       "  ('that', 'that'),\n",
       "  ('of', 'of'),\n",
       "  ('any', 'any'),\n",
       "  ('country', 'country'),\n",
       "  ('in', 'in'),\n",
       "  ('africa', 'Africa'),\n",
       "  ('?', '?')],\n",
       " [('NONE', '*'),\n",
       "  ('city', 'id'),\n",
       "  ('city', 'name'),\n",
       "  ('city', 'countrycode'),\n",
       "  ('city', 'district'),\n",
       "  ('city', 'population'),\n",
       "  ('sqlite_sequence', 'name'),\n",
       "  ('sqlite_sequence', 'seq'),\n",
       "  ('country', 'code'),\n",
       "  ('country', 'name'),\n",
       "  ('country', 'continent'),\n",
       "  ('country', 'region'),\n",
       "  ('country', 'surfacearea'),\n",
       "  ('country', 'indepyear'),\n",
       "  ('country', 'population'),\n",
       "  ('country', 'lifeexpectancy'),\n",
       "  ('country', 'gnp'),\n",
       "  ('country', 'gnpold'),\n",
       "  ('country', 'localname'),\n",
       "  ('country', 'governmentform'),\n",
       "  ('country', 'headofstate'),\n",
       "  ('country', 'capital'),\n",
       "  ('country', 'code2'),\n",
       "  ('countrylanguage', 'countrycode'),\n",
       "  ('countrylanguage', 'language'),\n",
       "  ('countrylanguage', 'isofficial'),\n",
       "  ('countrylanguage', 'percentage')],\n",
       " ['city', 'sqlite_sequence', 'country', 'countrylanguage'])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratsql nodes to sentence char ids \n",
    "\n",
    "ratsql_graph_nodes = sample['rat_sql_graph']['nodes']\n",
    "question_toks = sample['question_toks']\n",
    "\n",
    "_q_nodes = []  # [stem token (node name)]\n",
    "q_nodes = []  # [(stem token (node name), orig question token)]\n",
    "c_nodes = []  # [(orig table name, orig column name)]\n",
    "t_nodes = []  # [orig table name]\n",
    "\n",
    "for n in ratsql_graph_nodes:\n",
    "    if n.startswith('<C>'):\n",
    "        _n = n[3:]\n",
    "        _t, _c = _n.split('::')\n",
    "        c_nodes.append((_t, _c))\n",
    "    elif n.startswith('<T>'):\n",
    "        _n = n[3:]\n",
    "        t_nodes.append(_n)\n",
    "    else:\n",
    "        _q_nodes.append(n)\n",
    "\n",
    "assert len(_q_nodes) == len(question_toks), (_q_nodes, question_toks)\n",
    "q_nodes = list(zip(_q_nodes, question_toks))\n",
    "\n",
    "q_nodes, c_nodes, t_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f2550944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the Asian countries which have a population larger than that of any country in Africa?; structed knowledge:  | world_1 | city : id , name , countrycode , district , population | sqlite_sequence : name , seq | country : code , name , continent ( Africa , Asia ) , region , surfacearea , indepyear , population , lifeexpectancy , gnp , gnpold , localname , governmentform , headofstate , capital , code2 | countrylanguage : countrycode , language , isofficial , percentage'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2ceca9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_txt = tokenizer_fast([txt], max_length=1024, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "97c824e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt = \"This is t5's tokenization.; structed knowledge: | model | plm(t5), rnn\"\n",
    "\n",
    "# tokenized_txt = tokenizer_fast([test_txt], max_length=1024, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "ffb404b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 86)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of whitespace words \n",
    "max([w_id for w_id in tokenized_txt.word_ids() if w_id is not None]) + 1, len(txt.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "44360ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_splitter = \"; structed knowledge:  \"  # struct_in has a preceding white space \n",
    "text_in, struct_in = txt.split(_splitter)\n",
    "\n",
    "q_node_chars = []   # [(st, ed)]; same below\n",
    "c_node_chars = []\n",
    "t_node_chars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "116bdd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 18, CharSpan(start=0, end=4))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text part\n",
    "# Assumption: the mismatch between whitespace words (text_words) and question words only come from trailing puncts\n",
    "\n",
    "text_words = text_in.strip().split(' ') + ['<SENTINAL>']\n",
    "text_word_char_ranges = [tokenized_txt.word_to_chars(i) for i in range(len(text_words) - 1)] + [(None, None)]  # -1 to remove the sentinal \n",
    "\n",
    "len(text_words), len(text_word_char_ranges), text_word_char_ranges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1a83cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_tw_idx = 0\n",
    "curr_tw = text_words[0]\n",
    "curr_tw_char_range = text_word_char_ranges[0]\n",
    "curr_char_ptr = 0\n",
    "\n",
    "for stem_tok, orig_tok in q_nodes:\n",
    "    if curr_tw == orig_tok:\n",
    "        # finishing current word \n",
    "        q_node_chars.append((curr_char_ptr, curr_char_ptr + len(orig_tok)))   # curr pos to curr pos + len \n",
    "        curr_tw_idx += 1\n",
    "        curr_tw = text_words[curr_tw_idx]\n",
    "        curr_tw_char_range = text_word_char_ranges[curr_tw_idx]\n",
    "        curr_char_ptr = curr_tw_char_range[0]\n",
    "    else:\n",
    "        # not finishing current word \n",
    "        assert curr_tw.startswith(orig_tok), (curr_tw, orig_tok)\n",
    "        q_node_chars.append((curr_char_ptr, curr_char_ptr + len(orig_tok)))   # curr pos to curr pos + len \n",
    "        curr_char_ptr += len(orig_tok)     # move ptr forward by len \n",
    "        curr_tw = curr_tw[len(orig_tok):]  # get the remaining chars in the word \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1e7d2020",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[txt[st:ed] for st, ed in q_node_chars] == sample['question_toks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "4796ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Struct part \n",
    "assert not struct_in.startswith(' ')\n",
    "_str_before_struct = text_in + _splitter\n",
    "_n_words_before_struct = len(_str_before_struct.strip().split(' '))\n",
    "\n",
    "struct_words_st_idx = _n_words_before_struct\n",
    "assert len(_str_before_struct) == tokenized_txt.word_to_chars(struct_words_st_idx)[0], \\\n",
    "    (len(_str_before_struct), tokenized_txt.word_to_chars(struct_words_st_idx)[0])   # len(before) == starting char idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "e3ea5174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'| world_1 | city : id , name , countrycode , district , population | sqlite_sequence : name , seq | country : code , name , continent ( Africa , Asia ) , region , surfacearea , indepyear , population , lifeexpectancy , gnp , gnpold , localname , governmentform , headofstate , capital , code2 | countrylanguage : countrycode , language , isofficial , percentage'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "30b85a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_ranges_collector = StructCharRangesCollector()\n",
    "struct_ranges_collector.collect(struct_in, tokenized_txt, _n_words_before_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "23637af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for db_id_name, (st, ed) in struct_ranges_collector.db_id2char_ranges.items():\n",
    "    assert txt[st:ed] == db_id_name, (st, ed, txt[st:ed], db_id_name) \n",
    "\n",
    "for table_name, (st, ed) in struct_ranges_collector.table2char_ranges.items():\n",
    "    assert txt[st:ed] == table_name, (st, ed, txt[st:ed], table_name) \n",
    "\n",
    "for (_, col_name), (st, ed) in struct_ranges_collector.column2char_ranges.items():\n",
    "    txt_piece = txt[st:ed].split(' ( ')[0]\n",
    "    assert txt_piece == col_name, (st, ed, txt[st:ed], col_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "06080ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for db_id_name, (st, ed) in struct_ranges_collector.db_id2char_ranges.items():\n",
    "#     print(st, ed, txt[st:ed], db_id_name) \n",
    "# print()\n",
    "# for table_name, (st, ed) in struct_ranges_collector.table2char_ranges.items():\n",
    "#     print(st, ed, txt[st:ed], table_name) \n",
    "# print()\n",
    "# for (_, col_name), (st, ed) in struct_ranges_collector.column2char_ranges.items():\n",
    "#     txt_piece = txt[st:ed].split(' ( ')[0]\n",
    "#     print(st, ed, txt[st:ed], col_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "f8e92d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_node in c_nodes:\n",
    "    if c_node == ('NONE', '*'):\n",
    "        # the special column in spider, using db_id \n",
    "        c_node_chars.append(list(struct_ranges_collector.db_id2char_ranges.values())[0])   # assuming only 1 db_id, which should be true...\n",
    "    else:\n",
    "        c_node_chars.append(struct_ranges_collector.column2char_ranges[c_node])\n",
    "\n",
    "for t_node in t_nodes:\n",
    "    t_node_chars.append(struct_ranges_collector.table2char_ranges[t_node])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acecee44",
   "metadata": {},
   "source": [
    "#### Nodes to tokenized pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "735fd648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 49)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_char_ranges = char_ranges_dict['q_node_chars'] + char_ranges_dict['c_node_chars'] + char_ranges_dict['t_node_chars']\n",
    "len(node_char_ranges), len(sample['rat_sql_graph']['nodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1f9cae71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some chars can be mapped to multiple tokens (e.g. 'i' => '▁', 'i' )\n",
    "char_to_tokens_dict = defaultdict(list)\n",
    "\n",
    "for token_idx, tok in enumerate(tokenized_txt.tokens()):\n",
    "    if tok == '</s>':\n",
    "        break\n",
    "    char_span = tokenized_txt.token_to_chars(token_idx)\n",
    "    for char_idx in range(char_span[0], char_span[1]):\n",
    "        char_to_tokens_dict[char_idx].append(token_idx)\n",
    "\n",
    "len(char_to_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68291b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pieces_ranges = []\n",
    "\n",
    "for st, ed in node_ranges:\n",
    "    piece_ids = []\n",
    "    for char_idx in range(st, ed):\n",
    "        _piece_ids = char_to_tokens_dict[char_idx]\n",
    "        piece_ids.extend(_piece_ids)\n",
    "    \n",
    "    piece_st = piece_ids[0]\n",
    "    piece_ed = piece_ids[-1] + 1\n",
    "    # the collected piece_ids should be continuous \n",
    "    # ^ not true... some chars can be mapped to multiple tokens (started by ▁ )\n",
    "    # re-collect a char-to-token\n",
    "    assert set(range(piece_st, piece_ed)) == set(piece_ids), piece_ids\n",
    "    \n",
    "    node_pieces_ranges.append((piece_st, piece_ed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3da387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n, (p_st, p_ed) in zip(sample['rat_sql_graph']['nodes'], node_pieces_ranges):\n",
    "    print(n, '\\t', tokenized_txt.tokens()[p_st:p_ed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06a80d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2609d051",
   "metadata": {},
   "source": [
    "### data: server vs. local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e329d66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_data_path = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/probing/text2sql/link_prediction/spider/uskg/local/dev.train.X.pkl\"\n",
    "server_data_path = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/probing/text2sql/link_prediction/spider/uskg/dev.train.X.pkl\"\n",
    "\n",
    "with open(local_data_path, 'rb') as f:\n",
    "    local_data_X = pickle.load(f)\n",
    "with open(server_data_path, 'rb') as f:\n",
    "    server_data_X = pickle.load(f)\n",
    "\n",
    "type(local_data_X), type(server_data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1c4040f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16059, 3072), (16059, 3072))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_data_X = np.array(local_data_X)\n",
    "server_data_X = np.array(server_data_X)\n",
    "local_data_X.shape, server_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2ca624fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5264,  5255,  5247,   288,   292,   294, 11989, 11979,  8755,\n",
       "        8756])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_diff_ids = np.argsort(np.max(np.abs(local_data_X - server_data_X), axis=1))[::-1]\n",
    "large_diff_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "6f907972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034, 16059)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_path = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/spider/dev+ratsql_graph.json\"\n",
    "pos_path = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/probing/text2sql/link_prediction/spider/uskg/dev.train.pos.txt\"\n",
    "\n",
    "with open(ds_path, 'r') as f:\n",
    "    ds_samples = json.load(f)\n",
    "    for d in ds_samples:\n",
    "        d['rat_sql_graph']['relations'] = json.loads(d['rat_sql_graph']['relations'])\n",
    "with open(pos_path, 'r') as f:\n",
    "    pos_lines = f.read().strip().split('\\n')\n",
    "\n",
    "len(ds_samples), len(pos_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "0b9e292b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5245"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_idx = large_diff_ids[51]\n",
    "_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "b8eb2bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.16274624, -0.06520639,  0.11448385,  0.02375873,  0.00876573,\n",
       "        -0.10075585,  0.11073106,  0.00379219,  0.07125453,  0.01576799,\n",
       "         0.01186033], dtype=float32),\n",
       " array([-0.17650707, -0.07676765,  0.13225155,  0.01257438,  0.00267248,\n",
       "        -0.1266068 ,  0.12796973,  0.00241799,  0.05103843,  0.03357478,\n",
       "         0.01657346], dtype=float32),\n",
       " 0.07821107)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _idx = 8970\n",
    "local_data_X[_idx][::300], server_data_X[_idx][::300], max(local_data_X[_idx] - server_data_X[_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "c4e11124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696, 6, 6)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid, i, j = [int(s) for s in pos_lines[_idx].split('\\t')]\n",
    "sid, i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "54d762e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'be',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'vote',\n",
       " 'from',\n",
       " 'state',\n",
       " '`',\n",
       " 'ny',\n",
       " \"'\",\n",
       " 'or',\n",
       " '`',\n",
       " 'ca',\n",
       " \"'\",\n",
       " '?',\n",
       " '<C>NONE::*',\n",
       " '<C>area_code_state::area_code',\n",
       " '<C>area_code_state::state',\n",
       " '<C>contestant::contestant_number',\n",
       " '<C>contestant::contestant_name',\n",
       " '<C>vote::vote_id',\n",
       " '<C>vote::phone_number',\n",
       " '<C>vote::state',\n",
       " '<C>vote::contestant_number',\n",
       " '<C>vote::create',\n",
       " '<T>area_code_state',\n",
       " '<T>contestant',\n",
       " '<T>vote']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_nodes = ds_samples[sid]['rat_sql_graph']['nodes']\n",
    "_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "4f5ce644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('from', 'from')"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_nodes[i], _nodes[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef4356b",
   "metadata": {},
   "source": [
    "### Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "46a1cade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set args here for runnning on notebook, we make them out here to make it more illustrative.\n",
    "sys.argv = ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', # This is the name of your .py launcher when you run this line of code.\n",
    "            # belows are the parameters we set, take spider for example\n",
    "            '--cfg', 'Salesforce/T5_base_finetune_spider_with_cell_value.cfg', \n",
    "            '--output_dir', './tmp']\n",
    "parser = HfArgumentParser((WrappedSeq2SeqTrainingArguments,))\n",
    "training_args, = parser.parse_args_into_dataclasses()\n",
    "set_seed(training_args.seed)\n",
    "tmp_args = Configure.Get(training_args.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "1a6ecde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t5-base'"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_args.bert.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "6dc14a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 't5-base'\n",
    "# model_path = 'hkunlp/from_all_T5_large_prefix_spider_with_cell_value2'\n",
    "# model_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/UnifiedSKG/output/server_runs/A-T5_base_prefix_spider_with_cell_value-asr_mixed/checkpoint-79500/'\n",
    "# model_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/UnifiedSKG/output/server_runs/A-T5_base_prefix_spider_with_cell_value-rewritten_mixed/checkpoint-56500/'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "\n",
    "# for reconstruction\n",
    "# tokenizer_fast = AutoTokenizer.from_pretrained('t5-base', use_fast=True)\n",
    "\n",
    "tmp_model = finetune.Model(tmp_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "a806eb4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0762, -0.0471,  0.0309],\n",
       "        [ 0.0381, -0.0075,  0.0003],\n",
       "        [-0.0047, -0.0262, -0.0298]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model.pretrain_model.encoder.block[0].layer[0].SelfAttention.q.weight[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "796db228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 't5-base'\n",
    "# tmp_model.load(model_path)\n",
    "# # need tmp_model.pretrained_model.load(...)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "70a03cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model.pretrain_model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "62af037c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0013,  0.0083,  0.0015],\n",
       "        [ 0.0008, -0.0017,  0.0039],\n",
       "        [-0.0088, -0.0019, -0.0036]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model.pretrain_model.encoder.block[0].layer[0].SelfAttention.q.weight[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb2e77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb484d30",
   "metadata": {},
   "source": [
    "### others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4815219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t5-base'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.bert.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b76629b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__self__': None,\n",
       " '__default__': {'__call__',\n",
       "  '__class__',\n",
       "  '__default__',\n",
       "  '__delattr__',\n",
       "  '__dict__',\n",
       "  '__dir__',\n",
       "  '__doc__',\n",
       "  '__eq__',\n",
       "  '__format__',\n",
       "  '__ge__',\n",
       "  '__getattribute__',\n",
       "  '__gt__',\n",
       "  '__hash__',\n",
       "  '__init__',\n",
       "  '__init_subclass__',\n",
       "  '__iter__',\n",
       "  '__le__',\n",
       "  '__len__',\n",
       "  '__lt__',\n",
       "  '__module__',\n",
       "  '__ne__',\n",
       "  '__new__',\n",
       "  '__reduce__',\n",
       "  '__reduce_ex__',\n",
       "  '__repr__',\n",
       "  '__self__',\n",
       "  '__setattr__',\n",
       "  '__sizeof__',\n",
       "  '__str__',\n",
       "  '__subclasshook__',\n",
       "  '__weakref__'},\n",
       " 'name': 'unified.prefixtuning',\n",
       " 'use_description': False,\n",
       " 'concatenate_description': False,\n",
       " 'map_description': False,\n",
       " 'knowledge_usage': 'concatenate',\n",
       " 'freeze_plm': True,\n",
       " 'freeze_prefix': False}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de35454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
