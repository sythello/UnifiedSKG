{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d929048",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-05 21:23:58.498318: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-05 21:23:58.498346: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import datasets\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from utils.configue import Configure\n",
    "from utils.training_arguments import WrappedSeq2SeqTrainingArguments\n",
    "from models.unified import finetune, prefixtuning\n",
    "from models.unified.prefixtuning import Model\n",
    "\n",
    "import nltk\n",
    "\n",
    "# from filelock import FileLock\n",
    "# with FileLock(\".lock\") as lock:\n",
    "#     nltk.download(\"punkt\", quiet=True)\n",
    "#     nltk.download(\"stopwords\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3563347b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import deepcopy\n",
    "from collections import Counter, defaultdict\n",
    "import importlib\n",
    "import pickle\n",
    "from typing import List, Dict\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from seq2seq_construction import spider\n",
    "from third_party.spider.preprocess.get_tables import dump_db_json_schema\n",
    "from third_party.miscs.bridge_content_encoder import get_database_matches\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "# import editdistance\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "# from SpeakQL.Allennlp_models.utils.spider import process_sql, evaluation\n",
    "# from SpeakQL.Allennlp_models.utils.misc_utils import EvaluateSQL, EvaluateSQL_full, \\\n",
    "#     Postprocess_rewrite_seq, Postprocess_rewrite_seq_freeze_POS, Postprocess_rewrite_seq_modify_POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69056b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from language.xsp.data_preprocessing import spider_preprocessing, wikisql_preprocessing, michigan_preprocessing\n",
    "\n",
    "import sdr_analysis\n",
    "from sdr_analysis.helpers.general_helpers import db_dict_to_general_fmt\n",
    "from sdr_analysis.helpers.link_prediction_collector import collect_link_prediction_samples\n",
    "from sdr_analysis.helpers.single_node_reconstruction_collector import collect_single_node_reconstruction_samples\n",
    "from sdr_analysis.helpers.base_graph_data_collector import BaseGraphDataCollector, BaseGraphDataCollector_spider, BaseGraphDataCollector_wikisql\n",
    "\n",
    "from sdra.link_prediction_collectors import LinkPredictionDataCollector_USKG_spider, LinkPredictionDataCollector_USKG_wikisql\n",
    "from sdra.single_node_reconstruction_collectors import SingleNodeReconstructionDataCollector_USKG_spider, SingleNodeReconstructionDataCollector_USKG_wikisql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dd46f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sdr_analysis.helpers.base_graph_data_collector\n",
    "# importlib.reload(sdr_analysis.helpers.base_graph_data_collector)\n",
    "# import sdra.probing_data_collectors\n",
    "# importlib.reload(sdra.probing_data_collectors)\n",
    "\n",
    "# import sdra.probing_data_collectors\n",
    "# importlib.reload(sdra.probing_data_collectors)\n",
    "# import sdra.link_prediction_collectors\n",
    "# importlib.reload(sdra.link_prediction_collectors)\n",
    "\n",
    "# import sdra.single_node_reconstruction_collectors\n",
    "# importlib.reload(sdra.single_node_reconstruction_collectors)\n",
    "# from sdra.single_node_reconstruction_collectors import SingleNodeReconstructionDataCollector_USKG_spider, SingleNodeReconstructionDataCollector_USKG_wikisql\n",
    "\n",
    "# import sdr_analysis.helpers.general_helpers\n",
    "# importlib.reload(sdr_analysis.helpers.general_helpers)\n",
    "# from sdr_analysis.helpers.general_helpers import load_pickle_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4febc782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdr_analysis\n",
      "sdr_analysis.helpers\n",
      "sdr_analysis.helpers.general_helpers\n",
      "sdr_analysis.helpers.link_prediction_collector\n",
      "sdr_analysis.helpers.base_graph_data_collector\n",
      "sdr_analysis.helpers.single_node_reconstruction_collector\n",
      "sdra\n",
      "sdra.link_prediction_collectors\n",
      "sdra.probing_data_utils\n",
      "sdra.probing_data_collectors\n",
      "sdra.single_node_reconstruction_collectors\n"
     ]
    }
   ],
   "source": [
    "# might not work because of importing order... just as a sanity check\n",
    "for m_name, m in sys.modules.items():\n",
    "    if m_name.startswith('sdr'):\n",
    "        print(m_name)\n",
    "        importlib.reload(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d1708",
   "metadata": {},
   "source": [
    "## Read data - Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0468b9",
   "metadata": {},
   "source": [
    "### Spider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190d31cc",
   "metadata": {},
   "source": [
    "#### Original loading test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a997101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_cache = dict()\n",
    "\n",
    "db_path = '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/database'\n",
    "\n",
    "def format_spider_schema(db_id):\n",
    "    if db_id not in schema_cache:\n",
    "        schema_cache[db_id] = dump_db_json_schema(\n",
    "            db_path + \"/\" + db_id + \"/\" + db_id + \".sqlite\", db_id)\n",
    "    schema = schema_cache[db_id]\n",
    "\n",
    "    return {\n",
    "        \"db_id\": db_id,\n",
    "        \"db_path\": db_path,\n",
    "        \"db_table_names\": schema[\"table_names_original\"],\n",
    "        \"db_column_names\": {\n",
    "            \"table_id\": [table_id for table_id, column_name in schema[\"column_names_original\"]],\n",
    "            \"column_name\": [column_name for table_id, column_name in schema[\"column_names_original\"]]\n",
    "        },\n",
    "        \"db_column_types\": schema[\"column_types\"],\n",
    "        \"db_primary_keys\": [{\"column_id\": column_id} for column_id in schema[\"primary_keys\"]],\n",
    "        \"db_foreign_keys\": [\n",
    "            {\"column_id\": column_id, \"other_column_id\": other_column_id}\n",
    "            for column_id, other_column_id in schema[\"foreign_keys\"]\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "dfb79d81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'world_1',\n",
       " 'db_path': '/Users/mac/Desktop/syt/Deep-Learning/Dataset/spider/database',\n",
       " 'db_table_names': ['city', 'sqlite_sequence', 'country', 'countrylanguage'],\n",
       " 'db_column_names': {'table_id': [-1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   3,\n",
       "   3,\n",
       "   3,\n",
       "   3],\n",
       "  'column_name': ['*',\n",
       "   'ID',\n",
       "   'Name',\n",
       "   'CountryCode',\n",
       "   'District',\n",
       "   'Population',\n",
       "   'name',\n",
       "   'seq',\n",
       "   'Code',\n",
       "   'Name',\n",
       "   'Continent',\n",
       "   'Region',\n",
       "   'SurfaceArea',\n",
       "   'IndepYear',\n",
       "   'Population',\n",
       "   'LifeExpectancy',\n",
       "   'GNP',\n",
       "   'GNPOld',\n",
       "   'LocalName',\n",
       "   'GovernmentForm',\n",
       "   'HeadOfState',\n",
       "   'Capital',\n",
       "   'Code2',\n",
       "   'CountryCode',\n",
       "   'Language',\n",
       "   'IsOfficial',\n",
       "   'Percentage']},\n",
       " 'db_column_types': ['text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number'],\n",
       " 'db_primary_keys': [{'column_id': 1}, {'column_id': 8}, {'column_id': 23}],\n",
       " 'db_foreign_keys': [{'column_id': 3, 'other_column_id': 8},\n",
       "  {'column_id': 23, 'other_column_id': 8}]}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmt_schema = format_spider_schema('world_1')\n",
    "fmt_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0604d42b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'concert_singer',\n",
       " 'table_names_original': ['stadium', 'singer', 'concert', 'singer_in_concert'],\n",
       " 'table_names': ['stadium', 'singer', 'concert', 'singer in concert'],\n",
       " 'column_names_original': [(-1, '*'),\n",
       "  (0, 'Stadium_ID'),\n",
       "  (0, 'Location'),\n",
       "  (0, 'Name'),\n",
       "  (0, 'Capacity'),\n",
       "  (0, 'Highest'),\n",
       "  (0, 'Lowest'),\n",
       "  (0, 'Average'),\n",
       "  (1, 'Singer_ID'),\n",
       "  (1, 'Name'),\n",
       "  (1, 'Country'),\n",
       "  (1, 'Song_Name'),\n",
       "  (1, 'Song_release_year'),\n",
       "  (1, 'Age'),\n",
       "  (1, 'Is_male'),\n",
       "  (2, 'concert_ID'),\n",
       "  (2, 'concert_Name'),\n",
       "  (2, 'Theme'),\n",
       "  (2, 'Stadium_ID'),\n",
       "  (2, 'Year'),\n",
       "  (3, 'concert_ID'),\n",
       "  (3, 'Singer_ID')],\n",
       " 'column_names': [(-1, '*'),\n",
       "  (0, 'stadium id'),\n",
       "  (0, 'location'),\n",
       "  (0, 'name'),\n",
       "  (0, 'capacity'),\n",
       "  (0, 'highest'),\n",
       "  (0, 'lowest'),\n",
       "  (0, 'average'),\n",
       "  (1, 'singer id'),\n",
       "  (1, 'name'),\n",
       "  (1, 'country'),\n",
       "  (1, 'song name'),\n",
       "  (1, 'song release year'),\n",
       "  (1, 'age'),\n",
       "  (1, 'is male'),\n",
       "  (2, 'concert id'),\n",
       "  (2, 'concert name'),\n",
       "  (2, 'theme'),\n",
       "  (2, 'stadium id'),\n",
       "  (2, 'year'),\n",
       "  (3, 'concert id'),\n",
       "  (3, 'singer id')],\n",
       " 'column_types': ['text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'others',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text'],\n",
       " 'primary_keys': [1, 8, 15, 20],\n",
       " 'foreign_keys': [[18, 1], [21, 8], [20, 15]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_cache['concert_singer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "23178b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '*'),\n",
       " (1, 'Stadium_ID'),\n",
       " (2, 'Location'),\n",
       " (3, 'Name'),\n",
       " (4, 'Capacity'),\n",
       " (5, 'Highest'),\n",
       " (6, 'Lowest'),\n",
       " (7, 'Average'),\n",
       " (8, 'Singer_ID'),\n",
       " (9, 'Name'),\n",
       " (10, 'Country'),\n",
       " (11, 'Song_Name'),\n",
       " (12, 'Song_release_year'),\n",
       " (13, 'Age'),\n",
       " (14, 'Is_male'),\n",
       " (15, 'concert_ID'),\n",
       " (16, 'concert_Name'),\n",
       " (17, 'Theme'),\n",
       " (18, 'Stadium_ID'),\n",
       " (19, 'Year'),\n",
       " (20, 'concert_ID'),\n",
       " (21, 'Singer_ID')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(fmt_schema['db_column_names']['column_name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5075fb56",
   "metadata": {},
   "source": [
    "#### New loading from nested dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f435c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsp_data_dir = \"/Users/mac/Desktop/syt/Deep-Learning/Repos/Google-Research-Language/language/language/xsp/data\"\n",
    "\n",
    "spider_tables_path = os.path.join(xsp_data_dir, 'spider', 'tables.json')\n",
    "\n",
    "spider_dbs_dict = spider_preprocessing.load_spider_tables(spider_tables_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40dfc8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['perpetrator', 'college_2', 'flight_company', 'icfp_1', 'body_builder', 'storm_record', 'pilot_record', 'race_track', 'academic', 'department_store', 'music_4', 'insurance_fnol', 'cinema', 'decoration_competition', 'phone_market', 'store_product', 'assets_maintenance', 'student_assessment', 'dog_kennels', 'music_1', 'company_employee', 'farm', 'solvency_ii', 'city_record', 'swimming', 'flight_2', 'election', 'manufactory_1', 'debate', 'network_2', 'local_govt_in_alabama', 'climbing', 'e_learning', 'scientist_1', 'ship_1', 'entertainment_awards', 'allergy_1', 'imdb', 'products_for_hire', 'candidate_poll', 'chinook_1', 'flight_4', 'pets_1', 'dorm_1', 'journal_committee', 'flight_1', 'medicine_enzyme_interaction', 'local_govt_and_lot', 'station_weather', 'shop_membership', 'driving_school', 'concert_singer', 'music_2', 'sports_competition', 'railway', 'inn_1', 'museum_visit', 'browser_web', 'baseball_1', 'architecture', 'csu_1', 'tracking_orders', 'insurance_policies', 'gas_company', 'e_government', 'school_bus', 'machine_repair', 'theme_gallery', 'film_rank', 'party_people', 'hospital_1', 'customers_campaigns_ecommerce', 'gymnast', 'restaurants', 'mountain_photos', 'battle_death', 'cre_Doc_Control_Systems', 'tracking_share_transactions', 'apartment_rentals', 'student_transcripts_tracking', 'cre_Docs_and_Epenses', 'ship_mission', 'company_office', 'tracking_software_problems', 'products_gen_characteristics', 'coffee_shop', 'riding_club', 'customers_card_transactions', 'county_public_safety', 'performance_attendance', 'club_1', 'singer', 'culture_company', 'cre_Doc_Template_Mgt', 'musical', 'world_1', 'device', 'tracking_grants_for_research', 'employee_hire_evaluation', 'movie_1', 'network_1', 'poker_player', 'program_share', 'aircraft', 'restaurant_1', 'customers_and_invoices', 'insurance_and_eClaims', 'college_1', 'local_govt_mdm', 'book_2', 'hr_1', 'soccer_1', 'sakila_1', 'real_estate_properties', 'college_3', 'course_teach', 'roller_coaster', 'customer_deliveries', 'game_injury', 'school_finance', 'scholar', 'voter_1', 'match_season', 'small_bank_1', 'wta_1', 'yelp', 'student_1', 'manufacturer', 'store_1', 'train_station', 'document_management', 'formula_1', 'game_1', 'loan_1', 'bike_1', 'entrepreneur', 'orchestra', 'cre_Drama_Workshop_Groups', 'car_1', 'geo', 'behavior_monitoring', 'cre_Doc_Tracking_DB', 'university_basketball', 'soccer_2', 'activity_1', 'cre_Theme_park', 'twitter_1', 'election_representative', 'voter_2', 'wedding', 'news_report', 'wine_1', 'customers_and_addresses', 'protein_institute', 'school_player', 'phone_1', 'tvshow', 'wrestler', 'customer_complaints', 'department_management', 'customers_and_products_contacts', 'company_1', 'workshop_paper', 'epinions_1', 'party_host', 'product_catalog'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spider_dbs_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a08d3ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'stadium': [{'field name': 'Stadium_ID',\n",
       "   'is primary key': True,\n",
       "   'is foreign key': True,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'Location',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Name',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Capacity',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'Highest',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'Lowest',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'Average',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'number'}],\n",
       " 'singer': [{'field name': 'Singer_ID',\n",
       "   'is primary key': True,\n",
       "   'is foreign key': True,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'Name',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Country',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Song_Name',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Song_release_year',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Age',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'Is_male',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'others'}],\n",
       " 'concert': [{'field name': 'concert_ID',\n",
       "   'is primary key': True,\n",
       "   'is foreign key': True,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'concert_Name',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Theme',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Stadium_ID',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': True,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'Year',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'}],\n",
       " 'singer_in_concert': [{'field name': 'concert_ID',\n",
       "   'is primary key': True,\n",
       "   'is foreign key': True,\n",
       "   'type': 'number'},\n",
       "  {'field name': 'Singer_ID',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': True,\n",
       "   'type': 'text'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spider_dbs_dict['concert_singer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4085baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def db_dict_to_fmt_schema(db_dict):\n",
    "#     \"\"\"\n",
    "#     Args:\n",
    "#         db_dict: Dict[table_name, List[column_dict[\"field name\", \"is primary key\", \"is foreign key\", \"type\"]]]\n",
    "    \n",
    "#     Output:\n",
    "#         fmt_schema = (for reference) {\n",
    "#             \"db_table_names\": schema[\"table_names_original\"],\n",
    "#             \"db_column_names\": {\n",
    "#                 \"table_id\": [table_id for table_id, column_name in schema[\"column_names_original\"]],\n",
    "#                 \"column_name\": [column_name for table_id, column_name in schema[\"column_names_original\"]]\n",
    "#             },\n",
    "#             \"db_column_types\": schema[\"column_types\"],\n",
    "#             \"db_primary_keys\": [{\"column_id\": column_id} for column_id in schema[\"primary_keys\"]],\n",
    "#             \"db_foreign_keys\": [\n",
    "#                 {\"column_id\": column_id, \"other_column_id\": other_column_id}\n",
    "#                 for column_id, other_column_id in schema[\"foreign_keys\"]\n",
    "#             ],\n",
    "#         }\n",
    "#     \"\"\"\n",
    "\n",
    "#     db_table_names = []\n",
    "#     db_column_names = ['*']  # default values in spider, same below \n",
    "#     db_column_table_ids = [-1]\n",
    "#     db_column_types = ['text']\n",
    "#     db_primary_keys = []\n",
    "#     db_foreign_keys = []\n",
    "    \n",
    "#     # for a column name, find the primary key idx. If this is a foreign key and not primary key, then found a f-p pair\n",
    "#     # (assume the f-p pair have the same name)\n",
    "#     # This is not always true. Example DB: architecture::architect_id, college_1::PROF_NUM\n",
    "#     col_name2p_key_column_idx = dict()\n",
    "#     col_name2f_keys_column_idx = defaultdict(list)\n",
    "    \n",
    "#     for table_name, table_columns in db_dict.items():\n",
    "#         table_idx = len(db_table_names)\n",
    "#         db_table_names.append(table_name)\n",
    "        \n",
    "#         for col_dict in table_columns:\n",
    "#             col_idx = len(db_column_names)\n",
    "#             db_column_names.append(col_dict[\"field name\"])\n",
    "#             db_column_table_ids.append(table_idx)\n",
    "#             db_column_types.append(col_dict[\"type\"])\n",
    "            \n",
    "#             # in michigan datasets, \"is primary||foreign key\" is \"y\"/\"n\"; in spider and wikisql, it is true/false\n",
    "#             if col_dict[\"is primary key\"] in {True, 'y'}:\n",
    "#                 db_primary_keys.append({\"column_id\": col_idx})\n",
    "#                 if col_dict[\"field name\"] in col_name2p_key_column_idx:\n",
    "#                     ## already exists?? Yes, there could be f-p pairs where both are primary! \n",
    "#                     # print(f'Warning: {col_dict[\"field name\"]} already exists')\n",
    "#                     pass\n",
    "#                 else:\n",
    "#                     col_name2p_key_column_idx[col_dict[\"field name\"]] = col_idx\n",
    "            \n",
    "#             if col_dict[\"is foreign key\"] in {True, 'y'}:\n",
    "#                 col_name2f_keys_column_idx[col_dict[\"field name\"]].append(col_idx)\n",
    "        \n",
    "#     for col_name in col_name2f_keys_column_idx.keys():\n",
    "#         f_key_column_ids = col_name2f_keys_column_idx[col_name]\n",
    "#         if col_name in col_name2p_key_column_idx:\n",
    "#             p_key_column_idx = col_name2p_key_column_idx[col_name]\n",
    "#         else:\n",
    "#             print(f'Warning: {col_name}, no primary key found')\n",
    "#             continue\n",
    "            \n",
    "#         for f_key_column_idx in f_key_column_ids:\n",
    "#             if f_key_column_idx != p_key_column_idx:\n",
    "#                 db_foreign_keys.append({\"column_id\": f_key_column_idx, \"other_column_id\": p_key_column_idx})\n",
    "    \n",
    "#     return {\n",
    "#         \"db_table_names\": db_table_names,\n",
    "#         \"db_column_names\": {\n",
    "#             \"table_id\": db_column_table_ids,\n",
    "#             \"column_name\": db_column_names,\n",
    "#         },\n",
    "#         \"db_column_types\": db_column_types,\n",
    "#         \"db_primary_keys\": db_primary_keys,\n",
    "#         \"db_foreign_keys\": db_foreign_keys\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6bfc7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_fmt_dict_to_uskg_schema(general_fmt_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        general_fmt_dict (Dict): {\n",
    "            \"db_id\": str\n",
    "            \"table_names_original\": List[str], original table name (concert_singer)\n",
    "            \"table_names_clean\": List[str], clean table names (concert_singer)\n",
    "            \"column_names_original\": List[str], original column name (singer_id)\n",
    "            \"column_names_clean\": List[str], clean columns names (singer id)\n",
    "            \"column_db_full_names\": List[str], name of table::column in DB (may differ from column_names) (singer::singer_id)\n",
    "            \"column_table_ids\": List[int], for each column, the corresponding table index\n",
    "            \"column_types\": List[str], column types\n",
    "            \"primary_keys\": List[int], the columns indices that are primary key\n",
    "            \"foreign_keys\": List[[int, int]], the f-p column index pairs (fk_id, pk_id)\n",
    "            \"sqlite_path\": str\n",
    "            \"sqlite_conn\": sqlite3.Connection\n",
    "        }\n",
    "    \n",
    "    Output:\n",
    "        uskg_schema = (for reference) {\n",
    "            \"db_table_names\": schema[\"table_names_original\"],\n",
    "            \"db_column_names\": {\n",
    "                \"table_id\": [table_id for table_id, column_name in schema[\"column_names_original\"]],\n",
    "                \"column_name\": [column_name for table_id, column_name in schema[\"column_names_original\"]]\n",
    "            },\n",
    "            \"db_column_types\": schema[\"column_types\"],\n",
    "            \"db_primary_keys\": [{\"column_id\": column_id} for column_id in schema[\"primary_keys\"]],\n",
    "            \"db_foreign_keys\": [\n",
    "                {\"column_id\": column_id, \"other_column_id\": other_column_id}\n",
    "                for column_id, other_column_id in schema[\"foreign_keys\"]\n",
    "            ],\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    db_id = general_fmt_dict[\"db_id\"]\n",
    "    db_table_orig_names = general_fmt_dict[\"table_names_original\"]\n",
    "    db_table_clean_names = general_fmt_dict[\"table_names_clean\"]\n",
    "    db_column_orig_names = general_fmt_dict[\"column_names_original\"]\n",
    "    db_column_clean_names = general_fmt_dict[\"column_names_clean\"]\n",
    "    col_db_full_names = general_fmt_dict[\"column_db_full_names\"]\n",
    "    db_column_table_ids = general_fmt_dict[\"column_table_ids\"]\n",
    "    db_column_types = general_fmt_dict[\"column_types\"]\n",
    "    db_primary_keys = general_fmt_dict[\"primary_keys\"]\n",
    "    db_foreign_keys = general_fmt_dict[\"foreign_keys\"]\n",
    "    sqlite_path = general_fmt_dict[\"sqlite_path\"]\n",
    "    sqlite_conn = general_fmt_dict[\"sqlite_conn\"]\n",
    "    \n",
    "    # USKG specific\n",
    "    uskg_primary_keys = [{\"column_id\": col_idx} for col_idx in db_primary_keys]\n",
    "    uskg_foreign_keys = [{\"column_id\": fk_idx, \"other_column_id\": pk_idx} for fk_idx, pk_idx in db_foreign_keys]\n",
    "\n",
    "    uskg_schema = {\n",
    "        \"db_id\": db_id,\n",
    "        \"db_table_names\": db_table_orig_names,\n",
    "        \"db_column_names\": {\n",
    "            \"table_id\": db_column_table_ids,\n",
    "            \"column_name\": db_column_orig_names,\n",
    "        },\n",
    "        \"db_column_types\": db_column_types,\n",
    "        \"db_primary_keys\": [{\"column_id\": column_id} for column_id in db_primary_keys],\n",
    "        \"db_foreign_keys\": [\n",
    "            {\"column_id\": column_id, \"other_column_id\": other_column_id}\n",
    "            for column_id, other_column_id in db_foreign_keys\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    return uskg_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3861347",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_id = 'architecture'\n",
    "db_dict = spider_dbs_dict[db_id]\n",
    "\n",
    "general_fmt_dict = db_dict_to_general_fmt(db_dict, db_id,\n",
    "                                          sqlite_path=f\"/Users/mac/Desktop/syt/Deep-Learning/Repos/Google-Research-Language/language/language/xsp/data/spider/database/{db_id}/{db_id}.sqlite\",\n",
    "                                          rigorous_foreign_key=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b043156",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'architecture',\n",
       " 'table_names_original': ['architect', 'bridge', 'mill'],\n",
       " 'table_names_clean': ['architect', 'bridge', 'mill'],\n",
       " 'column_names_original': ['*',\n",
       "  'id',\n",
       "  'name',\n",
       "  'nationality',\n",
       "  'gender',\n",
       "  'architect_id',\n",
       "  'id',\n",
       "  'name',\n",
       "  'location',\n",
       "  'length_meters',\n",
       "  'length_feet',\n",
       "  'architect_id',\n",
       "  'id',\n",
       "  'location',\n",
       "  'name',\n",
       "  'type',\n",
       "  'built_year',\n",
       "  'notes'],\n",
       " 'column_names_clean': ['*',\n",
       "  'id',\n",
       "  'name',\n",
       "  'nationality',\n",
       "  'gender',\n",
       "  'architect id',\n",
       "  'id',\n",
       "  'name',\n",
       "  'location',\n",
       "  'length meters',\n",
       "  'length feet',\n",
       "  'architect id',\n",
       "  'id',\n",
       "  'location',\n",
       "  'name',\n",
       "  'type',\n",
       "  'built year',\n",
       "  'notes'],\n",
       " 'column_db_full_names': ['NONE::*',\n",
       "  'architect::id',\n",
       "  'architect::name',\n",
       "  'architect::nationality',\n",
       "  'architect::gender',\n",
       "  'bridge::architect_id',\n",
       "  'bridge::id',\n",
       "  'bridge::name',\n",
       "  'bridge::location',\n",
       "  'bridge::length_meters',\n",
       "  'bridge::length_feet',\n",
       "  'mill::architect_id',\n",
       "  'mill::id',\n",
       "  'mill::location',\n",
       "  'mill::name',\n",
       "  'mill::type',\n",
       "  'mill::built_year',\n",
       "  'mill::notes'],\n",
       " 'column_table_ids': [-1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2],\n",
       " 'column_types': ['text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text'],\n",
       " 'primary_keys': [1, 6, 12],\n",
       " 'foreign_keys': [[5, 1], [11, 1]],\n",
       " 'sqlite_path': '/Users/mac/Desktop/syt/Deep-Learning/Repos/Google-Research-Language/language/language/xsp/data/spider/database/architecture/architecture.sqlite',\n",
       " 'sqlite_conn': <sqlite3.Connection at 0x7fa6cdfd3b90>}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_fmt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2051e8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt_schema_2 = general_fmt_dict_to_uskg_schema(general_fmt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ced9134b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'architecture',\n",
       " 'db_table_names': ['architect', 'bridge', 'mill'],\n",
       " 'db_column_names': {'table_id': [-1,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2,\n",
       "   2],\n",
       "  'column_name': ['*',\n",
       "   'id',\n",
       "   'name',\n",
       "   'nationality',\n",
       "   'gender',\n",
       "   'architect_id',\n",
       "   'id',\n",
       "   'name',\n",
       "   'location',\n",
       "   'length_meters',\n",
       "   'length_feet',\n",
       "   'architect_id',\n",
       "   'id',\n",
       "   'location',\n",
       "   'name',\n",
       "   'type',\n",
       "   'built_year',\n",
       "   'notes']},\n",
       " 'db_column_types': ['text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text'],\n",
       " 'db_primary_keys': [{'column_id': 1}, {'column_id': 6}, {'column_id': 12}],\n",
       " 'db_foreign_keys': [{'column_id': 5, 'other_column_id': 1},\n",
       "  {'column_id': 11, 'other_column_id': 1}]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmt_schema_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b9e2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fmt_schema = format_spider_schema(db_id)\n",
    "# del fmt_schema['db_id']\n",
    "del fmt_schema['db_path']\n",
    "\n",
    "fmt_schema == fmt_schema_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05727041",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(fmt_schema[\"db_foreign_keys\"], key=lambda d: d['column_id']), sorted(fmt_schema_2[\"db_foreign_keys\"], key=lambda d: d['column_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc097416",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enumerate(fmt_schema['db_column_names']['column_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad07359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfc6208e",
   "metadata": {},
   "source": [
    "### WikiSQL\n",
    "- Should be able to reuse db_dict_to_fmt_schema()\n",
    "- KEEP these cells as concrete examples for Wikisql db_dict and general_fmt_dict!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6b0650d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsp_data_dir = \"/home/yshao/Projects/language/language/xsp/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cf0ea227",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikisql_tables_path = os.path.join(xsp_data_dir, 'wikisql', 'dev.tables.jsonl')\n",
    "\n",
    "wikisql_dbs_dict = wikisql_preprocessing.load_wikisql_tables(wikisql_tables_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "03144368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking: No primary keys and foreign keys in wikisql\n",
    "\n",
    "for db_id, db_dict in wikisql_dbs_dict.items():\n",
    "    col_list = list(db_dict.values())[0]\n",
    "    for col_dict in col_list:\n",
    "        if col_dict['is primary key'] or col_dict['is foreign key']:\n",
    "            print(db_dict)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cb5c71f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['RACE_RESULTS']),\n",
       " [{'field name': 'RD',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'RACE',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'POLE_POSITION',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'FASTEST_LAP',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'MOST_LAPS_LED',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'WINNING_DRIVER',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'WINNING_TEAM',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'}])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikisql_dbs_dict['1-29690363-3'].keys(), wikisql_dbs_dict['1-29690363-3']['RACE_RESULTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "893764f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': '1-29690363-3',\n",
       " 'table_names_original': ['RACE_RESULTS'],\n",
       " 'table_names_clean': ['race results'],\n",
       " 'column_names_original': ['*',\n",
       "  'RD',\n",
       "  'RACE',\n",
       "  'POLE_POSITION',\n",
       "  'FASTEST_LAP',\n",
       "  'MOST_LAPS_LED',\n",
       "  'WINNING_DRIVER',\n",
       "  'WINNING_TEAM'],\n",
       " 'column_names_clean': ['*',\n",
       "  'rd',\n",
       "  'race',\n",
       "  'pole position',\n",
       "  'fastest lap',\n",
       "  'most laps led',\n",
       "  'winning driver',\n",
       "  'winning team'],\n",
       " 'column_db_full_names': ['NONE::*'],\n",
       " 'column_table_ids': [-1, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'column_types': ['text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text',\n",
       "  'text'],\n",
       " 'primary_keys': [],\n",
       " 'foreign_keys': [],\n",
       " 'sqlite_path': None,\n",
       " 'sqlite_conn': None}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# db_dict_to_fmt_schema(wikisql_dbs_dict['1-29690363-3'])\n",
    "db_dict_to_general_fmt(wikisql_dbs_dict['1-29690363-3'], '1-29690363-3', rigorous_foreign_key=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a01063",
   "metadata": {},
   "source": [
    "### Michigan\n",
    "- Should be able to reuse db_dict_to_fmt_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d4394ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "atis_schema_path = os.path.join(xsp_data_dir, 'atis', 'atis_schema.csv')\n",
    "\n",
    "atis_db_dict = michigan_preprocessing.read_schema(atis_schema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964065fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "atis_db_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf5a696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "db_dict_to_fmt_schema(atis_db_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd95cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f288148f",
   "metadata": {},
   "source": [
    "## Get USKG encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "230f7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "#     - set up inference (for sanity check)\n",
    "#     - look into source (of transformers Bert.generate) to find ways to get the encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782cd80e",
   "metadata": {},
   "source": [
    "### Check USKG inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d548f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_pred(txt, model, tokenizer):\n",
    "    tokenized_txt = tokenizer([txt], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "    pred = tokenizer.batch_decode(\n",
    "      model.generate(\n",
    "        torch.LongTensor(tokenized_txt.data['input_ids']),\n",
    "        torch.LongTensor(tokenized_txt.data['attention_mask']),\n",
    "        num_beams=1, \n",
    "        max_length=256\n",
    "        ), \n",
    "      skip_special_tokens=True \n",
    "    )\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3559d196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set args here for runnning on notebook, we make them out here to make it more illustrative.\n",
    "sys.argv = ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', # This is the name of your .py launcher when you run this line of code.\n",
    "            # belows are the parameters we set, take spider for example\n",
    "            '--cfg', 'Salesforce/T5_large_prefix_spider_with_cell_value.cfg', \n",
    "            '--output_dir', './tmp']\n",
    "parser = HfArgumentParser((WrappedSeq2SeqTrainingArguments,))\n",
    "training_args, = parser.parse_args_into_dataclasses()\n",
    "set_seed(training_args.seed)\n",
    "args = Configure.Get(training_args.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7935884c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix-tuning sequence length is 10.\n"
     ]
    }
   ],
   "source": [
    "model_path = 'hkunlp/from_all_T5_large_prefix_spider_with_cell_value2'\n",
    "# model_path = 'hkunlp/from_all_T5_large_prefix_spider_with_cell_value2'\n",
    "# model_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/UnifiedSKG/output/server_runs/A-T5_base_prefix_spider_with_cell_value-asr_mixed/checkpoint-79500/'\n",
    "# model_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/UnifiedSKG/output/server_runs/A-T5_base_prefix_spider_with_cell_value-rewritten_mixed/checkpoint-56500/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "\n",
    "# for reconstruction\n",
    "tokenizer_fast = AutoTokenizer.from_pretrained('t5-large', use_fast=True)\n",
    "\n",
    "model = Model(args)\n",
    "model.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed3ac6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_large_fast = AutoTokenizer.from_pretrained('t5-large', use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "953185c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select min(age), avg(age), max(age) from singer where country = \"France\"']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_in = \"| concert_singer | stadium : stadium_id , location , name , capacity , highest , lowest , average | singer : singer_id , name , country ( France ) , song_name , song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id , year | singer_in_concert : concert_id , singer_id\"\n",
    "text_in = \"what is the minimum, average, and maximum age of all singers from France?\"\n",
    "\n",
    "play_pred(\"{}; structed knowledge: {}\".format(text_in, struct_in), model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77f4f478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['select min(age), avg(age), max(age) from singer where country = \"France\"']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_pred(\"{}; structed knowledge: {}\".format(text_in, struct_in), model, tokenizer_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed4ab0",
   "metadata": {},
   "source": [
    "### Check USKG inference on dataset samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "3a2a80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xsp_data_dir = \"/Users/mac/Desktop/syt/Deep-Learning/Repos/Google-Research-Language/language/language/xsp/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1eac4f7",
   "metadata": {},
   "source": [
    "#### Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a7e83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dataset_path = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/spider/dev+ratsql_graph.json\"\n",
    "orig_tables_path = \"/Users/mac/Desktop/syt/Deep-Learning/Repos/Google-Research-Language/language/language/xsp/data/spider/tables.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3b96ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spider_dbs_dict = spider_preprocessing.load_spider_tables(orig_tables_path)\n",
    "len(spider_dbs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80be11f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: (db_id: imdb) keyword::kid -- tags::kid pair invalid, skipped\n",
      "Warning: (db_id: baseball_1) player::team_id -- fielding_postseason::team_id pair invalid, skipped\n",
      "Warning: (db_id: restaurants) restaurant::restaurant_id -- location::restaurant_id pair invalid, skipped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uskg_schemas_dict = dict()\n",
    "for db_id, db_dict in spider_dbs_dict.items():\n",
    "    general_fmt_dict = db_dict_to_general_fmt(db_dict, db_id,\n",
    "                                              sqlite_path=os.path.join(xsp_data_dir, f\"spider/database/{db_id}/{db_id}.sqlite\"),\n",
    "                                              rigorous_foreign_key=True)\n",
    "    \n",
    "    uskg_schema = general_fmt_dict_to_uskg_schema(general_fmt_dict)\n",
    "    \n",
    "    uskg_schemas_dict[db_id] = uskg_schema\n",
    "    \n",
    "len(uskg_schemas_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b57aa94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(orig_dataset_path, 'r') as f:\n",
    "    orig_dataset = json.load(f)\n",
    "\n",
    "for d in orig_dataset:\n",
    "    d['rat_sql_graph']['relations'] = json.loads(d['rat_sql_graph']['relations'])\n",
    "\n",
    "len(orig_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "86bbde97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql', 'rat_sql_graph'])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d3ecf7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPIDER_DB_PATH = os.path.join(xsp_data_dir, \"spider/database\")\n",
    "\n",
    "def uskg_sample_to_struct_input(uskg_sample):\n",
    "    db_id = uskg_sample[\"db_id\"]\n",
    "    uskg_schema = uskg_schemas_dict[db_id]\n",
    "    \n",
    "    return spider.serialize_schema(\n",
    "        question=uskg_sample[\"question\"],\n",
    "        db_path=SPIDER_DB_PATH,\n",
    "        db_id=db_id,\n",
    "        db_column_names=uskg_schema[\"db_column_names\"],\n",
    "        db_table_names=uskg_schema[\"db_table_names\"],\n",
    "        schema_serialization_type=\"peteshaw\",\n",
    "        schema_serialization_randomized=False,\n",
    "        schema_serialization_with_db_id=True,\n",
    "        schema_serialization_with_db_content=True,\n",
    "        normalize_query=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "e31e5e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql', 'rat_sql_graph']),\n",
       " dict_keys(['nodes', 'q_nodes_orig', 'relations']))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 777\n",
    "sample = orig_dataset[idx]\n",
    "sample.keys(), sample['rat_sql_graph'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "647b9f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What are the Asian countries which have a population larger than that of any country in Africa?',\n",
       " ' | world_1 | city : id , name , countrycode , district , population | sqlite_sequence : name , seq | country : code , name , continent ( Africa , Asia ) , region , surfacearea , indepyear , population , lifeexpectancy , gnp , gnpold , localname , governmentform , headofstate , capital , code2 | countrylanguage : countrycode , language , isofficial , percentage')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_in = sample['question']\n",
    "struct_in = uskg_sample_to_struct_input(sample)\n",
    "text_in, struct_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb118104",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_pred(\"{}; structed knowledge: {}\".format(text_in, struct_in), model, tokenizer_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22544ec6",
   "metadata": {},
   "source": [
    "#### Wikisql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "7c971779",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dataset_path = \"/Users/mac/Desktop/syt/Deep-Learning/Repos/Google-Research-Language/language/language/xsp/data/wikisql/dev.jsonl\"\n",
    "orig_tables_path = \"/Users/mac/Desktop/syt/Deep-Learning/Repos/Google-Research-Language/language/language/xsp/data/wikisql/dev.tables.jsonl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "f49990bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8421"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset \n",
    "with open(orig_dataset_path, 'r') as f:\n",
    "    wikisql_dataset = [json.loads(l) for l in f]\n",
    "len(wikisql_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "68684702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phase': 1,\n",
       " 'table_id': '1-10015132-11',\n",
       " 'question': 'What position does the player who played for butler cc (ks) play?',\n",
       " 'sql': {'sel': 3, 'conds': [[5, 0, 'Butler CC (KS)']], 'agg': 0}}"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = wikisql_dataset[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "a14e4d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2716"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dbs \n",
    "wikisql_dbs_dict = wikisql_preprocessing.load_wikisql_tables(orig_tables_path)\n",
    "len(wikisql_dbs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "e7915dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1-10015132-11',\n",
       " '1-19422702-1',\n",
       " '1-27756314-8',\n",
       " '2-10932739-3',\n",
       " '2-11999396-1',\n",
       " '2-13845847-2',\n",
       " '2-15442855-1',\n",
       " '2-16977283-2',\n",
       " '2-17822-4',\n",
       " '2-18964684-2']"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(wikisql_dbs_dict.keys())[::300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "f06f7f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'L': [{'field name': 'PLAYER',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'NO',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'NATIONALITY',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'POSITION',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'YEARS_IN_TORONTO',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'},\n",
       "  {'field name': 'SCHOOL_OR_CLUB_TEAM',\n",
       "   'is primary key': False,\n",
       "   'is foreign key': False,\n",
       "   'type': 'text'}]}"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikisql_dbs_dict['1-10015132-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "787f7977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlite3.Connection"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for wikisql, get the sqlite_conn first \n",
    "db_id = sample['table_id']\n",
    "db_dict = wikisql_dbs_dict[db_id]\n",
    "general_fmt_dict = db_dict_to_general_fmt(db_dict, db_id,\n",
    "                                          sqlite_path=os.path.join(xsp_data_dir, \"wikisql/dev.db\"),\n",
    "                                          rigorous_foreign_key=False)\n",
    "\n",
    "wikisql_sqlite_conn = general_fmt_dict['sqlite_conn']\n",
    "type(wikisql_sqlite_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "1288d1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': '1-10015132-11',\n",
       " 'table_names_original': ['L'],\n",
       " 'table_names_clean': ['l'],\n",
       " 'column_names_original': ['*',\n",
       "  'PLAYER',\n",
       "  'NO',\n",
       "  'NATIONALITY',\n",
       "  'POSITION',\n",
       "  'YEARS_IN_TORONTO',\n",
       "  'SCHOOL_OR_CLUB_TEAM'],\n",
       " 'column_names_clean': ['*',\n",
       "  'player',\n",
       "  'no',\n",
       "  'nationality',\n",
       "  'position',\n",
       "  'years in toronto',\n",
       "  'school or club team'],\n",
       " 'column_db_full_names': ['NONE::*'],\n",
       " 'column_table_ids': [-1, 0, 0, 0, 0, 0, 0],\n",
       " 'column_types': ['text', 'text', 'text', 'text', 'text', 'text', 'text'],\n",
       " 'primary_keys': [],\n",
       " 'foreign_keys': [],\n",
       " 'sqlite_path': '/Users/mac/Desktop/syt/Deep-Learning/Repos/Google-Research-Language/language/language/xsp/data/wikisql/dev.db',\n",
       " 'sqlite_conn': <sqlite3.Connection at 0x7fa6cf67d2d0>}"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_fmt_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "a19f2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uskg_schema = general_fmt_dict_to_uskg_schema(general_fmt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "e528f4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': '1-10015132-11',\n",
       " 'db_table_names': ['L'],\n",
       " 'db_column_names': {'table_id': [-1, 0, 0, 0, 0, 0, 0],\n",
       "  'column_name': ['*',\n",
       "   'PLAYER',\n",
       "   'NO',\n",
       "   'NATIONALITY',\n",
       "   'POSITION',\n",
       "   'YEARS_IN_TORONTO',\n",
       "   'SCHOOL_OR_CLUB_TEAM']},\n",
       " 'db_column_types': ['text', 'text', 'text', 'text', 'text', 'text', 'text'],\n",
       " 'db_primary_keys': [],\n",
       " 'db_foreign_keys': []}"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uskg_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "fb1706de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look into wikisql db \n",
    "def sql_identifier(s):\n",
    "    return '\"' + s.replace('\"', '\"\"') + '\"'\n",
    "\n",
    "rows = wikisql_sqlite_conn.execute(\"PRAGMA table_info({})\".format(sql_identifier('table_1_10015132_11')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "db230749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cid': 0, 'name': 'col0', 'type': 'text', 'notnull': 0, 'dflt_value': None, 'pk': 0}\n",
      "{'cid': 1, 'name': 'col1', 'type': 'text', 'notnull': 0, 'dflt_value': None, 'pk': 0}\n",
      "{'cid': 2, 'name': 'col2', 'type': 'text', 'notnull': 0, 'dflt_value': None, 'pk': 0}\n",
      "{'cid': 3, 'name': 'col3', 'type': 'text', 'notnull': 0, 'dflt_value': None, 'pk': 0}\n",
      "{'cid': 4, 'name': 'col4', 'type': 'text', 'notnull': 0, 'dflt_value': None, 'pk': 0}\n",
      "{'cid': 5, 'name': 'col5', 'type': 'text', 'notnull': 0, 'dflt_value': None, 'pk': 0}\n"
     ]
    }
   ],
   "source": [
    "for r in rows.fetchall():\n",
    "    print(dict(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11096167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the correct table names?? - \"table_xxx_xxx\" (when db_id = \"xxx-xxx\")\n",
    "rows = wikisql_sqlite_conn.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70278c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in rows.fetchmany(10):\n",
    "    print(dict(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d85b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "uskg_schemas_dict = dict()\n",
    "for db_id, db_dict in wikisql_dbs_dict.items():\n",
    "    general_fmt_dict = db_dict_to_general_fmt(db_dict, db_id,\n",
    "                                              sqlite_conn=wikisql_sqlite_conn,\n",
    "                                              rigorous_foreign_key=False)\n",
    "    \n",
    "    uskg_schema = general_fmt_dict_to_uskg_schema(general_fmt_dict)\n",
    "    \n",
    "    uskg_schemas_dict[db_id] = uskg_schema\n",
    "    \n",
    "len(uskg_schemas_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "eff9d344",
   "metadata": {},
   "outputs": [],
   "source": [
    "## schema serialization for wikisql \n",
    "\n",
    "def _wikisql_db_id_to_table_name(db_id):\n",
    "    return '_'.join(['table'] + db_id.split('-'))\n",
    "\n",
    "def serialize_schema_wikisql(\n",
    "        question: str,\n",
    "        db_path: str,\n",
    "        db_id: str,\n",
    "        db_column_names: Dict[str, str],\n",
    "        db_table_names: List[str],\n",
    "        schema_serialization_type: str = \"peteshaw\",\n",
    "        schema_serialization_randomized: bool = False,\n",
    "        schema_serialization_with_db_id: bool = True,\n",
    "        schema_serialization_with_db_content: bool = False,\n",
    "        normalize_query: bool = True,\n",
    ") -> str:\n",
    "    if schema_serialization_type == \"verbose\":\n",
    "        db_id_str = \"Database: {db_id}. \"\n",
    "        table_sep = \". \"\n",
    "        table_str = \"Table: {table}. Columns: {columns}\"\n",
    "        column_sep = \", \"\n",
    "        column_str_with_values = \"{column} ({values})\"\n",
    "        column_str_without_values = \"{column}\"\n",
    "        value_sep = \", \"\n",
    "    elif schema_serialization_type == \"peteshaw\":\n",
    "        # see https://github.com/google-research/language/blob/master/language/nqg/tasks/spider/append_schema.py#L42\n",
    "        db_id_str = \" | {db_id}\"\n",
    "        table_sep = \"\"\n",
    "        table_str = \" | {table} : {columns}\"\n",
    "        column_sep = \" , \"\n",
    "        column_str_with_values = \"{column} ( {values} )\"\n",
    "        column_str_without_values = \"{column}\"\n",
    "        value_sep = \" , \"\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_column_str(table_name: str, column_name: str, sqlite_table_name: str, sqlite_column_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            table_name: the original table name in wikisql dataset (natural langauge)\n",
    "            column_name: the name is wikisql dataset (natural langauge)\n",
    "            sqlite_table_name: the table name in .db (table_xxx_xxx)\n",
    "            sqlite_column_name: the column name in .db (col0, col1, ...)\n",
    "        \"\"\"\n",
    "        column_name_str = column_name.lower() if normalize_query else column_name\n",
    "        if schema_serialization_with_db_content:\n",
    "            matches = get_database_matches(\n",
    "                question=question,\n",
    "                table_name=sqlite_table_name,\n",
    "                column_name=sqlite_column_name,\n",
    "                # db_path=(db_path + \"/\" + db_id + \"/\" + db_id + \".sqlite\"),\n",
    "                db_path=db_path,\n",
    "            )\n",
    "            if matches:\n",
    "                return column_str_with_values.format(\n",
    "                    column=column_name_str, values=value_sep.join(matches)\n",
    "                )\n",
    "            else:\n",
    "                return column_str_without_values.format(column=column_name_str)\n",
    "        else:\n",
    "            return column_str_without_values.format(column=column_name_str)\n",
    "\n",
    "#     tables = [\n",
    "#         table_str.format(\n",
    "#             table=table_name.lower() if normalize_query else table_name,\n",
    "#             columns=column_sep.join(\n",
    "#                 map(\n",
    "#                     lambda y: get_column_str(table_name=table_name,\n",
    "#                                              column_name=y[1],\n",
    "#                                              sqlite_table_name=_wikisql_db_id_to_table_name(db_id),\n",
    "#                                              sqlite_column_name=f'col{y[2]}'),\n",
    "#                     filter(\n",
    "#                         lambda y: y[0] == table_id,\n",
    "#                         zip(\n",
    "#                             db_column_names[\"table_id\"],\n",
    "#                             db_column_names[\"column_name\"],\n",
    "#                             range(len(db_column_names[\"table_id\"])),  # incorrect, should be in-table index! \n",
    "#                         ),\n",
    "#                     ),\n",
    "#                 )\n",
    "#             ),\n",
    "#         )\n",
    "#         for table_id, table_name in enumerate(db_table_names)\n",
    "#     ]\n",
    "\n",
    "    tables = []\n",
    "    for table_id, table_name in enumerate(db_table_names):\n",
    "        table_col_idx = 0\n",
    "        column_str_list = []\n",
    "        for col_table_id, col_name in zip(db_column_names[\"table_id\"], db_column_names[\"column_name\"]):\n",
    "            if col_table_id != table_id:\n",
    "                continue\n",
    "            column_str_list.append(get_column_str(table_name=table_name,\n",
    "                                                  column_name=col_name,\n",
    "                                                  sqlite_table_name=_wikisql_db_id_to_table_name(db_id),\n",
    "                                                  sqlite_column_name=f'col{table_col_idx}'))\n",
    "            table_col_idx += 1\n",
    "            \n",
    "        tables.append(table_str.format(\n",
    "            table=table_name.lower() if normalize_query else table_name,\n",
    "            columns=column_sep.join(column_str_list),\n",
    "        ))\n",
    "\n",
    "    if schema_serialization_randomized:\n",
    "        random.shuffle(tables)\n",
    "    if schema_serialization_with_db_id:\n",
    "        serialized_schema = db_id_str.format(db_id=db_id) + table_sep.join(tables)\n",
    "    else:\n",
    "        serialized_schema = table_sep.join(tables)\n",
    "    return serialized_schema\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "5533413d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'phase': 1,\n",
       "  'table_id': '1-10015132-11',\n",
       "  'question': 'What position does the player who played for butler cc (ks) play?',\n",
       "  'sql': {'sel': 3, 'conds': [[5, 0, 'Butler CC (KS)']], 'agg': 0}},\n",
       " {'db_id': '1-10015132-11',\n",
       "  'db_table_names': ['L'],\n",
       "  'db_column_names': {'table_id': [-1, 0, 0, 0, 0, 0, 0],\n",
       "   'column_name': ['*',\n",
       "    'PLAYER',\n",
       "    'NO',\n",
       "    'NATIONALITY',\n",
       "    'POSITION',\n",
       "    'YEARS_IN_TORONTO',\n",
       "    'SCHOOL_OR_CLUB_TEAM']},\n",
       "  'db_column_types': ['text', 'text', 'text', 'text', 'text', 'text', 'text'],\n",
       "  'db_primary_keys': [],\n",
       "  'db_foreign_keys': []})"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample, uskg_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "0f05eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_id = sample[\"table_id\"]\n",
    "\n",
    "struct_in = serialize_schema_wikisql(\n",
    "    question=sample[\"question\"],\n",
    "    db_path=os.path.join(xsp_data_dir, \"wikisql/dev.db\"),\n",
    "    db_id=db_id,\n",
    "    db_column_names=uskg_schema[\"db_column_names\"],\n",
    "    db_table_names=uskg_schema[\"db_table_names\"],\n",
    "    schema_serialization_type=\"peteshaw\",\n",
    "    schema_serialization_randomized=False,\n",
    "    schema_serialization_with_db_id=True,\n",
    "    schema_serialization_with_db_content=True,\n",
    "    normalize_query=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "8eb0edef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' | 1-10015132-11 | l : player , no , nationality , position , years_in_toronto , school_or_club_team ( butler cc (ks) )'"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d802d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35c42d89",
   "metadata": {},
   "source": [
    "### Tokenized pieces-nodes mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9bec039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (done):\n",
    "#     find mapping between tokenized pieces to ratsql nodes\n",
    "#         ratsql nodes to sentence char ids (done)\n",
    "#         sentence char ids to tokenized pieces (done)\n",
    "#     pool the encodings of pieces in a node (done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d9b25bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructCharRangesCollector:\n",
    "    def __init__(self):\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        self.db_id2char_ranges = dict()\n",
    "        self.table2char_ranges = dict()\n",
    "        self.column2char_ranges = dict()\n",
    "        \n",
    "        # Due to rat-sql stemming tokens, rat-sql nodes and uskg text may mismatch,\n",
    "        # so we save a list and use the order instead of name for indexing \n",
    "        self.db_id_char_ranges_list = []\n",
    "        self.column_char_ranges_list = []\n",
    "        self.table_char_ranges_list = []\n",
    "\n",
    "        self.bar_cnt = 0\n",
    "        self.curr_table = None\n",
    "        self.curr_node_type = None   # [None, 'db_id', 'table', 'column']\n",
    "        self.curr_node_toks = []\n",
    "        self.curr_node_char_start = None\n",
    "        self.curr_node_char_end = None\n",
    "        self.open_bracket = False\n",
    "        \n",
    "    def _register_curr_node(self):\n",
    "        curr_node_name = ' '.join(self.curr_node_toks)\n",
    "        curr_range = (self.curr_node_char_start, self.curr_node_char_end)\n",
    "        \n",
    "        if self.curr_node_type == 'db_id':\n",
    "            self.db_id2char_ranges[curr_node_name] = curr_range\n",
    "            self.db_id_char_ranges_list.append(curr_range)   \n",
    "        elif self.curr_node_type == 'table':\n",
    "            self.table2char_ranges[curr_node_name] = curr_range\n",
    "            self.table_char_ranges_list.append(curr_range)\n",
    "            self.curr_table = curr_node_name\n",
    "        elif self.curr_node_type == 'column':\n",
    "            self.column2char_ranges[(self.curr_table, curr_node_name)] = curr_range\n",
    "            self.column_char_ranges_list.append(curr_range)\n",
    "        else:\n",
    "            raise ValueError(curr_node_type)\n",
    "\n",
    "        self.curr_node_toks = []\n",
    "        self.curr_node_char_start = None\n",
    "        self.curr_node_char_end = None\n",
    "    \n",
    "    def collect(self, struct_in, tokenized_txt, _n_words_before_struct):\n",
    "#         struct_words = struct_in.strip().split(' ')\n",
    "        struct_words = struct_in.strip().split()\n",
    "        \n",
    "        for sw_id, sw in enumerate(struct_words):\n",
    "            char_range = tokenized_txt.word_to_chars(sw_id + _n_words_before_struct)\n",
    "\n",
    "            # print(sw_id, char_range, sw, self.curr_node_type, self.open_bracket)\n",
    "\n",
    "            if sw == '(':\n",
    "                self.open_bracket = True\n",
    "                continue\n",
    "\n",
    "            if sw == ')':\n",
    "                self.open_bracket = False\n",
    "                self.curr_node_char_end = char_range[1]\n",
    "                continue\n",
    "\n",
    "            if self.open_bracket:\n",
    "                # in the list of cells, do not add tokens here to name \n",
    "                continue\n",
    "\n",
    "            if sw == '|':\n",
    "                if self.curr_node_type is not None:\n",
    "                    self._register_curr_node()\n",
    "                self.bar_cnt += 1\n",
    "                if self.bar_cnt == 1:\n",
    "                    self.curr_node_type = 'db_id'\n",
    "                if self.bar_cnt > 1:\n",
    "                    self.curr_node_type = 'table'\n",
    "                continue\n",
    "\n",
    "            if sw == ':':\n",
    "                assert self.curr_node_type == 'table'\n",
    "                self._register_curr_node()\n",
    "                self.curr_node_type = 'column'\n",
    "                continue\n",
    "\n",
    "            if sw == ',':\n",
    "                assert self.curr_node_type == 'column'\n",
    "                self._register_curr_node()\n",
    "                self.curr_node_type = 'column'\n",
    "                continue\n",
    "\n",
    "            self.curr_node_toks.append(sw)\n",
    "            if self.curr_node_char_start is None:\n",
    "                self.curr_node_char_start = char_range[0]\n",
    "            self.curr_node_char_end = char_range[1]\n",
    "\n",
    "        self._register_curr_node()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ca43967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine experiment codes \n",
    "def collect_node_char_ranges(sample, tokenizer=None, tokenizer_args=None, txt=None, tokenized_txt=None, debug=False):\n",
    "    text_in = sample['question']\n",
    "    struct_in = uskg_sample_to_struct_input(sample)\n",
    "    _splitter = \"; structed knowledge: \"\n",
    "    \n",
    "    if tokenizer_args is None:\n",
    "        tokenizer_args = dict()\n",
    "        \n",
    "    if txt is None:\n",
    "        txt = \"{}{}{}\".format(text_in, _splitter, struct_in)\n",
    "    \n",
    "    if tokenized_txt is None:\n",
    "        # tokenized_txt = tokenizer([txt], max_length=1024, padding=\"max_length\", truncation=True)\n",
    "        tokenized_txt = tokenizer([txt], **tokenizer_args)\n",
    "        ## possible problem: exceeding max length!\n",
    "    \n",
    "    ratsql_graph_nodes = sample['rat_sql_graph']['nodes']\n",
    "#     question_toks = sample['question_toks']\n",
    "    question_toks = sample['rat_sql_graph']['q_nodes_orig']\n",
    "\n",
    "    _q_nodes = []  # [stem token (node name)]\n",
    "    q_nodes = []  # [(stem token (node name), orig question token)]\n",
    "    c_nodes = []  # [(orig table name, orig column name)]\n",
    "    t_nodes = []  # [orig table name]\n",
    "\n",
    "    for n in ratsql_graph_nodes:\n",
    "        if n.startswith('<C>'):\n",
    "            _n = n[3:]\n",
    "            _t, _c = _n.split('::')\n",
    "            c_nodes.append((_t, _c))\n",
    "        elif n.startswith('<T>'):\n",
    "            _n = n[3:]\n",
    "            t_nodes.append(_n)\n",
    "        else:\n",
    "            _q_nodes.append(n)\n",
    "\n",
    "    assert len(_q_nodes) == len(question_toks), (_q_nodes, question_toks)\n",
    "    q_nodes = list(zip(_q_nodes, question_toks))\n",
    "    \n",
    "    # Collection char ranges \n",
    "    q_node_chars = []   # [(st, ed)]; same below\n",
    "    c_node_chars = []\n",
    "    t_node_chars = []\n",
    "    \n",
    "    # Text part\n",
    "    # Assumption: the mismatch between whitespace words (text_words) and question words only come from trailing puncts\n",
    "    # Currently the code can handle combining question toks into whitespace words\n",
    "#     text_words = text_in.strip().split(' ') + ['<SENTINAL>']\n",
    "    text_words = text_in.lower().strip().split() + ['<SENTINAL>']\n",
    "    text_word_char_ranges = [tokenized_txt.word_to_chars(i) for i in range(len(text_words) - 1)] + [(None, None)]  # -1 to remove the sentinal \n",
    "\n",
    "    curr_tw_idx = 0\n",
    "    curr_tw = text_words[0]\n",
    "    curr_tw_char_range = text_word_char_ranges[0]\n",
    "    curr_char_ptr = 0\n",
    "    for stem_tok, orig_tok in q_nodes:\n",
    "        if curr_tw == orig_tok:\n",
    "            # finishing current word \n",
    "            q_node_chars.append((curr_char_ptr, curr_char_ptr + len(orig_tok)))   # curr pos to curr pos + len \n",
    "            curr_tw_idx += 1\n",
    "            curr_tw = text_words[curr_tw_idx]\n",
    "            curr_tw_char_range = text_word_char_ranges[curr_tw_idx]\n",
    "            curr_char_ptr = curr_tw_char_range[0]\n",
    "        else:\n",
    "            # not finishing current word \n",
    "            assert curr_tw.startswith(orig_tok), (curr_tw, orig_tok)\n",
    "            q_node_chars.append((curr_char_ptr, curr_char_ptr + len(orig_tok)))   # curr pos to curr pos + len \n",
    "            curr_char_ptr += len(orig_tok)     # move ptr forward by len \n",
    "            curr_tw = curr_tw[len(orig_tok):]  # get the remaining chars in the word \n",
    "\n",
    "    assert [txt[st:ed].lower() for st, ed in q_node_chars] == question_toks, ([txt[st:ed] for st, ed in q_node_chars], question_toks)\n",
    "    \n",
    "    # Struct part \n",
    "    _str_before_struct = text_in + _splitter\n",
    "    _n_words_before_struct = len(_str_before_struct.strip().split())\n",
    "\n",
    "    struct_ranges_collector = StructCharRangesCollector()\n",
    "    struct_ranges_collector.collect(struct_in, tokenized_txt, _n_words_before_struct)\n",
    "    \n",
    "    # Due to rat-sql stemming tokens, rat-sql nodes and uskg text may mismatch\n",
    "#     for c_node in c_nodes:\n",
    "#         if c_node == ('NONE', '*'):\n",
    "#             # the special column in spider, using db_id \n",
    "#             c_node_chars.append(list(struct_ranges_collector.db_id2char_ranges.values())[0])   # assuming only 1 db_id, which should be true...\n",
    "#         else:\n",
    "#             c_node_chars.append(struct_ranges_collector.column2char_ranges[c_node])\n",
    "\n",
    "#     for t_node in t_nodes:\n",
    "#         t_node_chars.append(struct_ranges_collector.table2char_ranges[t_node])\n",
    "\n",
    "    c_node_chars.extend(struct_ranges_collector.db_id_char_ranges_list + struct_ranges_collector.column_char_ranges_list)\n",
    "    t_node_chars.extend(struct_ranges_collector.table_char_ranges_list)\n",
    "\n",
    "    ## Check all \n",
    "    if debug:\n",
    "        for q_node, (st, ed) in zip(q_nodes, q_node_chars):\n",
    "            print(q_node, txt[st:ed])\n",
    "        print()\n",
    "        for t_node, (st, ed) in zip(t_nodes, t_node_chars):\n",
    "            print(t_node, txt[st:ed])\n",
    "        print()\n",
    "        for c_node, (st, ed) in zip(c_nodes, c_node_chars):\n",
    "            print(c_node, txt[st:ed])\n",
    "        \n",
    "    return {\n",
    "        \"q_node_chars\": q_node_chars,\n",
    "        \"c_node_chars\": c_node_chars,\n",
    "        \"t_node_chars\": t_node_chars,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8c5a6601",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 209\n",
    "sample = orig_dataset[idx]\n",
    "\n",
    "text_in = sample['question']\n",
    "struct_in = uskg_sample_to_struct_input(sample)\n",
    "\n",
    "txt = \"{}; structed knowledge: {}\".format(text_in, struct_in)\n",
    "\n",
    "tokenized_txt = tokenizer_fast([txt], max_length=1024, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "0094344d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('how', 'how') How\n",
      "('many', 'many') many\n",
      "('flight', 'flights') flights\n",
      "('arrive', 'arriving') arriving\n",
      "('in', 'in') in\n",
      "('aberdeen', 'aberdeen') Aberdeen\n",
      "('city', 'city') city\n",
      "('?', '?') ?\n",
      "\n",
      "airline airlines\n",
      "airport airports\n",
      "flight flights\n",
      "\n",
      "('NONE', '*') flight_2\n",
      "('airline', 'uid') uid\n",
      "('airline', 'airline') airline\n",
      "('airline', 'abbreviation') abbreviation\n",
      "('airline', 'country') country\n",
      "('airport', 'city') city ( Aberdeen  )\n",
      "('airport', 'airportcode') airportcode\n",
      "('airport', 'airportname') airportname\n",
      "('airport', 'country') country\n",
      "('airport', 'countryabbrev') countryabbrev\n",
      "('flight', 'airline') airline\n",
      "('flight', 'flightno') flightno\n",
      "('flight', 'sourceairport') sourceairport\n",
      "('flight', 'destairport') destairport\n"
     ]
    }
   ],
   "source": [
    "# tokenizer_args = {\n",
    "#     \"max_length\": 1024,\n",
    "#     \"padding\": \"max_length\",\n",
    "#     \"truncation\": True\n",
    "# }\n",
    "\n",
    "# char_ranges_dict = collect_node_char_ranges(sample, tokenizer=tokenizer_fast, tokenizer_args=tokenizer_args, debug=True)\n",
    "char_ranges_dict = collect_node_char_ranges(sample, txt=txt, tokenized_txt=tokenized_txt, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "956e805d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How many flights arriving in Aberdeen city?; structed knowledge:  | flight_2 | airlines : uid , airline , abbreviation , country | airports : city ( Aberdeen  ) , airportcode , airportname , country , countryabbrev | flights : airline , flightno , sourceairport , destairport'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4daa2812",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q_node_chars': [(0, 3),\n",
       "  (4, 8),\n",
       "  (9, 16),\n",
       "  (17, 25),\n",
       "  (26, 28),\n",
       "  (29, 37),\n",
       "  (38, 42),\n",
       "  (42, 43)],\n",
       " 'c_node_chars': [(68, 76),\n",
       "  (90, 93),\n",
       "  (96, 103),\n",
       "  (106, 118),\n",
       "  (121, 128),\n",
       "  (142, 160),\n",
       "  (163, 174),\n",
       "  (177, 188),\n",
       "  (191, 198),\n",
       "  (201, 214),\n",
       "  (227, 234),\n",
       "  (237, 245),\n",
       "  (248, 261),\n",
       "  (264, 275)],\n",
       " 't_node_chars': [(79, 87), (131, 139), (217, 224)]}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_ranges_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b08a188e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_txt.char_to_token(79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "99d00da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'airlines'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_txt.tokens()[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e6087dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tokenized_txt.data['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b258858",
   "metadata": {},
   "source": [
    "### Get encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4a0346a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_USKG_node_encodings(sample, model, tokenizer, tokenizer_args=None, pooling_func=None, debug=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pooling_func (Callable): np.array(n_pieces, dim) ==> np.array(dim,); default is np.mean\n",
    "    \"\"\"\n",
    "    text_in = sample['question']\n",
    "    struct_in = uskg_sample_to_struct_input(sample)\n",
    "    \n",
    "    _splitter = \"; structed knowledge: \"\n",
    "    txt = \"{}{}{}\".format(text_in, _splitter, struct_in)\n",
    "\n",
    "    if tokenizer_args is None:\n",
    "        tokenizer_args = {\n",
    "            \"max_length\": 1024,\n",
    "            \"padding\": \"max_length\",\n",
    "            \"truncation\": True\n",
    "        }\n",
    "    if pooling_func is None:\n",
    "        pooling_func = lambda l: np.mean(l, axis=0)\n",
    "        \n",
    "    tokenized_txt = tokenizer([txt], **tokenizer_args)\n",
    "    \n",
    "    # Get encoding tensor \n",
    "    with torch.no_grad():\n",
    "        past_prompt = model.get_prompt(\n",
    "            bsz=1,              # bsz = input_ids.shape[0]\n",
    "            sample_size=1,      # sample_size=kwargs['num_beams']\n",
    "            description=None,   \n",
    "            knowledge=None,     \n",
    "        )\n",
    "        encoder_outputs = model.pretrain_model.encoder(\n",
    "            input_ids=torch.LongTensor(tokenized_txt.data['input_ids']),\n",
    "            attention_mask=torch.LongTensor(tokenized_txt.data['attention_mask']),\n",
    "            past_prompt=past_prompt,\n",
    "        )\n",
    "    encoder_output_hidden_states = encoder_outputs.last_hidden_state.detach().squeeze(0).cpu().numpy()\n",
    "    if debug:\n",
    "        print('encoder_output_hidden_states:', encoder_output_hidden_states.shape)\n",
    "    \n",
    "    # Get node-pieces mapping via char ranges \n",
    "    char_ranges_dict = collect_node_char_ranges(sample, txt=txt, tokenized_txt=tokenized_txt)\n",
    "    node_char_ranges = char_ranges_dict['q_node_chars'] + char_ranges_dict['c_node_chars'] + char_ranges_dict['t_node_chars']\n",
    "\n",
    "    # some chars can be mapped to multiple tokens (e.g. 'i' => '', 'i' )\n",
    "    char_to_tokens_dict = defaultdict(list)\n",
    "    for token_idx, tok in enumerate(tokenized_txt.tokens()):\n",
    "        if tok == '</s>':\n",
    "            break\n",
    "        char_span = tokenized_txt.token_to_chars(token_idx)\n",
    "        for char_idx in range(char_span[0], char_span[1]):\n",
    "            char_to_tokens_dict[char_idx].append(token_idx)\n",
    "    \n",
    "    node_pieces_ranges = []\n",
    "    for st, ed in node_char_ranges:\n",
    "        piece_ids = []\n",
    "        for char_idx in range(st, ed):\n",
    "            _piece_ids = char_to_tokens_dict[char_idx]\n",
    "            piece_ids.extend(_piece_ids)\n",
    "\n",
    "        piece_st = piece_ids[0]\n",
    "        piece_ed = piece_ids[-1] + 1\n",
    "        # the collected piece_ids should be continuous \n",
    "        # ^ not true... some chars can be mapped to multiple tokens (started by  )\n",
    "        # re-collect a char-to-token\n",
    "        assert set(range(piece_st, piece_ed)) == set(piece_ids), piece_ids\n",
    "\n",
    "        node_pieces_ranges.append((piece_st, piece_ed))\n",
    "    \n",
    "    if debug:\n",
    "        print('node_pieces_ranges:', node_pieces_ranges)\n",
    "    \n",
    "    # Pool the encodings per node \n",
    "    node_encodings = []\n",
    "    for piece_st, piece_ed in node_pieces_ranges:\n",
    "        enc_vecs = encoder_output_hidden_states[piece_st : piece_ed]\n",
    "        enc_pooled = pooling_func(enc_vecs)\n",
    "        node_encodings.append(enc_pooled)\n",
    "    \n",
    "    return node_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0ac6d088",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 209\n",
    "sample = orig_dataset[idx]\n",
    "\n",
    "text_in = sample['question']\n",
    "struct_in = uskg_sample_to_struct_input(sample)\n",
    "\n",
    "txt = \"{}; structed knowledge: {}\".format(text_in, struct_in)\n",
    "\n",
    "tokenized_txt = tokenizer_fast([txt], max_length=1024, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "72e4643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_output_hidden_states: (1024, 1024)\n",
      "node_pieces_ranges: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (7, 8), (16, 19), (23, 27), (29, 30), (32, 36), (38, 39), (44, 49), (51, 53), (55, 57), (59, 60), (62, 66), (70, 71), (73, 76), (78, 81), (83, 86), (20, 21), (40, 42), (67, 68)]\n"
     ]
    }
   ],
   "source": [
    "node_encodings = get_USKG_node_encodings(sample, model, tokenizer_fast, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "07b7fa25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_encodings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40ac95d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab2a3d04",
   "metadata": {},
   "source": [
    "## Load data collectors from py scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d842d3",
   "metadata": {},
   "source": [
    "### Data samples load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efd3dd91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8421, dict_keys(['phase', 'table_id', 'question', 'sql', 'rat_sql_graph']))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load dataset\n",
    "\n",
    "with open(dataset_path, 'r') as f:\n",
    "    orig_dataset = json.load(f)\n",
    "    \n",
    "for d in orig_dataset:\n",
    "    d['rat_sql_graph']['relations'] = json.loads(d['rat_sql_graph']['relations'])\n",
    "\n",
    "len(orig_dataset), orig_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4403ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('What position does the player who played for butler cc (ks) play?',\n",
       " '1-10015132-11')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_dataset[0]['question'], orig_dataset[0]['table_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e3afb4",
   "metadata": {},
   "source": [
    "## Probing: link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74cf61e",
   "metadata": {},
   "source": [
    "### Sample test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa473e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build collector\n",
    "\n",
    "PROJ_DIR='/home/yshao/Projects'\n",
    "DATASET='wikisql'\n",
    "\n",
    "args = SimpleNamespace()\n",
    "\n",
    "args.orig_dataset_dir = f'{PROJ_DIR}/language/language/xsp/data/{DATASET}'\n",
    "args.graph_dataset_dir = f'{PROJ_DIR}/SDR-analysis/data/{DATASET}'\n",
    "args.probing_data_in_dir = f'{PROJ_DIR}/SDR-analysis/data/probing/text2sql/link_prediction/{DATASET}/uskg'\n",
    "args.probing_data_out_dir = f'{PROJ_DIR}/SDR-analysis/data/probing/text2sql/link_prediction/{DATASET}/uskg-tmp'\n",
    "args.model = 'hkunlp/from_all_T5_large_prefix_spider_with_cell_value2'\n",
    "args.cfg = 'Salesforce/T5_large_prefix_spider_with_cell_value.cfg'\n",
    "\n",
    "\n",
    "probe_data_collector = LinkPredictionDataCollector_USKG_wikisql(\n",
    "    orig_dataset_dir=args.orig_dataset_dir,\n",
    "    graph_dataset_dir=args.graph_dataset_dir,\n",
    "    probing_data_in_dir=args.probing_data_in_dir,\n",
    "    probing_data_out_dir=args.probing_data_out_dir,\n",
    ")\n",
    "\n",
    "probe_data_collector.orig_ds_list = ['dev']\n",
    "probe_data_collector.prob_ds_list = ['train']\n",
    "# probe_data_collector._start_idx = 490\n",
    "\n",
    "# probe_data_collector.load_model(args)\n",
    "probe_data_collector.model = model\n",
    "probe_data_collector.tokenizer_fast = tokenizer_fast\n",
    "\n",
    "# probe_data_collector.collect_all_probing_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "889f2bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_ds = 'dev'\n",
    "prob_ds = 'train'\n",
    "# dataset_path = f\"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/spider/{orig_ds}+ratsql_graph.json\"\n",
    "dataset_path = f\"/home/yshao/Projects/SDR-analysis/data/wikisql/{orig_ds}+ratsql_graph.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac17365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dev': {'orig_dataset_path': '/home/yshao/Projects/language/language/xsp/data/wikisql/dev.jsonl',\n",
       "  'orig_tables_path': '/home/yshao/Projects/language/language/xsp/data/wikisql/dev.tables.jsonl',\n",
       "  'db_path': '/home/yshao/Projects/language/language/xsp/data/wikisql/dev.db'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_dict = probe_data_collector.get_paths_dict()\n",
    "paths_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3693a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_tables_path=/home/yshao/Projects/language/language/xsp/data/wikisql/dev.tables.jsonl; db_path=/home/yshao/Projects/language/language/xsp/data/wikisql/dev.db: not match the cache, recompute!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2716"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_tables_path = paths_dict[orig_ds]['orig_tables_path']\n",
    "db_path = paths_dict[orig_ds]['db_path']\n",
    "\n",
    "db_schemas_dict = probe_data_collector.get_schemas_dict(orig_tables_path, db_path)\n",
    "len(db_schemas_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe70f5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## start link prediction data collection \n",
    "\n",
    "enc_repr = probe_data_collector.get_node_encodings(orig_dataset[0])\n",
    "type(enc_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e5575fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled output idx: 7\n",
      "i = 14, j = 8\n",
      "Nodes: ?, butler\n",
      "Relation: 0 (-)\n",
      "Repr vectors:\n",
      "[0.06752008 0.02653399 0.13214661] ... [-0.02297289  0.00501938  0.03224115]\n",
      "[-0.14800712  0.01252076 -0.26400036] ... [-0.20672157  0.01301077  0.12664273]\n",
      "Combined vector:\n",
      "[0.06752008 0.02653399 0.13214661] ... [4.748992e-03 6.530606e-05 4.083108e-03]\n",
      "Label:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "graph_dict = orig_dataset[0]['rat_sql_graph']\n",
    "\n",
    "X, y, pos = collect_link_prediction_samples(graph_dict,\n",
    "                                           enc_repr,\n",
    "                                           pos=None,\n",
    "                                           max_rel_occ=1,\n",
    "                                           debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c09e647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['question', 'butler'],\n",
       " ['question', 'cc'],\n",
       " ['question', 'what'],\n",
       " ['question', '?'],\n",
       " ['question', 'the'],\n",
       " ['column', 'year', 'in', 'toronto'],\n",
       " ['column', 'school', 'or', 'club', 'team'],\n",
       " ['column', 'position'],\n",
       " ['column', 'nationality'],\n",
       " ['column', '*'],\n",
       " ['table', 'l']]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffb8dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f07f7cbb",
   "metadata": {},
   "source": [
    "### Data collection (moved to script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8de125f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_probing_samples_link_prediction_uskg(dataset_sample,\n",
    "                                                 db_schemas_dict,\n",
    "                                                 model,\n",
    "                                                 tokenizer,\n",
    "                                                 pos=None,\n",
    "                                                 max_rel_occ=None,\n",
    "                                                 debug=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset_sample (Dict): a sample dict from spider dataset\n",
    "        db_schemas_dict (Dict): db_id => db_schema, precomputed for all DBs (not used here)\n",
    "        model (EncDec): the rat-sql model\n",
    "        pos (List[Tuple]): the position pairs to use. If none, will randomly generate\n",
    "        max_rel_occ (int): each relation occur at most this many times in each (original) sample\n",
    "    \n",
    "    Return:\n",
    "        X (List[np.array]): input features, \"shape\" = (n, dim)\n",
    "        y (List): output labels, \"shape\" = (n,)\n",
    "        pos (List[Tuple]): actual position (node-id) pairs for X and y\n",
    "    \"\"\"\n",
    "    \n",
    "    d = dataset_sample\n",
    "    \n",
    "    db_id = d['db_id']\n",
    "    # db_schema = db_schemas_dict[db_id]\n",
    "    question = d['question']\n",
    "\n",
    "    # get relation matrix (relation_id2name not available as it needs rat-sql model)\n",
    "    graph_dict = dataset_sample['rat_sql_graph']\n",
    "    # graph_dict['relation_id2name'] = {v : k for k, v in model.encoder.encs_update.relation_ids.items()}\n",
    "    \n",
    "    # get encodings\n",
    "    # rat_sql_encoder_state = get_rat_sql_encoder_state(question=question, db_schema=db_schema, model=model)\n",
    "    # enc_repr = rat_sql_encoder_state.memory.squeeze(0).detach().cpu().numpy()\n",
    "    enc_repr = get_USKG_node_encodings(sample=dataset_sample,\n",
    "                                       model=model,\n",
    "                                       tokenizer=tokenizer,\n",
    "                                       debug=debug)\n",
    "    \n",
    "    X, y, pos = collect_link_prediction_samples(\n",
    "        graph_dict,\n",
    "        enc_repr,\n",
    "        pos=pos,\n",
    "        max_rel_occ=max_rel_occ,\n",
    "        debug=debug)\n",
    "    \n",
    "    return X, y, pos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "773e26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "probing_data_dir = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/probing/text2sql/link_prediction/spider/ratsql\"\n",
    "\n",
    "orig_ds = 'dev'\n",
    "prob_ds = 'test'\n",
    "dataset_path = f\"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/spider/{orig_ds}+ratsql_graph.json\"\n",
    "\n",
    "pos_file_path = os.path.join(probing_data_dir, f'{orig_ds}.{prob_ds}.pos.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "966e135b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034,\n",
       " dict_keys(['db_id', 'query', 'query_toks', 'query_toks_no_value', 'question', 'question_toks', 'sql', 'rat_sql_graph']))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(dataset_path, 'r') as f:\n",
    "    orig_dataset = json.load(f)\n",
    "    \n",
    "for d in orig_dataset:\n",
    "    d['rat_sql_graph']['relations'] = json.loads(d['rat_sql_graph']['relations'])\n",
    "\n",
    "len(orig_dataset), orig_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6b41f15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(pos_file_path, 'r') as f:\n",
    "    lines = f.read().strip().split('\\n')\n",
    "    all_pos_triplets = [tuple([int(s) for s in l.split('\\t')]) for l in lines]\n",
    "len(all_pos_triplets), all_pos_triplets[0]\n",
    "\n",
    "# Load pos file \n",
    "sample_ds_indices = []               # [ds_idx], based on occurring order \n",
    "pos_per_sample = defaultdict(list)   # key = ds_idx, value = pos_list: List[(i, j)]\n",
    "\n",
    "for ds_idx, i, j in all_pos_triplets:\n",
    "    if not sample_ds_indices or sample_ds_indices[-1] != ds_idx:\n",
    "        sample_ds_indices.append(ds_idx)\n",
    "    pos_per_sample[ds_idx].append((i, j))\n",
    "\n",
    "len(sample_ds_indices), len(pos_per_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a2e63e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test loading pos file \n",
    "set(sample_ds_indices) == set(pos_per_sample.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "105efe12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e44c87da274935a2f6704741ea450e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(16059, 16059, 16059)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: for each sample_ds_idx, get the sample & pos_list, get the X, y, pos; save. \n",
    "\n",
    "all_X = []\n",
    "all_y = []\n",
    "all_pos = []\n",
    "\n",
    "for sample_ds_idx in tqdm(sample_ds_indices):\n",
    "    dataset_sample = orig_dataset[sample_ds_idx]\n",
    "    pos_list = pos_per_sample[sample_ds_idx]\n",
    "\n",
    "    X, y, pos = extract_probing_samples_link_prediction_uskg(dataset_sample=dataset_sample,\n",
    "                                                             db_schemas_dict=None,\n",
    "                                                             model=model,\n",
    "                                                             tokenizer=tokenizer_fast,\n",
    "                                                             pos=pos_list,\n",
    "                                                             max_rel_occ=None,  # when given pos, this is not needed \n",
    "                                                             debug=False)\n",
    "    \n",
    "    all_X.extend(X)\n",
    "    all_y.extend(y)\n",
    "    pos = [(sample_ds_idx, i, j) for i, j in pos]   # add sample idx \n",
    "    all_pos.extend(pos)\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "\n",
    "len(all_X), len(all_y), len(all_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d061be59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ds_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "55520a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "probing_data_out_dir = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/probing/text2sql/link_prediction/spider/uskg\"\n",
    "os.makedirs(probing_data_out_dir, exist_ok=True)\n",
    "\n",
    "output_path_test_X = os.path.join(probing_data_out_dir, f'{orig_ds}.{prob_ds}.X.pkl')\n",
    "output_path_test_y = os.path.join(probing_data_out_dir, f'{orig_ds}.{prob_ds}.y.pkl')\n",
    "output_path_test_pos = os.path.join(probing_data_out_dir, f'{orig_ds}.{prob_ds}.pos.txt')\n",
    "\n",
    "with open(output_path_test_X, 'wb') as f:\n",
    "    pickle.dump(all_X, f)\n",
    "with open(output_path_test_y, 'wb') as f:\n",
    "    pickle.dump(all_y, f)\n",
    "with open(output_path_test_pos, 'w') as f:\n",
    "    for idx, i, j in all_pos:\n",
    "        f.write(f'{idx}\\t{i}\\t{j}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0297039",
   "metadata": {},
   "source": [
    "## Probing: single node reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a2df966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sdr_analysis.helpers.single_node_reconstruction_collector\n",
    "importlib.reload(sdr_analysis.helpers.single_node_reconstruction_collector)\n",
    "from sdr_analysis.helpers.single_node_reconstruction_collector import collect_single_node_reconstruction_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558a59f1",
   "metadata": {},
   "source": [
    "### Sample test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "feb6e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build collector \n",
    "\n",
    "PROJ_DIR='/home/yshao/Projects'\n",
    "DATASET='wikisql'\n",
    "\n",
    "args = SimpleNamespace()\n",
    "\n",
    "args.orig_dataset_dir = f'{PROJ_DIR}/language/language/xsp/data/{DATASET}'\n",
    "args.graph_dataset_dir = f'{PROJ_DIR}/SDR-analysis/data/{DATASET}'\n",
    "args.probing_data_in_dir = f'{PROJ_DIR}/SDR-analysis/data/probing/text2sql/single_node_reconstruction/{DATASET}/uskg'\n",
    "args.probing_data_out_dir = f'{PROJ_DIR}/SDR-analysis/data/probing/text2sql/single_node_reconstruction/{DATASET}/uskg-tmp'\n",
    "args.model = 'hkunlp/from_all_T5_large_prefix_spider_with_cell_value2'\n",
    "args.cfg = 'Salesforce/T5_large_prefix_spider_with_cell_value.cfg'\n",
    "\n",
    "\n",
    "SNR_probe_data_collector = SingleNodeReconstructionDataCollector_USKG_wikisql(\n",
    "    orig_dataset_dir=args.orig_dataset_dir,\n",
    "    graph_dataset_dir=args.graph_dataset_dir,\n",
    "    probing_data_in_dir=args.probing_data_in_dir,\n",
    "    probing_data_out_dir=args.probing_data_out_dir,\n",
    ")\n",
    "\n",
    "SNR_probe_data_collector.orig_ds_list = ['dev']\n",
    "SNR_probe_data_collector.prob_ds_list = ['train']\n",
    "# SNR_probe_data_collector._start_idx = 490\n",
    "\n",
    "# SNR_probe_data_collector.load_model(args)\n",
    "SNR_probe_data_collector.model = model\n",
    "SNR_probe_data_collector.tokenizer_fast = tokenizer_fast\n",
    "\n",
    "# SNR_probe_data_collector.collect_all_probing_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7856f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_ds = 'dev'\n",
    "prob_ds = 'train'\n",
    "# dataset_path = f\"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/spider/{orig_ds}+ratsql_graph.json\"\n",
    "dataset_path = f\"/home/yshao/Projects/SDR-analysis/data/wikisql/{orig_ds}+ratsql_graph.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cefe00b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dev': {'orig_dataset_path': '/home/yshao/Projects/language/language/xsp/data/wikisql/dev.jsonl',\n",
       "  'orig_tables_path': '/home/yshao/Projects/language/language/xsp/data/wikisql/dev.tables.jsonl',\n",
       "  'db_path': '/home/yshao/Projects/language/language/xsp/data/wikisql/dev.db'}}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_dict = SNR_probe_data_collector.get_paths_dict()\n",
    "paths_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9037feda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_tables_path=/home/yshao/Projects/language/language/xsp/data/wikisql/dev.tables.jsonl; db_path=/home/yshao/Projects/language/language/xsp/data/wikisql/dev.db: not match the cache, recompute!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2716"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_tables_path = paths_dict[orig_ds]['orig_tables_path']\n",
    "db_path = paths_dict[orig_ds]['db_path']\n",
    "\n",
    "db_schemas_dict = SNR_probe_data_collector.get_schemas_dict(orig_tables_path, db_path)\n",
    "len(db_schemas_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4291ab57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list, numpy.ndarray)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sample = orig_dataset[0]\n",
    "\n",
    "enc_repr = SNR_probe_data_collector.get_node_encodings(_sample, pooling_func=lambda x: x)\n",
    "\n",
    "type(enc_repr), type(enc_repr[0]), type(enc_repr[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e3cfd7d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1024), (1, 1024), (1, 1024), (1, 1024), (1, 1024), (1, 1024), (1, 1024), (1, 1024), (2, 1024), (3, 1024), (1, 1024), (2, 1024), (1, 1024), (1, 1024), (1, 1024), (5, 1024), (1, 1024), (1, 1024), (2, 1024), (1, 1024), (7, 1024), (19, 1024), (2, 1024)]\n"
     ]
    }
   ],
   "source": [
    "print([e.shape for e in enc_repr[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5559ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = _sample['rat_sql_graph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "51a25819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What position does the player who played for butler cc (ks) play?; structed knowledge:  | 1-10015132-11 | l : player , no , nationality , position , years_in_toronto , school_or_club_team ( butler cc (ks) )'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_sample['txt_pieces']['txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad426b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample idx: 3\n",
      "node_id = 3\n",
      "Node: ['question', 'the'] (the)\n",
      "Repr vectors:\n",
      "[-0.10546452 -0.16901198 -0.06285214] ... [-0.17854233 -0.3109485   0.19226295]\n"
     ]
    }
   ],
   "source": [
    "X, y, pos = collect_single_node_reconstruction_samples(graph_dict,\n",
    "                                                       enc_repr,\n",
    "                                                       pos=None,\n",
    "                                                       max_node_type_occ=None,\n",
    "                                                       debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f24b0005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, ['question', 'what']),\n",
       " (1, ['question', 'position']),\n",
       " (2, ['question', 'do']),\n",
       " (3, ['question', 'the']),\n",
       " (4, ['question', 'player']),\n",
       " (5, ['question', 'who']),\n",
       " (6, ['question', 'play']),\n",
       " (7, ['question', 'for']),\n",
       " (8, ['question', 'butler']),\n",
       " (9, ['question', 'cc']),\n",
       " (10, ['question', '-lrb-']),\n",
       " (11, ['question', 'k']),\n",
       " (12, ['question', '-rrb-']),\n",
       " (13, ['question', 'play']),\n",
       " (14, ['question', '?']),\n",
       " (16, ['column', 'player']),\n",
       " (17, ['column', 'no']),\n",
       " (18, ['column', 'nationality']),\n",
       " (19, ['column', 'position']),\n",
       " (20, ['column', 'year', 'in', 'toronto']),\n",
       " (21, ['column', 'school', 'or', 'club', 'team']),\n",
       " (22, ['table', 'l'])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(pos, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0067243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.16765554, 0.19590843, 0.1182106 ]], dtype=float32),\n",
       " array([[0.18989907, 0.15153025, 0.3066842 ]], dtype=float32),\n",
       " array([[ 0.18852772, -0.14816955,  0.06566472]], dtype=float32),\n",
       " array([[-0.10546452, -0.16901198, -0.06285214]], dtype=float32),\n",
       " array([[ 0.01387155, -0.1469807 , -0.04858986]], dtype=float32),\n",
       " array([[-0.09723643, -0.19441439, -0.3072489 ]], dtype=float32),\n",
       " array([[-0.0749758 , -0.15097   , -0.29927436]], dtype=float32),\n",
       " array([[-0.02502756,  0.18236493, -0.02920943]], dtype=float32),\n",
       " array([[-0.15982388, -0.1592177 , -0.22788137],\n",
       "        [-0.13619037,  0.18425922, -0.30011937]], dtype=float32),\n",
       " array([[ 0.06221096, -0.00930876, -0.18367842],\n",
       "        [ 0.15632832,  0.10616314, -0.2654981 ],\n",
       "        [-0.1114219 , -0.06422008, -0.43678653]], dtype=float32),\n",
       " array([[ 0.0325256 , -0.08600596, -0.11446463]], dtype=float32),\n",
       " array([[-0.09835997, -0.18946472,  0.01884983],\n",
       "        [-0.02021622, -0.3451298 , -0.0703741 ]], dtype=float32),\n",
       " array([[ 0.11468986, -0.11053255, -0.04476618]], dtype=float32),\n",
       " array([[ 0.29802877, -0.10237666,  0.1824489 ]], dtype=float32),\n",
       " array([[0.06752008, 0.02653399, 0.13214661]], dtype=float32),\n",
       " array([[-0.04055703, -0.0835851 , -0.00168023]], dtype=float32),\n",
       " array([[0.3059911 , 0.20925671, 0.15982005]], dtype=float32),\n",
       " array([[ 0.24862656,  0.4053381 ,  0.3166551 ],\n",
       "        [ 0.25019133,  0.02275216, -0.10505518]], dtype=float32),\n",
       " array([[0.4004848 , 0.15975988, 0.10018693]], dtype=float32),\n",
       " array([[ 0.2062138 ,  0.08433893, -0.13015473],\n",
       "        [ 0.37052512, -0.11813328, -0.09284374],\n",
       "        [ 0.29820347, -0.07872313, -0.2177051 ],\n",
       "        [ 0.27749425, -0.11601175,  0.07801633],\n",
       "        [ 0.25900024,  0.12330141,  0.06825793],\n",
       "        [ 0.02506857,  0.0372823 ,  0.04234718],\n",
       "        [ 0.2534797 ,  0.12620139, -0.02146743]], dtype=float32),\n",
       " array([[-1.4174293e-01, -7.1932979e-02, -4.3704253e-02],\n",
       "        [ 2.8280253e-02, -3.6632469e-01, -2.5665587e-01],\n",
       "        [-7.1764761e-03,  5.0640676e-02, -2.6804310e-01],\n",
       "        [ 4.8768114e-02, -1.1655305e-01,  9.7540192e-02],\n",
       "        [-7.3743492e-02, -9.0397619e-02, -2.1101760e-02],\n",
       "        [ 1.6820539e-02, -2.5278094e-01,  7.1575753e-03],\n",
       "        [-1.3399197e-01, -3.9372092e-01, -7.2159395e-02],\n",
       "        [-1.7581167e-02, -1.3693738e-01,  8.7725391e-05],\n",
       "        [-2.2770342e-01, -2.2525111e-01, -1.6575974e-01],\n",
       "        [-1.9393463e-01,  1.6264266e-01, -1.5238023e-01],\n",
       "        [ 1.3700745e-01, -2.4003321e-01, -1.9282317e-01],\n",
       "        [ 2.2082526e-02,  2.1833552e-01, -3.1921807e-01],\n",
       "        [-2.1216500e-01, -1.7282322e-01, -5.8224505e-01],\n",
       "        [-4.1098572e-02, -8.8617645e-02,  1.2470823e-02],\n",
       "        [-1.4283587e-01, -2.5003430e-01,  7.1410455e-02],\n",
       "        [ 5.0448708e-02, -3.0718869e-01,  8.1860879e-03],\n",
       "        [ 2.1949232e-01,  1.5681326e-02, -7.0343696e-02],\n",
       "        [ 2.1492505e-02,  3.9428659e-02, -2.7074227e-01],\n",
       "        [-5.8693871e-02, -8.7499432e-02, -7.9797223e-02]], dtype=float32),\n",
       " array([[-0.17731698,  0.03398876, -0.33114293],\n",
       "        [-0.14057073,  0.10517828, -0.30610174]], dtype=float32)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[:, :3] for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "76315826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test collector pipeline (calling extract_probing_samples_single_node_reconstruction() )\n",
    "\n",
    "X, y, pos = SNR_probe_data_collector.extract_probing_samples_single_node_reconstruction([_sample], pos_list=[pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6f856589",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([((0, 0), ['question', 'what']),\n",
       "  ((0, 1), ['question', 'position']),\n",
       "  ((0, 2), ['question', 'do']),\n",
       "  ((0, 3), ['question', 'the']),\n",
       "  ((0, 4), ['question', 'player']),\n",
       "  ((0, 5), ['question', 'who']),\n",
       "  ((0, 6), ['question', 'play']),\n",
       "  ((0, 7), ['question', 'for']),\n",
       "  ((0, 8), ['question', 'butler']),\n",
       "  ((0, 9), ['question', 'cc']),\n",
       "  ((0, 10), ['question', '-lrb-']),\n",
       "  ((0, 11), ['question', 'k']),\n",
       "  ((0, 12), ['question', '-rrb-']),\n",
       "  ((0, 13), ['question', 'play']),\n",
       "  ((0, 14), ['question', '?']),\n",
       "  ((0, 16), ['column', 'player']),\n",
       "  ((0, 17), ['column', 'no']),\n",
       "  ((0, 18), ['column', 'nationality']),\n",
       "  ((0, 19), ['column', 'position']),\n",
       "  ((0, 20), ['column', 'year', 'in', 'toronto']),\n",
       "  ((0, 21), ['column', 'school', 'or', 'club', 'team']),\n",
       "  ((0, 22), ['table', 'l'])],\n",
       " [array([[0.16765554, 0.19590843, 0.1182106 ]], dtype=float32),\n",
       "  array([[0.18989907, 0.15153025, 0.3066842 ]], dtype=float32),\n",
       "  array([[ 0.18852772, -0.14816955,  0.06566472]], dtype=float32),\n",
       "  array([[-0.10546452, -0.16901198, -0.06285214]], dtype=float32),\n",
       "  array([[ 0.01387155, -0.1469807 , -0.04858986]], dtype=float32),\n",
       "  array([[-0.09723643, -0.19441439, -0.3072489 ]], dtype=float32),\n",
       "  array([[-0.0749758 , -0.15097   , -0.29927436]], dtype=float32),\n",
       "  array([[-0.02502756,  0.18236493, -0.02920943]], dtype=float32),\n",
       "  array([[-0.15982388, -0.1592177 , -0.22788137],\n",
       "         [-0.13619037,  0.18425922, -0.30011937]], dtype=float32),\n",
       "  array([[ 0.06221096, -0.00930876, -0.18367842],\n",
       "         [ 0.15632832,  0.10616314, -0.2654981 ],\n",
       "         [-0.1114219 , -0.06422008, -0.43678653]], dtype=float32),\n",
       "  array([[ 0.0325256 , -0.08600596, -0.11446463]], dtype=float32),\n",
       "  array([[-0.09835997, -0.18946472,  0.01884983],\n",
       "         [-0.02021622, -0.3451298 , -0.0703741 ]], dtype=float32),\n",
       "  array([[ 0.11468986, -0.11053255, -0.04476618]], dtype=float32),\n",
       "  array([[ 0.29802877, -0.10237666,  0.1824489 ]], dtype=float32),\n",
       "  array([[0.06752008, 0.02653399, 0.13214661]], dtype=float32),\n",
       "  array([[-0.04055703, -0.0835851 , -0.00168023]], dtype=float32),\n",
       "  array([[0.3059911 , 0.20925671, 0.15982005]], dtype=float32),\n",
       "  array([[ 0.24862656,  0.4053381 ,  0.3166551 ],\n",
       "         [ 0.25019133,  0.02275216, -0.10505518]], dtype=float32),\n",
       "  array([[0.4004848 , 0.15975988, 0.10018693]], dtype=float32),\n",
       "  array([[ 0.2062138 ,  0.08433893, -0.13015473],\n",
       "         [ 0.37052512, -0.11813328, -0.09284374],\n",
       "         [ 0.29820347, -0.07872313, -0.2177051 ],\n",
       "         [ 0.27749425, -0.11601175,  0.07801633],\n",
       "         [ 0.25900024,  0.12330141,  0.06825793],\n",
       "         [ 0.02506857,  0.0372823 ,  0.04234718],\n",
       "         [ 0.2534797 ,  0.12620139, -0.02146743]], dtype=float32),\n",
       "  array([[-1.4174293e-01, -7.1932979e-02, -4.3704253e-02],\n",
       "         [ 2.8280253e-02, -3.6632469e-01, -2.5665587e-01],\n",
       "         [-7.1764761e-03,  5.0640676e-02, -2.6804310e-01],\n",
       "         [ 4.8768114e-02, -1.1655305e-01,  9.7540192e-02],\n",
       "         [-7.3743492e-02, -9.0397619e-02, -2.1101760e-02],\n",
       "         [ 1.6820539e-02, -2.5278094e-01,  7.1575753e-03],\n",
       "         [-1.3399197e-01, -3.9372092e-01, -7.2159395e-02],\n",
       "         [-1.7581167e-02, -1.3693738e-01,  8.7725391e-05],\n",
       "         [-2.2770342e-01, -2.2525111e-01, -1.6575974e-01],\n",
       "         [-1.9393463e-01,  1.6264266e-01, -1.5238023e-01],\n",
       "         [ 1.3700745e-01, -2.4003321e-01, -1.9282317e-01],\n",
       "         [ 2.2082526e-02,  2.1833552e-01, -3.1921807e-01],\n",
       "         [-2.1216500e-01, -1.7282322e-01, -5.8224505e-01],\n",
       "         [-4.1098572e-02, -8.8617645e-02,  1.2470823e-02],\n",
       "         [-1.4283587e-01, -2.5003430e-01,  7.1410455e-02],\n",
       "         [ 5.0448708e-02, -3.0718869e-01,  8.1860879e-03],\n",
       "         [ 2.1949232e-01,  1.5681326e-02, -7.0343696e-02],\n",
       "         [ 2.1492505e-02,  3.9428659e-02, -2.7074227e-01],\n",
       "         [-5.8693871e-02, -8.7499432e-02, -7.9797223e-02]], dtype=float32),\n",
       "  array([[-0.17731698,  0.03398876, -0.33114293],\n",
       "         [-0.14057073,  0.10517828, -0.30610174]], dtype=float32)])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(pos, y)), [x[:, :3] for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff7021b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e88ea82e",
   "metadata": {},
   "source": [
    "## KBQA - WebQSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6929c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_construction import webqsp_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "8c22e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def webqsp_get_sample(ex):\n",
    "    _ents = [[elm['text'], elm['kb_id']] for elm in ex[\"entities\"]]\n",
    "    _answers = [[elm['text'], elm['kb_id']] for elm in ex[\"answers\"]]\n",
    "    return {\n",
    "        # \"id\": ex[\"ID\"],\n",
    "        \"id\": ex[\"id\"],\n",
    "        \"question\": ex[\"question\"],\n",
    "        # \"answers\": ex[\"answers\"],\n",
    "        \"answers\": _answers,\n",
    "        # \"s_expression\": ex[\"s_expression\"],\n",
    "        # \"kg_tuples\": ex[\"kg_tuples\"],\n",
    "        \"subgraph_tuples\": ex[\"subgraph\"][\"tuples\"],\n",
    "        # \"entities\": ex[\"entities\"],\n",
    "        \"entities\": _ents,\n",
    "        # \"paths\": ex[\"paths\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2921a7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "webqsp_path = '/home/yshao/Projects/SDR-analysis/data/webqsp/train_simple.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "c1cd3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "webqsp_path = '/home/yshao/Projects/SDR-analysis/data/webqsp/train_simple_short.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "2ccf5bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2848"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(webqsp_path, 'r') as f:\n",
    "    webqsp_dataset_orig = [json.loads(l) for l in f]\n",
    "    webqsp_dataset = [webqsp_get_sample(ex) for ex in webqsp_dataset_orig]\n",
    "len(webqsp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "d47f044a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'WebQTrn-837',\n",
       " 'question': 'where did jennifer hudson go to school',\n",
       " 'answers': [['Langston University', 'j'],\n",
       "  ['Dunbar Vocational High School', 'i']],\n",
       " 'subgraph_tuples': [['a', 'music.artist.origin', 'd'],\n",
       "  ['a', 'people.person.places_lived', 'g'],\n",
       "  ['c', 'people.person.education', 'b'],\n",
       "  ['a', 'people.person.education', 'e'],\n",
       "  ['e', 'education.education.institution', 'j'],\n",
       "  ['g', 'people.place_lived.location', 'h'],\n",
       "  ['e', 'education.education.student', 'a'],\n",
       "  ['d', 'authority.stanford.place', 'Chicago'],\n",
       "  ['f', 'education.education.student', 'a'],\n",
       "  ['d', 'location.hud_county_place.place', 'd'],\n",
       "  ['a', 'people.person.education', 'f'],\n",
       "  ['f', 'type.object.type', 'education.education'],\n",
       "  ['e', 'type.object.type', 'education.education'],\n",
       "  ['d', 'authority.hud.countyplace', '171296'],\n",
       "  ['f', 'education.education.institution', 'i'],\n",
       "  ['a', 'influence.influence_node.influenced_by', 'c']],\n",
       " 'entities': [['Jennifer Hudson', 'a']]}"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = webqsp_dataset[624]\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24373e5",
   "metadata": {},
   "source": [
    "### entities in subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "08e4a7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 1238\n",
      "228 1012\n",
      "504 1002\n",
      "755 1004\n",
      "821 1059\n",
      "879 1034\n",
      "913 1012\n",
      "2044 1012\n",
      "2205 1005\n",
      "2222 1058\n"
     ]
    }
   ],
   "source": [
    "subg_entities_counter = Counter()\n",
    "\n",
    "for i, d in enumerate(webqsp_dataset_orig):\n",
    "    ent_ids = [e for e in d['subgraph']['entities'] if e.startswith('m.')]\n",
    "    n_ents = len(ent_ids)\n",
    "    if n_ents > 1000:\n",
    "        print(i, n_ents)\n",
    "    subg_entities_counter[n_ents] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "501ef4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1614, 2817, 2848)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_26 = 0\n",
    "cnt_676 = 0\n",
    "\n",
    "for l, cnt in subg_entities_counter.items():\n",
    "    if l < 26:\n",
    "        cnt_26 += cnt\n",
    "        cnt_676 += cnt\n",
    "    elif l < 676:\n",
    "        cnt_676 += cnt\n",
    "\n",
    "cnt_26, cnt_676, len(webqsp_dataset_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12b5fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1c1c6f6",
   "metadata": {},
   "source": [
    "### linearized input length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "0ae293e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fast = AutoTokenizer.from_pretrained('t5-large', use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "ac982901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('where did jennifer hudson go to school',\n",
       " 'Jennifer Hudson: a | a music.artist.origin d | a people.person.places_lived g | c people.person.education b | a people.person.education e | e education.education.institution j | g people.place_lived.location h | e education.education.student a | d authority.stanford.place Chicago | f education.education.student a | d location.hud_county_place.place d | a people.person.education f | f type.object.type education.education | e type.object.type education.education | d authority.hud.countyplace 171296 | f education.education.institution i | a influence.influence_node.influenced_by c')"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, struct_in = webqsp_sr.kgqa_get_input(d['question'], d['subgraph_tuples'], d['entities'])\n",
    "q, struct_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "6552ee75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'where did jennifer hudson go to school; structed knowledge: Jennifer Hudson: a | a music.artist.origin d | a people.person.places_lived g | c people.person.education b | a people.person.education e | e education.education.institution j | g people.place_lived.location h | e education.education.student a | d authority.stanford.place Chicago | f education.education.student a | d location.hud_county_place.place d | a people.person.education f | f type.object.type education.education | e type.object.type education.education | d authority.hud.countyplace 171296 | f education.education.institution i | a influence.influence_node.influenced_by c'"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_input = \"{}; structed knowledge: {}\".format(q, struct_in)\n",
    "lin_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "dad43b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_fast.tokenize(lin_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "68936d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where',\n",
       " 'did',\n",
       " 'je',\n",
       " 'n',\n",
       " 'n',\n",
       " 'if',\n",
       " 'er',\n",
       " '',\n",
       " 'h',\n",
       " 'u',\n",
       " 'd',\n",
       " 'son',\n",
       " 'go',\n",
       " 'to',\n",
       " 'school',\n",
       " ';',\n",
       " '',\n",
       " 'struct',\n",
       " 'e',\n",
       " 'd',\n",
       " 'knowledge',\n",
       " ':',\n",
       " 'Jennifer',\n",
       " 'Hudson',\n",
       " ':',\n",
       " '',\n",
       " 'a',\n",
       " '|',\n",
       " '',\n",
       " 'a',\n",
       " 'music',\n",
       " '.',\n",
       " 'art',\n",
       " 'ist',\n",
       " '.',\n",
       " 'o',\n",
       " 'rig',\n",
       " 'in',\n",
       " '',\n",
       " 'd',\n",
       " '|',\n",
       " '',\n",
       " 'a',\n",
       " 'people',\n",
       " '.',\n",
       " 'person',\n",
       " '.',\n",
       " 'place',\n",
       " 's',\n",
       " '_',\n",
       " 'live',\n",
       " 'd',\n",
       " '',\n",
       " 'g',\n",
       " '|',\n",
       " '',\n",
       " 'c',\n",
       " 'people',\n",
       " '.',\n",
       " 'person',\n",
       " '.',\n",
       " 'education',\n",
       " '',\n",
       " 'b',\n",
       " '|',\n",
       " '',\n",
       " 'a',\n",
       " 'people',\n",
       " '.',\n",
       " 'person',\n",
       " '.',\n",
       " 'education',\n",
       " '',\n",
       " 'e',\n",
       " '|',\n",
       " '',\n",
       " 'e',\n",
       " 'education',\n",
       " '.',\n",
       " 'education',\n",
       " '.',\n",
       " 'in',\n",
       " 'stitution',\n",
       " '',\n",
       " 'j',\n",
       " '|',\n",
       " '',\n",
       " 'g',\n",
       " 'people',\n",
       " '.',\n",
       " 'place',\n",
       " '_',\n",
       " 'live',\n",
       " 'd',\n",
       " '.',\n",
       " 'location',\n",
       " '',\n",
       " 'h',\n",
       " '|',\n",
       " '',\n",
       " 'e',\n",
       " 'education',\n",
       " '.',\n",
       " 'education',\n",
       " '.',\n",
       " 'stud',\n",
       " 'ent',\n",
       " '',\n",
       " 'a',\n",
       " '|',\n",
       " '',\n",
       " 'd',\n",
       " 'authority',\n",
       " '.',\n",
       " 'stan',\n",
       " 'ford',\n",
       " '.',\n",
       " 'place',\n",
       " 'Chicago',\n",
       " '|',\n",
       " '',\n",
       " 'f',\n",
       " 'education',\n",
       " '.',\n",
       " 'education',\n",
       " '.',\n",
       " 'stud',\n",
       " 'ent',\n",
       " '',\n",
       " 'a',\n",
       " '|',\n",
       " '',\n",
       " 'd',\n",
       " 'location',\n",
       " '.',\n",
       " 'h',\n",
       " 'u',\n",
       " 'd',\n",
       " '_',\n",
       " 'count',\n",
       " 'y',\n",
       " '_',\n",
       " 'place',\n",
       " '.',\n",
       " 'place',\n",
       " '',\n",
       " 'd',\n",
       " '|',\n",
       " '',\n",
       " 'a',\n",
       " 'people',\n",
       " '.',\n",
       " 'person',\n",
       " '.',\n",
       " 'education',\n",
       " '',\n",
       " 'f',\n",
       " '|',\n",
       " '',\n",
       " 'f',\n",
       " 'type',\n",
       " '.',\n",
       " 'object',\n",
       " '.',\n",
       " 'type',\n",
       " 'education',\n",
       " '.',\n",
       " 'education',\n",
       " '|',\n",
       " '',\n",
       " 'e',\n",
       " 'type',\n",
       " '.',\n",
       " 'object',\n",
       " '.',\n",
       " 'type',\n",
       " 'education',\n",
       " '.',\n",
       " 'education',\n",
       " '|',\n",
       " '',\n",
       " 'd',\n",
       " 'authority',\n",
       " '.',\n",
       " 'h',\n",
       " 'u',\n",
       " 'd',\n",
       " '.',\n",
       " 'count',\n",
       " 'y',\n",
       " 'place',\n",
       " '17',\n",
       " '12',\n",
       " '96',\n",
       " '|',\n",
       " '',\n",
       " 'f',\n",
       " 'education',\n",
       " '.',\n",
       " 'education',\n",
       " '.',\n",
       " 'in',\n",
       " 'stitution',\n",
       " '',\n",
       " 'i',\n",
       " '|',\n",
       " '',\n",
       " 'a',\n",
       " 'influence',\n",
       " '.',\n",
       " 'influence',\n",
       " '_',\n",
       " 'n',\n",
       " 'o',\n",
       " 'de',\n",
       " '.',\n",
       " 'influenced',\n",
       " '_',\n",
       " 'by',\n",
       " '',\n",
       " 'c']"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_fast.tokenize(lin_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fddd8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_txt = tokenizer_fast(lin_input, truncation=True, padding=\"max_length\", max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdfffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_txt.data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b772c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_len_counter = Counter()\n",
    "\n",
    "for d in webqsp_dataset:\n",
    "    q, struct_in = webqsp_sr.kgqa_get_input(d['question'], d['subgraph_tuples'], d['entities'])\n",
    "    lin_input = \"{}; structed knowledge: {}\".format(q, struct_in)\n",
    "    t5_len = len(tokenizer_fast.tokenize(lin_input))\n",
    "    t5_len_counter[t5_len] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "dcbc0a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(766, 1555, 2848)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original SR\n",
    "cnt_512 = 0\n",
    "cnt_1024 = 0\n",
    "\n",
    "for l, cnt in t5_len_counter.items():\n",
    "    if l < 512:\n",
    "        cnt_512 += cnt\n",
    "        cnt_1024 += cnt\n",
    "    elif l < 1024:\n",
    "        cnt_1024 += cnt\n",
    "\n",
    "cnt_512, cnt_1024, len(webqsp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "aa02f7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1346, 1985, 2848)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shorter\n",
    "cnt_512 = 0\n",
    "cnt_1024 = 0\n",
    "\n",
    "for l, cnt in t5_len_counter.items():\n",
    "    if l < 512:\n",
    "        cnt_512 += cnt\n",
    "        cnt_1024 += cnt\n",
    "    elif l < 1024:\n",
    "        cnt_1024 += cnt\n",
    "\n",
    "cnt_512, cnt_1024, len(webqsp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e025d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36891ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33476d23",
   "metadata": {},
   "source": [
    "## Temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f29f103",
   "metadata": {},
   "source": [
    "### Test tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "02386405",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_fast = AutoTokenizer.from_pretrained('t5-base', use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aae841",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt = \"This is t5's tokenization.; structed knowledge: | model | plm(t5), rnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9a983b8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " '',\n",
       " 't',\n",
       " '5',\n",
       " \"'\",\n",
       " 's',\n",
       " 'token',\n",
       " 'ization',\n",
       " '.',\n",
       " ';',\n",
       " '',\n",
       " 'struct',\n",
       " 'e',\n",
       " 'd',\n",
       " 'knowledge',\n",
       " ':',\n",
       " '|',\n",
       " 'model',\n",
       " '|',\n",
       " 'pl',\n",
       " 'm',\n",
       " '(',\n",
       " 't',\n",
       " '5)',\n",
       " ',',\n",
       " '',\n",
       " 'r',\n",
       " 'n',\n",
       " 'n']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_fast = tokenizer_fast.tokenize(test_txt)\n",
    "tok_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "feada4d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tokenizer.encode(test_txt)\n",
    "encoded_fast = tokenizer_fast.encode(test_txt)\n",
    "type(encoded), type(encoded_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "89c3c0f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100, 100),\n",
       " (19, 19),\n",
       " (3, 3),\n",
       " (17, 17),\n",
       " (755, 755),\n",
       " (31, 31),\n",
       " (7, 7),\n",
       " (14145, 14145),\n",
       " (1707, 1707),\n",
       " (5, 5),\n",
       " (117, 117),\n",
       " (3, 3),\n",
       " (7593, 7593),\n",
       " (15, 15),\n",
       " (26, 26),\n",
       " (1103, 1103),\n",
       " (10, 10),\n",
       " (1820, 1820),\n",
       " (825, 825),\n",
       " (1820, 1820),\n",
       " (4752, 4752),\n",
       " (51, 51),\n",
       " (599, 599),\n",
       " (17, 17),\n",
       " (9120, 9120),\n",
       " (6, 6),\n",
       " (3, 3),\n",
       " (52, 52),\n",
       " (29, 29),\n",
       " (29, 29),\n",
       " (1, 1)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(encoded, encoded_fast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "abe10300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.tokenization_utils_base.BatchEncoding,\n",
       " {'input_ids': [100, 19, 3, 17, 755, 31, 7, 14145, 1707, 5, 117, 3, 7593, 15, 26, 1103, 10, 1820, 825, 1820, 4752, 51, 599, 17, 9120, 6, 3, 52, 29, 29, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_fast = tokenizer_fast(test_txt)\n",
    "type(ed_fast), ed_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3c940bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ed_fast.data['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "36ce4fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " None]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_fast.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a8589a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ed_fast.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a39afa95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "71ba8cf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This          This\n",
      "is            is\n",
      "              t5's\n",
      "t              t5's\n",
      "5              t5's\n",
      "'              t5's\n",
      "s              t5's\n",
      "token         tokenization.;\n",
      "ization        tokenization.;\n",
      ".              tokenization.;\n",
      ";              tokenization.;\n",
      "              structed\n",
      "struct         structed\n",
      "e              structed\n",
      "d              structed\n",
      "knowledge     knowledge:\n",
      ":              knowledge:\n",
      "|             |\n",
      "model         model\n",
      "|             |\n",
      "pl            plm(t5),\n",
      "m              plm(t5),\n",
      "(              plm(t5),\n",
      "t              plm(t5),\n",
      "5)             plm(t5),\n",
      ",              plm(t5),\n",
      "              rnn\n",
      "r              rnn\n",
      "n              rnn\n",
      "n              rnn\n"
     ]
    }
   ],
   "source": [
    "for sw_id, w_id in enumerate(ed_fast.word_ids()[:-1]):\n",
    "    # the last is eos\n",
    "    sw = tok_fast[sw_id]\n",
    "    w = test_txt.split(' ')[w_id]\n",
    "    print(f'{sw:<15s}{w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "efe0403a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenSpan(start=7, end=11)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed_fast.word_to_tokens(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "56a056bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(transformers.tokenization_utils_base.BatchEncoding,\n",
       " {'input_ids': [100, 19, 3, 17, 755, 31, 7, 14145, 1707, 5, 117, 3, 7593, 15, 26, 1103, 10, 1820, 825, 1820, 4752, 51, 599, 17, 9120, 6, 3, 52, 29, 29, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed = tokenizer(test_txt)\n",
    "type(ed), ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51387fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797d1444",
   "metadata": {},
   "source": [
    "#### Different tokenizers consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "ca20988d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1034"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_path = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/spider/dev+ratsql_graph.json\"\n",
    "\n",
    "with open(ds_path, 'r') as f:\n",
    "    dataset2 = json.load(f)\n",
    "\n",
    "for d in dataset2:\n",
    "    d['rat_sql_graph']['relations'] = json.loads(d['rat_sql_graph']['relations'])\n",
    "\n",
    "len(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "42032456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e565419bb952498bb1b68cec36b4c40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, d in enumerate(tqdm(dataset2)):\n",
    "    text_in = d['question']\n",
    "    struct_in = uskg_sample_to_struct_input(d)\n",
    "    txt = \"{}; structed knowledge: {}\".format(text_in, struct_in)\n",
    "    \n",
    "    if \"<\" not in txt:\n",
    "        continue\n",
    "    \n",
    "    uskg_tokenized_txt = tokenizer(txt, max_length=1024, padding=\"max_length\", truncation=True)\n",
    "    base_tokenized_txt = tokenizer_fast(txt, max_length=1024, padding=\"max_length\", truncation=True)\n",
    "    large_tokenized_txt = tokenizer_large_fast(txt, max_length=1024, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    assert uskg_tokenized_txt.data == base_tokenized_txt.data == large_tokenized_txt.data, (i, txt)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6097e475",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_uskg = tokenizer.get_vocab()\n",
    "vocab_base = tokenizer_fast.get_vocab()\n",
    "vocab_large = tokenizer_large_fast.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "a94772cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " <= 32101 None None\n",
      " < 32100 None None\n"
     ]
    }
   ],
   "source": [
    "for k in set(vocab_uskg.keys()) | set(vocab_base.keys()) | set(vocab_large.keys()):\n",
    "    v = vocab_uskg.get(k, None)\n",
    "    v1 = vocab_base.get(k, None)\n",
    "    v2 = vocab_large.get(k, None)\n",
    "    if not v == v1 == v2:\n",
    "        print(k, v, v1, v2)\n",
    "#     assert v == v2, (k, v, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "3f4661ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32100"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_uskg[' <']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "561ec935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de01e42229ba4d78b7438b2cf9910a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, d in enumerate(tqdm(dataset2)):\n",
    "    text_in = d['question']\n",
    "    struct_in = uskg_sample_to_struct_input(d)\n",
    "    txt = \"{}; structed knowledge: {}\".format(text_in, struct_in)\n",
    "    \n",
    "    if \"<\" in txt:\n",
    "        print(i, txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60b75cd",
   "metadata": {},
   "source": [
    "#### max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "03157128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the Asian countries which have a population larger than that of any country in Africa?; structed knowledge:  | real_estate_properties | ref_feature_types : feature_type_code , feature_type_name | ref_property_types : property_type_code ( Apartment , House ) , property_type_description | other_available_features : feature_id , feature_type_code , feature_name , feature_description | properties : property_id , property_type_code ( Apartment , House ) , date_on_market , date_sold , property_name , property_address , room_count , vendor_requested_price , buyer_offered_price , agreed_selling_price , apt_feature_1 , apt_feature_2 , apt_feature_3 , fld_feature_1 , fld_feature_2 , fld_feature_3 , hse_feature_1 , hse_feature_2 , hse_feature_3 , oth_feature_1 , oth_feature_2 , oth_feature_3 , shp_feature_1 , shp_feature_2 , shp_feature_3 , other_property_details | other_property_features : property_id , feature_id , property_feature_description'"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 777\n",
    "text_in = dataset2[idx]['question']\n",
    "struct_in = uskg_sample_to_struct_input(d)\n",
    "txt = \"{}; structed knowledge: {}\".format(text_in, struct_in)\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "efc19d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_tkn_txt = tokenizer_fast(txt, max_length=32, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "d46a8daf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What\n",
      "are\n",
      "the\n",
      "Asian\n",
      "countries\n",
      "which\n",
      "have\n",
      "a\n",
      "a\n",
      "population\n",
      "larger\n",
      "than\n",
      "that\n",
      "of\n",
      "any\n",
      "country\n",
      "in\n",
      "Africa\n",
      "?\n",
      ";\n",
      "s\n",
      "struct\n",
      "e\n",
      "d\n",
      "knowledge\n",
      ":\n",
      "|\n",
      "real\n",
      "_\n",
      "e\n",
      "state\n"
     ]
    }
   ],
   "source": [
    "for i in range(31):\n",
    "    st, ed = _tkn_txt.token_to_chars(i)\n",
    "    print(txt[st:ed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "08023094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'What'),\n",
       " (1, 'are'),\n",
       " (2, 'the'),\n",
       " (3, 'Asian'),\n",
       " (4, 'countries'),\n",
       " (5, 'which'),\n",
       " (6, 'have'),\n",
       " (7, 'a'),\n",
       " (8, 'population'),\n",
       " (9, 'larger'),\n",
       " (10, 'than'),\n",
       " (11, 'that'),\n",
       " (12, 'of'),\n",
       " (13, 'any'),\n",
       " (14, 'country'),\n",
       " (15, 'in'),\n",
       " (16, 'Africa?;'),\n",
       " (17, 'structed'),\n",
       " (18, 'knowledge:'),\n",
       " (19, '|'),\n",
       " (20, 'real_estate_properties'),\n",
       " (21, '|'),\n",
       " (22, 'ref_feature_types'),\n",
       " (23, ':'),\n",
       " (24, 'feature_type_code'),\n",
       " (25, ','),\n",
       " (26, 'feature_type_name'),\n",
       " (27, '|'),\n",
       " (28, 'ref_property_types'),\n",
       " (29, ':'),\n",
       " (30, 'property_type_code'),\n",
       " (31, '('),\n",
       " (32, 'Apartment'),\n",
       " (33, ','),\n",
       " (34, 'House'),\n",
       " (35, ')'),\n",
       " (36, ','),\n",
       " (37, 'property_type_description'),\n",
       " (38, '|'),\n",
       " (39, 'other_available_features'),\n",
       " (40, ':'),\n",
       " (41, 'feature_id'),\n",
       " (42, ','),\n",
       " (43, 'feature_type_code'),\n",
       " (44, ','),\n",
       " (45, 'feature_name'),\n",
       " (46, ','),\n",
       " (47, 'feature_description'),\n",
       " (48, '|'),\n",
       " (49, 'properties'),\n",
       " (50, ':'),\n",
       " (51, 'property_id'),\n",
       " (52, ','),\n",
       " (53, 'property_type_code'),\n",
       " (54, '('),\n",
       " (55, 'Apartment'),\n",
       " (56, ','),\n",
       " (57, 'House'),\n",
       " (58, ')'),\n",
       " (59, ','),\n",
       " (60, 'date_on_market'),\n",
       " (61, ','),\n",
       " (62, 'date_sold'),\n",
       " (63, ','),\n",
       " (64, 'property_name'),\n",
       " (65, ','),\n",
       " (66, 'property_address'),\n",
       " (67, ','),\n",
       " (68, 'room_count'),\n",
       " (69, ','),\n",
       " (70, 'vendor_requested_price'),\n",
       " (71, ','),\n",
       " (72, 'buyer_offered_price'),\n",
       " (73, ','),\n",
       " (74, 'agreed_selling_price'),\n",
       " (75, ','),\n",
       " (76, 'apt_feature_1'),\n",
       " (77, ','),\n",
       " (78, 'apt_feature_2'),\n",
       " (79, ','),\n",
       " (80, 'apt_feature_3'),\n",
       " (81, ','),\n",
       " (82, 'fld_feature_1'),\n",
       " (83, ','),\n",
       " (84, 'fld_feature_2'),\n",
       " (85, ','),\n",
       " (86, 'fld_feature_3'),\n",
       " (87, ','),\n",
       " (88, 'hse_feature_1'),\n",
       " (89, ','),\n",
       " (90, 'hse_feature_2'),\n",
       " (91, ','),\n",
       " (92, 'hse_feature_3'),\n",
       " (93, ','),\n",
       " (94, 'oth_feature_1'),\n",
       " (95, ','),\n",
       " (96, 'oth_feature_2'),\n",
       " (97, ','),\n",
       " (98, 'oth_feature_3'),\n",
       " (99, ','),\n",
       " (100, 'shp_feature_1'),\n",
       " (101, ','),\n",
       " (102, 'shp_feature_2'),\n",
       " (103, ','),\n",
       " (104, 'shp_feature_3'),\n",
       " (105, ','),\n",
       " (106, 'other_property_details'),\n",
       " (107, '|'),\n",
       " (108, 'other_property_features'),\n",
       " (109, ':'),\n",
       " (110, 'property_id'),\n",
       " (111, ','),\n",
       " (112, 'feature_id'),\n",
       " (113, ','),\n",
       " (114, 'property_feature_description')]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(txt.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "4e9f2032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 131, 'real_estate')"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st, ed = _tkn_txt.word_to_chars(20)\n",
    "st, ed, txt[st:ed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "3bb8ed42",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "type object argument after * must be an iterable, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yv/jvxn5bdn5_q0cc9_d83cqdr00000gn/T/ipykernel_14287/568061396.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_tkn_txt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/py3.7pytorch1.8new/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mword_to_chars\u001b[0;34m(self, batch_or_word_index, word_index, sequence_index)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_or_word_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCharSpan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_to_chars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchar_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_or_char_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: type object argument after * must be an iterable, not NoneType"
     ]
    }
   ],
   "source": [
    "_tkn_txt.word_to_chars(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0901381b",
   "metadata": {},
   "source": [
    "#### Unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "114990aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'question '"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_txt = \"question \\u516b\\u5c3e\"\n",
    "_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e56e513c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question', '', '']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tkn_txt = tokenizer_fast.tokenize(_txt)\n",
    "_tkn_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "34a73530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([822, 3, 2], ['question', '', '<unk>'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_tkn_txt_ids = tokenizer_fast.convert_tokens_to_ids(_tkn_txt)\n",
    "_tkn_txt_recon = tokenizer_fast.convert_ids_to_tokens(_tkn_txt_ids)\n",
    "_tkn_txt_ids, _tkn_txt_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "345ae47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32100"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_vocab = tokenizer_fast.get_vocab()\n",
    "len(t5_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d0ee115f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_fast.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ac9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad33457d",
   "metadata": {},
   "source": [
    "### Test get encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "303975a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "models.prompt.modeling_t5.T5Stack"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.pretrain_model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a6d6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = tokenizer.batch_decode(\n",
    "#   model.generate(\n",
    "#     torch.LongTensor(tokenized_txt.data['input_ids']),\n",
    "#     torch.LongTensor(tokenized_txt.data['attention_mask']),\n",
    "#     num_beams=1, \n",
    "#     max_length=256\n",
    "#     ), \n",
    "#   skip_special_tokens=True \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0d050a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_ids = self.pretrain_model.generate(\n",
    "#     input_ids=input_ids,\n",
    "#     attention_mask=attention_mask,\n",
    "#     past_prompt=past_prompt,\n",
    "#     use_cache=True,\n",
    "#     **kwargs,\n",
    "# )\n",
    "\n",
    "# model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "\n",
    "# outputs = self(\n",
    "#     **model_inputs,\n",
    "#     return_dict=True,\n",
    "#     output_attentions=output_attentions,\n",
    "#     output_hidden_states=output_hidden_states,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "681145fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_prompt = model.get_prompt(\n",
    "    bsz=1,              # bsz = input_ids.shape[0]\n",
    "    sample_size=1,      # sample_size=kwargs['num_beams']\n",
    "    description=None,   \n",
    "    knowledge=None,     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28905823",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = model.pretrain_model.encoder(\n",
    "    input_ids=torch.LongTensor(tokenized_txt.data['input_ids']),\n",
    "    attention_mask=torch.LongTensor(tokenized_txt.data['attention_mask']),\n",
    "    past_prompt=past_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6defd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['decoder_prompt', 'cross_attention_prompt', 'encoder_prompt'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_prompt[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "024215f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['last_hidden_state', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aa1089f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 1024])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_outputs.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fbfec86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tokenized_txt.data['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3523fccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 158)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# has zero-padding and EOS, no BOS  \n",
    "len(tokenized_txt.data['input_ids'][0]), np.greater(tokenized_txt.data['input_ids'][0], 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c172a38c",
   "metadata": {},
   "source": [
    "### Tokenized pieces-nodes mapping (old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0272ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_txt.data['input_ids'][0][:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "dff4b0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.tokenize(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "90f41967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What',\n",
       " 'are',\n",
       " 'the',\n",
       " 'Asian',\n",
       " 'countries',\n",
       " 'which',\n",
       " 'have',\n",
       " '',\n",
       " 'a',\n",
       " 'population',\n",
       " 'larger',\n",
       " 'than',\n",
       " 'that',\n",
       " 'of',\n",
       " 'any',\n",
       " 'country',\n",
       " 'in',\n",
       " 'Africa',\n",
       " '?',\n",
       " ';',\n",
       " '',\n",
       " 'struct',\n",
       " 'e',\n",
       " 'd',\n",
       " 'knowledge',\n",
       " ':',\n",
       " '|',\n",
       " 'world',\n",
       " '_',\n",
       " '1',\n",
       " '|',\n",
       " 'city',\n",
       " '',\n",
       " ':',\n",
       " '',\n",
       " 'i',\n",
       " 'd',\n",
       " '',\n",
       " ',',\n",
       " 'name',\n",
       " '',\n",
       " ',',\n",
       " 'country',\n",
       " 'code',\n",
       " '',\n",
       " ',',\n",
       " 'district',\n",
       " '',\n",
       " ',',\n",
       " 'population',\n",
       " '|',\n",
       " 'sq',\n",
       " 'lite',\n",
       " '_',\n",
       " 's',\n",
       " 'e',\n",
       " 'que',\n",
       " 'nce',\n",
       " '',\n",
       " ':',\n",
       " 'name',\n",
       " '',\n",
       " ',',\n",
       " 'se',\n",
       " 'q',\n",
       " '|',\n",
       " 'country',\n",
       " '',\n",
       " ':',\n",
       " 'code',\n",
       " '',\n",
       " ',',\n",
       " 'name',\n",
       " '',\n",
       " ',',\n",
       " 'continent',\n",
       " '(',\n",
       " 'Africa',\n",
       " '',\n",
       " ',',\n",
       " 'Asia',\n",
       " '',\n",
       " ')',\n",
       " '',\n",
       " ',',\n",
       " 'region',\n",
       " '',\n",
       " ',',\n",
       " 'surface',\n",
       " 'area',\n",
       " '',\n",
       " ',',\n",
       " 'in',\n",
       " 'de',\n",
       " 'p',\n",
       " 'year',\n",
       " '',\n",
       " ',',\n",
       " 'population',\n",
       " '',\n",
       " ',',\n",
       " 'life',\n",
       " 'ex',\n",
       " 'pe',\n",
       " 'c',\n",
       " 't',\n",
       " 'ancy',\n",
       " '',\n",
       " ',',\n",
       " '',\n",
       " 'g',\n",
       " 'n',\n",
       " 'p',\n",
       " '',\n",
       " ',',\n",
       " '',\n",
       " 'g',\n",
       " 'n',\n",
       " 'pol',\n",
       " 'd',\n",
       " '',\n",
       " ',',\n",
       " 'local',\n",
       " 'name',\n",
       " '',\n",
       " ',',\n",
       " 'government',\n",
       " 'form',\n",
       " '',\n",
       " ',',\n",
       " 'head',\n",
       " 'of',\n",
       " 'state',\n",
       " '',\n",
       " ',',\n",
       " 'capital',\n",
       " '',\n",
       " ',',\n",
       " 'code',\n",
       " '2',\n",
       " '|',\n",
       " 'country',\n",
       " 'language',\n",
       " '',\n",
       " ':',\n",
       " 'country',\n",
       " 'code',\n",
       " '',\n",
       " ',',\n",
       " 'language',\n",
       " '',\n",
       " ',',\n",
       " 'is',\n",
       " 'official',\n",
       " '',\n",
       " ',',\n",
       " 'percentage']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a38ff853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('what be the asian country which have a population larger than that of any country in africa ? <C>NONE::* <C>city::id <C>city::name <C>city::countrycode <C>city::district <C>city::population <C>sqlite_sequence::name <C>sqlite_sequence::seq <C>country::code <C>country::name <C>country::continent <C>country::region <C>country::surfacearea <C>country::indepyear <C>country::population <C>country::lifeexpectancy <C>country::gnp <C>country::gnpold <C>country::localname <C>country::governmentform <C>country::headofstate <C>country::capital <C>country::code2 <C>countrylanguage::countrycode <C>countrylanguage::language <C>countrylanguage::isofficial <C>countrylanguage::percentage <T>city <T>sqlite_sequence <T>country <T>countrylanguage',\n",
       " 'What are the Asian countries which have a population larger than that of any country in Africa?; structed knowledge:  | world_1 | city : id , name , countrycode , district , population | sqlite_sequence : name , seq | country : code , name , continent ( Africa , Asia ) , region , surfacearea , indepyear , population , lifeexpectancy , gnp , gnpold , localname , governmentform , headofstate , capital , code2 | countrylanguage : countrycode , language , isofficial , percentage')"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(sample['rat_sql_graph']['nodes']), txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a0de6b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the Asian countries which have a population larger than that of any country in Africa ?'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(sample['question_toks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "47d6dd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f880770d",
   "metadata": {},
   "source": [
    "#### Nodes to char ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f6ffceae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('what', 'What'),\n",
       "  ('be', 'are'),\n",
       "  ('the', 'the'),\n",
       "  ('asian', 'Asian'),\n",
       "  ('country', 'countries'),\n",
       "  ('which', 'which'),\n",
       "  ('have', 'have'),\n",
       "  ('a', 'a'),\n",
       "  ('population', 'population'),\n",
       "  ('larger', 'larger'),\n",
       "  ('than', 'than'),\n",
       "  ('that', 'that'),\n",
       "  ('of', 'of'),\n",
       "  ('any', 'any'),\n",
       "  ('country', 'country'),\n",
       "  ('in', 'in'),\n",
       "  ('africa', 'Africa'),\n",
       "  ('?', '?')],\n",
       " [('NONE', '*'),\n",
       "  ('city', 'id'),\n",
       "  ('city', 'name'),\n",
       "  ('city', 'countrycode'),\n",
       "  ('city', 'district'),\n",
       "  ('city', 'population'),\n",
       "  ('sqlite_sequence', 'name'),\n",
       "  ('sqlite_sequence', 'seq'),\n",
       "  ('country', 'code'),\n",
       "  ('country', 'name'),\n",
       "  ('country', 'continent'),\n",
       "  ('country', 'region'),\n",
       "  ('country', 'surfacearea'),\n",
       "  ('country', 'indepyear'),\n",
       "  ('country', 'population'),\n",
       "  ('country', 'lifeexpectancy'),\n",
       "  ('country', 'gnp'),\n",
       "  ('country', 'gnpold'),\n",
       "  ('country', 'localname'),\n",
       "  ('country', 'governmentform'),\n",
       "  ('country', 'headofstate'),\n",
       "  ('country', 'capital'),\n",
       "  ('country', 'code2'),\n",
       "  ('countrylanguage', 'countrycode'),\n",
       "  ('countrylanguage', 'language'),\n",
       "  ('countrylanguage', 'isofficial'),\n",
       "  ('countrylanguage', 'percentage')],\n",
       " ['city', 'sqlite_sequence', 'country', 'countrylanguage'])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ratsql nodes to sentence char ids \n",
    "\n",
    "ratsql_graph_nodes = sample['rat_sql_graph']['nodes']\n",
    "question_toks = sample['question_toks']\n",
    "\n",
    "_q_nodes = []  # [stem token (node name)]\n",
    "q_nodes = []  # [(stem token (node name), orig question token)]\n",
    "c_nodes = []  # [(orig table name, orig column name)]\n",
    "t_nodes = []  # [orig table name]\n",
    "\n",
    "for n in ratsql_graph_nodes:\n",
    "    if n.startswith('<C>'):\n",
    "        _n = n[3:]\n",
    "        _t, _c = _n.split('::')\n",
    "        c_nodes.append((_t, _c))\n",
    "    elif n.startswith('<T>'):\n",
    "        _n = n[3:]\n",
    "        t_nodes.append(_n)\n",
    "    else:\n",
    "        _q_nodes.append(n)\n",
    "\n",
    "assert len(_q_nodes) == len(question_toks), (_q_nodes, question_toks)\n",
    "q_nodes = list(zip(_q_nodes, question_toks))\n",
    "\n",
    "q_nodes, c_nodes, t_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e9b703ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the Asian countries which have a population larger than that of any country in Africa?; structed knowledge:  | world_1 | city : id , name , countrycode , district , population | sqlite_sequence : name , seq | country : code , name , continent ( Africa , Asia ) , region , surfacearea , indepyear , population , lifeexpectancy , gnp , gnpold , localname , governmentform , headofstate , capital , code2 | countrylanguage : countrycode , language , isofficial , percentage'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "1e586b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_txt = tokenizer_fast([txt], max_length=1024, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8640e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# txt = \"This is t5's tokenization.; structed knowledge: | model | plm(t5), rnn\"\n",
    "\n",
    "# tokenized_txt = tokenizer_fast([test_txt], max_length=1024, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "def4d4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 86)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of whitespace words \n",
    "max([w_id for w_id in tokenized_txt.word_ids() if w_id is not None]) + 1, len(txt.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "72dc9c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_splitter = \"; structed knowledge:  \"  # struct_in has a preceding white space \n",
    "text_in, struct_in = txt.split(_splitter)\n",
    "\n",
    "q_node_chars = []   # [(st, ed)]; same below\n",
    "c_node_chars = []\n",
    "t_node_chars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "28ed86af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 18, CharSpan(start=0, end=4))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text part\n",
    "# Assumption: the mismatch between whitespace words (text_words) and question words only come from trailing puncts\n",
    "\n",
    "text_words = text_in.strip().split(' ') + ['<SENTINAL>']\n",
    "text_word_char_ranges = [tokenized_txt.word_to_chars(i) for i in range(len(text_words) - 1)] + [(None, None)]  # -1 to remove the sentinal \n",
    "\n",
    "len(text_words), len(text_word_char_ranges), text_word_char_ranges[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "74369e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_tw_idx = 0\n",
    "curr_tw = text_words[0]\n",
    "curr_tw_char_range = text_word_char_ranges[0]\n",
    "curr_char_ptr = 0\n",
    "\n",
    "for stem_tok, orig_tok in q_nodes:\n",
    "    if curr_tw == orig_tok:\n",
    "        # finishing current word \n",
    "        q_node_chars.append((curr_char_ptr, curr_char_ptr + len(orig_tok)))   # curr pos to curr pos + len \n",
    "        curr_tw_idx += 1\n",
    "        curr_tw = text_words[curr_tw_idx]\n",
    "        curr_tw_char_range = text_word_char_ranges[curr_tw_idx]\n",
    "        curr_char_ptr = curr_tw_char_range[0]\n",
    "    else:\n",
    "        # not finishing current word \n",
    "        assert curr_tw.startswith(orig_tok), (curr_tw, orig_tok)\n",
    "        q_node_chars.append((curr_char_ptr, curr_char_ptr + len(orig_tok)))   # curr pos to curr pos + len \n",
    "        curr_char_ptr += len(orig_tok)     # move ptr forward by len \n",
    "        curr_tw = curr_tw[len(orig_tok):]  # get the remaining chars in the word \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "ea0a6e94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[txt[st:ed] for st, ed in q_node_chars] == sample['question_toks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "436b4077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Struct part \n",
    "assert not struct_in.startswith(' ')\n",
    "_str_before_struct = text_in + _splitter\n",
    "_n_words_before_struct = len(_str_before_struct.strip().split(' '))\n",
    "\n",
    "struct_words_st_idx = _n_words_before_struct\n",
    "assert len(_str_before_struct) == tokenized_txt.word_to_chars(struct_words_st_idx)[0], \\\n",
    "    (len(_str_before_struct), tokenized_txt.word_to_chars(struct_words_st_idx)[0])   # len(before) == starting char idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "4af49991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'| world_1 | city : id , name , countrycode , district , population | sqlite_sequence : name , seq | country : code , name , continent ( Africa , Asia ) , region , surfacearea , indepyear , population , lifeexpectancy , gnp , gnpold , localname , governmentform , headofstate , capital , code2 | countrylanguage : countrycode , language , isofficial , percentage'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "3aa9bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_ranges_collector = StructCharRangesCollector()\n",
    "struct_ranges_collector.collect(struct_in, tokenized_txt, _n_words_before_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "7d46aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for db_id_name, (st, ed) in struct_ranges_collector.db_id2char_ranges.items():\n",
    "    assert txt[st:ed] == db_id_name, (st, ed, txt[st:ed], db_id_name) \n",
    "\n",
    "for table_name, (st, ed) in struct_ranges_collector.table2char_ranges.items():\n",
    "    assert txt[st:ed] == table_name, (st, ed, txt[st:ed], table_name) \n",
    "\n",
    "for (_, col_name), (st, ed) in struct_ranges_collector.column2char_ranges.items():\n",
    "    txt_piece = txt[st:ed].split(' ( ')[0]\n",
    "    assert txt_piece == col_name, (st, ed, txt[st:ed], col_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "453e17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for db_id_name, (st, ed) in struct_ranges_collector.db_id2char_ranges.items():\n",
    "#     print(st, ed, txt[st:ed], db_id_name) \n",
    "# print()\n",
    "# for table_name, (st, ed) in struct_ranges_collector.table2char_ranges.items():\n",
    "#     print(st, ed, txt[st:ed], table_name) \n",
    "# print()\n",
    "# for (_, col_name), (st, ed) in struct_ranges_collector.column2char_ranges.items():\n",
    "#     txt_piece = txt[st:ed].split(' ( ')[0]\n",
    "#     print(st, ed, txt[st:ed], col_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "366b67fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_node in c_nodes:\n",
    "    if c_node == ('NONE', '*'):\n",
    "        # the special column in spider, using db_id \n",
    "        c_node_chars.append(list(struct_ranges_collector.db_id2char_ranges.values())[0])   # assuming only 1 db_id, which should be true...\n",
    "    else:\n",
    "        c_node_chars.append(struct_ranges_collector.column2char_ranges[c_node])\n",
    "\n",
    "for t_node in t_nodes:\n",
    "    t_node_chars.append(struct_ranges_collector.table2char_ranges[t_node])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebadf20",
   "metadata": {},
   "source": [
    "#### Nodes to tokenized pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f8de0ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 49)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_char_ranges = char_ranges_dict['q_node_chars'] + char_ranges_dict['c_node_chars'] + char_ranges_dict['t_node_chars']\n",
    "len(node_char_ranges), len(sample['rat_sql_graph']['nodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cd87dfbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some chars can be mapped to multiple tokens (e.g. 'i' => '', 'i' )\n",
    "char_to_tokens_dict = defaultdict(list)\n",
    "\n",
    "for token_idx, tok in enumerate(tokenized_txt.tokens()):\n",
    "    if tok == '</s>':\n",
    "        break\n",
    "    char_span = tokenized_txt.token_to_chars(token_idx)\n",
    "    for char_idx in range(char_span[0], char_span[1]):\n",
    "        char_to_tokens_dict[char_idx].append(token_idx)\n",
    "\n",
    "len(char_to_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8090a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_pieces_ranges = []\n",
    "\n",
    "for st, ed in node_ranges:\n",
    "    piece_ids = []\n",
    "    for char_idx in range(st, ed):\n",
    "        _piece_ids = char_to_tokens_dict[char_idx]\n",
    "        piece_ids.extend(_piece_ids)\n",
    "    \n",
    "    piece_st = piece_ids[0]\n",
    "    piece_ed = piece_ids[-1] + 1\n",
    "    # the collected piece_ids should be continuous \n",
    "    # ^ not true... some chars can be mapped to multiple tokens (started by  )\n",
    "    # re-collect a char-to-token\n",
    "    assert set(range(piece_st, piece_ed)) == set(piece_ids), piece_ids\n",
    "    \n",
    "    node_pieces_ranges.append((piece_st, piece_ed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee9fa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n, (p_st, p_ed) in zip(sample['rat_sql_graph']['nodes'], node_pieces_ranges):\n",
    "    print(n, '\\t', tokenized_txt.tokens()[p_st:p_ed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28a37a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15707138",
   "metadata": {},
   "source": [
    "### data: server vs. local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4aead2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_data_path = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/probing/text2sql/link_prediction/spider/uskg/local/dev.train.X.pkl\"\n",
    "server_data_path = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/probing/text2sql/link_prediction/spider/uskg/dev.train.X.pkl\"\n",
    "\n",
    "with open(local_data_path, 'rb') as f:\n",
    "    local_data_X = pickle.load(f)\n",
    "with open(server_data_path, 'rb') as f:\n",
    "    server_data_X = pickle.load(f)\n",
    "\n",
    "type(local_data_X), type(server_data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3b781c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16059, 3072), (16059, 3072))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_data_X = np.array(local_data_X)\n",
    "server_data_X = np.array(server_data_X)\n",
    "local_data_X.shape, server_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "615204af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5264,  5255,  5247,   288,   292,   294, 11989, 11979,  8755,\n",
       "        8756])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_diff_ids = np.argsort(np.max(np.abs(local_data_X - server_data_X), axis=1))[::-1]\n",
    "large_diff_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d725abfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1034, 16059)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_path = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/spider/dev+ratsql_graph.json\"\n",
    "pos_path = \"/Users/mac/Desktop/syt/Deep-Learning/Projects-M/SDR-analysis/data/probing/text2sql/link_prediction/spider/uskg/dev.train.pos.txt\"\n",
    "\n",
    "with open(ds_path, 'r') as f:\n",
    "    ds_samples = json.load(f)\n",
    "    for d in ds_samples:\n",
    "        d['rat_sql_graph']['relations'] = json.loads(d['rat_sql_graph']['relations'])\n",
    "with open(pos_path, 'r') as f:\n",
    "    pos_lines = f.read().strip().split('\\n')\n",
    "\n",
    "len(ds_samples), len(pos_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "6047aefb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5245"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_idx = large_diff_ids[51]\n",
    "_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "e7aa274b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.16274624, -0.06520639,  0.11448385,  0.02375873,  0.00876573,\n",
       "        -0.10075585,  0.11073106,  0.00379219,  0.07125453,  0.01576799,\n",
       "         0.01186033], dtype=float32),\n",
       " array([-0.17650707, -0.07676765,  0.13225155,  0.01257438,  0.00267248,\n",
       "        -0.1266068 ,  0.12796973,  0.00241799,  0.05103843,  0.03357478,\n",
       "         0.01657346], dtype=float32),\n",
       " 0.07821107)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _idx = 8970\n",
    "local_data_X[_idx][::300], server_data_X[_idx][::300], max(local_data_X[_idx] - server_data_X[_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ada838c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696, 6, 6)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid, i, j = [int(s) for s in pos_lines[_idx].split('\\t')]\n",
    "sid, i, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "739500be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 'be',\n",
       " 'the',\n",
       " 'number',\n",
       " 'of',\n",
       " 'vote',\n",
       " 'from',\n",
       " 'state',\n",
       " '`',\n",
       " 'ny',\n",
       " \"'\",\n",
       " 'or',\n",
       " '`',\n",
       " 'ca',\n",
       " \"'\",\n",
       " '?',\n",
       " '<C>NONE::*',\n",
       " '<C>area_code_state::area_code',\n",
       " '<C>area_code_state::state',\n",
       " '<C>contestant::contestant_number',\n",
       " '<C>contestant::contestant_name',\n",
       " '<C>vote::vote_id',\n",
       " '<C>vote::phone_number',\n",
       " '<C>vote::state',\n",
       " '<C>vote::contestant_number',\n",
       " '<C>vote::create',\n",
       " '<T>area_code_state',\n",
       " '<T>contestant',\n",
       " '<T>vote']"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_nodes = ds_samples[sid]['rat_sql_graph']['nodes']\n",
    "_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "5ca89f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('from', 'from')"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_nodes[i], _nodes[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92939f6b",
   "metadata": {},
   "source": [
    "### Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "378c9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set args here for runnning on notebook, we make them out here to make it more illustrative.\n",
    "sys.argv = ['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', # This is the name of your .py launcher when you run this line of code.\n",
    "            # belows are the parameters we set, take spider for example\n",
    "            '--cfg', 'Salesforce/T5_base_finetune_spider_with_cell_value.cfg', \n",
    "            '--output_dir', './tmp']\n",
    "parser = HfArgumentParser((WrappedSeq2SeqTrainingArguments,))\n",
    "training_args, = parser.parse_args_into_dataclasses()\n",
    "set_seed(training_args.seed)\n",
    "tmp_args = Configure.Get(training_args.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "e52c81a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t5-base'"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_args.bert.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "daa2ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 't5-base'\n",
    "# model_path = 'hkunlp/from_all_T5_large_prefix_spider_with_cell_value2'\n",
    "# model_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/UnifiedSKG/output/server_runs/A-T5_base_prefix_spider_with_cell_value-asr_mixed/checkpoint-79500/'\n",
    "# model_path = '/Users/mac/Desktop/syt/Deep-Learning/Repos/UnifiedSKG/output/server_runs/A-T5_base_prefix_spider_with_cell_value-rewritten_mixed/checkpoint-56500/'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "\n",
    "# for reconstruction\n",
    "# tokenizer_fast = AutoTokenizer.from_pretrained('t5-base', use_fast=True)\n",
    "\n",
    "tmp_model = finetune.Model(tmp_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "16e80b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0762, -0.0471,  0.0309],\n",
       "        [ 0.0381, -0.0075,  0.0003],\n",
       "        [-0.0047, -0.0262, -0.0298]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model.pretrain_model.encoder.block[0].layer[0].SelfAttention.q.weight[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "c5723599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = 't5-base'\n",
    "# tmp_model.load(model_path)\n",
    "# # need tmp_model.pretrained_model.load(...)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "9905b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model.pretrain_model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "87642af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0013,  0.0083,  0.0015],\n",
       "        [ 0.0008, -0.0017,  0.0039],\n",
       "        [-0.0088, -0.0019, -0.0036]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model.pretrain_model.encoder.block[0].layer[0].SelfAttention.q.weight[:3,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1b031694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "t5_model_1 = T5ForConditionalGeneration.from_pretrained('t5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1d723314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_model_1.model_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb63261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18fc530f",
   "metadata": {},
   "source": [
    "### Data inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63060d56",
   "metadata": {},
   "source": [
    "#### Original dataset samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "8254d3bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15878"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_path = f\"/home/yshao/Projects/SDR-analysis/data/wikisql/test+ratsql_graph.json\"\n",
    "\n",
    "with open(orig_path, 'r') as f:\n",
    "    orig_dataset = json.load(f)\n",
    "\n",
    "len(orig_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "4cc8e0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "983"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_ascii_samples = []\n",
    "\n",
    "for d in orig_dataset:\n",
    "    if not ' '.join(d['question']).isascii():\n",
    "        non_ascii_samples.append(d)\n",
    "\n",
    "len(non_ascii_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a58c65",
   "metadata": {},
   "source": [
    "#### Probing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5cdd9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sdr_analysis.helpers.general_helpers\n",
    "importlib.reload(sdr_analysis.helpers.general_helpers)\n",
    "from sdr_analysis.helpers.general_helpers import load_pickle_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "6876e10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_probe_task = 'link_prediction'  # link_prediction / single_node_reconstruction \n",
    "_dataset = 'spider'\n",
    "_method = 'uskg'\n",
    "_orig_ds = 'train'\n",
    "_prob_ds = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "3ec837a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yshao/Projects/SDR-analysis/data/probing/text2sql/link_prediction/spider/uskg/train.test.X.pkl has only 1 object, type is <class 'list'>; return as-is\n"
     ]
    }
   ],
   "source": [
    "X_path = f\"/home/yshao/Projects/SDR-analysis/data/probing/text2sql/{_probe_task}/{_dataset}/{_method}/{_orig_ds}.{_prob_ds}.X.pkl\"\n",
    "\n",
    "# with open(X_path, 'rb') as f:\n",
    "X = load_pickle_list(X_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "3a801416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 14660)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X), len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a661bd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (3072,))"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X[0]), X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f1a93990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yshao/Projects/SDR-analysis/data/probing/text2sql/link_prediction/spider/uskg/train.test.y.pkl has only 1 object, type is <class 'list'>; return as-is\n"
     ]
    }
   ],
   "source": [
    "y_path = f\"/home/yshao/Projects/SDR-analysis/data/probing/text2sql/{_probe_task}/{_dataset}/{_method}/{_orig_ds}.{_prob_ds}.y.pkl\"\n",
    "\n",
    "# with open(y_path, 'r') as f:\n",
    "#     y = f.read().strip().split('\\n')\n",
    "y = load_pickle_list(y_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "78dcab8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14660"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "83c78246",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 1, 0, 45, 49, 7]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "11d02e72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 496,\n",
       "         3: 496,\n",
       "         4: 496,\n",
       "         5: 496,\n",
       "         6: 496,\n",
       "         1: 496,\n",
       "         0: 496,\n",
       "         45: 95,\n",
       "         49: 110,\n",
       "         7: 496,\n",
       "         14: 496,\n",
       "         8: 496,\n",
       "         21: 496,\n",
       "         46: 95,\n",
       "         11: 496,\n",
       "         10: 438,\n",
       "         19: 473,\n",
       "         17: 496,\n",
       "         20: 496,\n",
       "         50: 110,\n",
       "         9: 438,\n",
       "         22: 496,\n",
       "         26: 496,\n",
       "         24: 473,\n",
       "         25: 496,\n",
       "         23: 496,\n",
       "         34: 496,\n",
       "         28: 432,\n",
       "         30: 423,\n",
       "         29: 423,\n",
       "         41: 287,\n",
       "         47: 27,\n",
       "         48: 27,\n",
       "         42: 287,\n",
       "         39: 197,\n",
       "         43: 73,\n",
       "         37: 266,\n",
       "         38: 266,\n",
       "         40: 197,\n",
       "         44: 73,\n",
       "         18: 13,\n",
       "         27: 13})"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter([_y.split(' ')[0] for _y in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a25001b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_path = f\"/home/yshao/Projects/SDR-analysis/data/probing/text2sql/{_probe_task}/{_dataset}/{_method}/{_orig_ds}.{_prob_ds}.pos.txt\"\n",
    "\n",
    "with open(pos_path, 'r') as f:\n",
    "    pos_lines = f.read().strip().split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4820c4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87206"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1b6a3e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['3756\\t0',\n",
       "  '3756\\t1',\n",
       "  '3756\\t2',\n",
       "  '3756\\t3',\n",
       "  '3756\\t4',\n",
       "  '3756\\t5',\n",
       "  '3756\\t6',\n",
       "  '3756\\t7',\n",
       "  '3756\\t8',\n",
       "  '3756\\t9',\n",
       "  '3756\\t10',\n",
       "  '3756\\t11',\n",
       "  '3756\\t12',\n",
       "  '3756\\t13',\n",
       "  '3756\\t14',\n",
       "  '3756\\t15',\n",
       "  '3756\\t16',\n",
       "  '3756\\t17',\n",
       "  '3756\\t18',\n",
       "  '3756\\t19',\n",
       "  '3756\\t20',\n",
       "  '3756\\t21',\n",
       "  '3756\\t22',\n",
       "  '3756\\t23',\n",
       "  '3756\\t24',\n",
       "  '3756\\t25',\n",
       "  '3756\\t26',\n",
       "  '3756\\t27',\n",
       "  '3756\\t28',\n",
       "  '3756\\t29'],\n",
       " ['1138\\t2',\n",
       "  '1138\\t4',\n",
       "  '1138\\t5',\n",
       "  '1138\\t6',\n",
       "  '1138\\t7',\n",
       "  '1138\\t8',\n",
       "  '1138\\t9',\n",
       "  '1138\\t10',\n",
       "  '1138\\t11',\n",
       "  '5971\\t0',\n",
       "  '5971\\t1',\n",
       "  '5971\\t2',\n",
       "  '5971\\t3',\n",
       "  '5971\\t4',\n",
       "  '5971\\t6',\n",
       "  '5971\\t7',\n",
       "  '5971\\t8',\n",
       "  '5971\\t9',\n",
       "  '5971\\t10',\n",
       "  '5971\\t11',\n",
       "  '8211\\t0',\n",
       "  '8211\\t1',\n",
       "  '8211\\t2',\n",
       "  '8211\\t3',\n",
       "  '8211\\t5',\n",
       "  '8211\\t6',\n",
       "  '8211\\t7',\n",
       "  '8211\\t8',\n",
       "  '8211\\t9',\n",
       "  '8211\\t10'])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_lines[:30], pos_lines[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "31ed6257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4210"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([pos.split('\\t')[0] for pos in pos_lines]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "5b845a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "abf3aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_probe_task = 'link_prediction'  # link_prediction / single_node_reconstruction \n",
    "_orig_ds = 'train'\n",
    "_prob_ds = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b059c6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yshao/Projects/SDR-analysis/data/probing/text2sql/link_prediction/spider/uskg/train.test.y.pkl has only 1 object, type is <class 'list'>; return as-is\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14660"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_path = f\"/home/yshao/Projects/SDR-analysis/data/probing/text2sql/{_probe_task}/spider/uskg/{_orig_ds}.{_prob_ds}.y.pkl\"\n",
    "y1 = load_pickle_list(y1_path)\n",
    "len(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "f1a846fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yshao/Projects/SDR-analysis/data/probing/text2sql/link_prediction/spider/lgesql/train.test.y.pkl has 14660 objects, type is <class 'numpy.int64'>; combined to a list\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14660"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2_path = f\"/home/yshao/Projects/SDR-analysis/data/probing/text2sql/{_probe_task}/spider/lgesql/{_orig_ds}.{_prob_ds}.y.pkl\"\n",
    "y2 = load_pickle_list(y2_path)\n",
    "len(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "49809045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 == y2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df0433d",
   "metadata": {},
   "source": [
    "#### Tokenized len counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edfa39d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.026132822036743164,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 31,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 581931,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff20c694cc80406d86e5c9bb8314060f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/581931 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "581931"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenized = [tokenizer_fast.tokenize(_y) for _y in tqdm(y)]\n",
    "len(y_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f9da802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['question', 'what'],\n",
       " ['question', 'be'],\n",
       " ['question', 'the'],\n",
       " ['question', '', \"'\"],\n",
       " ['question', '', '-', 'l', 'c', 'b', '-'],\n",
       " ['question', '', '\\\\'],\n",
       " ['question', 'math', 'r', 'm'],\n",
       " ['question', '', '-', 'l', 'c', 'b', '-'],\n",
       " ['question', '', 'h'],\n",
       " ['question', '', ',']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c103be6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_counter = Counter([len(_y) for _y in y_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3262c759",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 14),\n",
       " (2, 380975),\n",
       " (3, 113910),\n",
       " (4, 36631),\n",
       " (5, 18764),\n",
       " (6, 11131),\n",
       " (7, 12137),\n",
       " (8, 3089),\n",
       " (9, 1585),\n",
       " (10, 1039),\n",
       " (11, 629),\n",
       " (12, 423),\n",
       " (13, 326),\n",
       " (14, 313),\n",
       " (15, 255),\n",
       " (16, 122),\n",
       " (17, 117),\n",
       " (18, 129),\n",
       " (19, 69),\n",
       " (20, 40),\n",
       " (21, 42),\n",
       " (22, 25),\n",
       " (23, 39),\n",
       " (24, 15),\n",
       " (25, 4),\n",
       " (26, 55),\n",
       " (27, 5),\n",
       " (29, 9),\n",
       " (30, 3),\n",
       " (32, 3),\n",
       " (33, 8),\n",
       " (35, 2),\n",
       " (36, 3),\n",
       " (38, 1),\n",
       " (44, 2),\n",
       " (51, 3),\n",
       " (72, 3),\n",
       " (92, 6),\n",
       " (134, 3),\n",
       " (182, 2)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(len_counter.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdc4f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab601e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fdfede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db68ac13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7724f90",
   "metadata": {},
   "source": [
    "### T5 decoder loading\n",
    "- TODO: use these in allennlp SNR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6b41cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "81d50e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a90c3876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_model.config.decoder_start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1f2a151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d9676c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_model.config.num_decoder_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d5d4f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[  0.0364,  -0.2363,  -0.2236,  ...,  -0.2812,  -3.8281,  -5.9688],\n",
       "        [  5.2188,  -7.0312,   7.3438,  ...,   2.7500,  14.9375,  25.1250],\n",
       "        [  3.4844,   5.9688,  12.5000,  ...,  15.9375,  -4.2188,   6.4688],\n",
       "        ...,\n",
       "        [-20.0000,  -8.8125,   7.2188,  ...,  -5.8438,  11.0625,  23.8750],\n",
       "        [-20.8750,  -7.0312,   8.5625,  ...,  -4.6562,  11.8750,  23.0000],\n",
       "        [-20.6250,  -7.0000,   9.3125,  ...,  -6.5000,  11.2500,  22.0000]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_model.lm_head.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "437d7cd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.12346454,  0.16762893, -0.14894915, ...,  0.01294581,\n",
       "         -0.12181459, -0.11360805]], dtype=float32),\n",
       " 'question c')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 55\n",
    "X[p], y[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cf7ffcfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question', '', 'c']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_fast.tokenize(y[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22ba9492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[822,   3,  75,   1]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tokenized = tokenizer_fast.batch_encode_plus([y[p]], return_tensors='pt')\n",
    "y_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "35cc1f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question', '', 'c', '</s>']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_fast.convert_ids_to_tokens(y_tokenized['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aba8e37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 822,   3,  75]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = y_tokenized['input_ids']\n",
    "dec_input_ids = t5_model._shift_right(labels)\n",
    "dec_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a098d700",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_hidden_states = torch.tensor([X[p]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f31c3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_output = t5_model.decoder.forward(\n",
    "    input_ids = dec_input_ids,\n",
    "    attention_mask = y_tokenized['attention_mask'],\n",
    "    encoder_hidden_states = enc_hidden_states,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f1e05249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['last_hidden_state', 'past_key_values', 'hidden_states', 'attentions', 'cross_attentions'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_output.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "45f3809e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1024])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_output.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7fd5e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "## copied from t5 source\n",
    "\n",
    "sequence_output = dec_output[0]\n",
    "# Rescale output before projecting on vocab\n",
    "# See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/transformer.py#L586\n",
    "sequence_output = sequence_output * (t5_model.model_dim ** -0.5)\n",
    "lm_logits = t5_model.lm_head(sequence_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fe22ec18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 32128])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "13bb24f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[205,  75,  75,  75]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_preds = lm_logits.argmax(dim=-1)\n",
    "lm_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1d0113dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C', 'c', 'c', 'c']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_gen_toks = tokenizer_fast.convert_ids_to_tokens(lm_preds[0])\n",
    "_gen_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84839720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cccc'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_fast.decode(lm_preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa042b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee8a8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc68f68d",
   "metadata": {},
   "source": [
    "### others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b75ac60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t5-base'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.bert.location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3bd93a40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__self__': None,\n",
       " '__default__': {'__call__',\n",
       "  '__class__',\n",
       "  '__default__',\n",
       "  '__delattr__',\n",
       "  '__dict__',\n",
       "  '__dir__',\n",
       "  '__doc__',\n",
       "  '__eq__',\n",
       "  '__format__',\n",
       "  '__ge__',\n",
       "  '__getattribute__',\n",
       "  '__gt__',\n",
       "  '__hash__',\n",
       "  '__init__',\n",
       "  '__init_subclass__',\n",
       "  '__iter__',\n",
       "  '__le__',\n",
       "  '__len__',\n",
       "  '__lt__',\n",
       "  '__module__',\n",
       "  '__ne__',\n",
       "  '__new__',\n",
       "  '__reduce__',\n",
       "  '__reduce_ex__',\n",
       "  '__repr__',\n",
       "  '__self__',\n",
       "  '__setattr__',\n",
       "  '__sizeof__',\n",
       "  '__str__',\n",
       "  '__subclasshook__',\n",
       "  '__weakref__'},\n",
       " 'name': 'unified.prefixtuning',\n",
       " 'use_description': False,\n",
       " 'concatenate_description': False,\n",
       " 'map_description': False,\n",
       " 'knowledge_usage': 'concatenate',\n",
       " 'freeze_plm': True,\n",
       " 'freeze_prefix': False}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.model.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b9eb3682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(True or 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a54fd471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print([] or 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "454833f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_list = ['quick brown fox', 'old lazy dog', 'you jump I jump']\n",
    "\n",
    "tokenized_txt_list = tokenizer_fast(txt_list, max_length=16, padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5af709d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_txt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c876399a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': [[1704, 4216, 3, 20400, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [625, 19743, 1782, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [25, 4418, 27, 4418, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "  'attention_mask': [[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "   [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]},\n",
       " [Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       "  Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]),\n",
       "  Encoding(num_tokens=16, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_txt_list.data, tokenized_txt_list.encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0d2291b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'date', 'was', 'the', 'izmir', 'cup', 'in', 'which', 'ilhan', 'played', 'against', 'somdev', 'devvarman?']\n"
     ]
    }
   ],
   "source": [
    "print(\"What date was the Izmir Cup in which \\u0130lhan played against Somdev Devvarman?\".lower().strip().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bf5933b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I', '', False)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"I\\u0307\", \"\\u0130\", \"I\\u0307\" == \"\\u0130\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "59fabb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('i', 'i', True)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"I\\u0307\".lower(), \"\\u0130\".lower(), \"I\\u0307\".lower() == \"\\u0130\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d65a8422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"I\\u0307\"), len(\"\\u0130\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8989f7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"I\\u0307\".lower()), len(\"\\u0130\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "b92670e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import setup, find_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "5c3cc748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models',\n",
       " 'utils',\n",
       " 'models.prompt',\n",
       " 'models.adapter',\n",
       " 'models.unified',\n",
       " 'utils.processor']"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_packages(include=['models*', 'utils*'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ecf6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec4ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621dbf15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03169b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9463ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
